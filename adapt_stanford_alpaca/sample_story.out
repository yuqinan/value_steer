SLURM_JOBID=8746878
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
/var/lib/slurm/slurmd/job8746878/slurm_script: line 28: activate: No such file or directory
W1010 13:32:44.893000 140714473116032 torch/distributed/run.py:779] 
W1010 13:32:44.893000 140714473116032 torch/distributed/run.py:779] *****************************************
W1010 13:32:44.893000 140714473116032 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1010 13:32:44.893000 140714473116032 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.12s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:43, 21.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:43, 21.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:43, 21.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:04<00:21, 21.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:04<00:21, 21.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:04<00:21, 21.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:10<00:00, 15.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:10<00:00, 17.68s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:10<00:00, 15.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:10<00:00, 17.71s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:11<00:00, 15.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:11<00:00, 17.75s/it]
loaded
loaded
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
load trainer
load trainer
load trainer
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_133520-dg775ki2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /nlp/scr/qinanyu/model_cache/result_story
wandb: ⭐️ View project at https://wandb.ai/limber/huggingface
wandb: 🚀 View run at https://wandb.ai/limber/huggingface/runs/dg775ki2
  0%|          | 0/624 [00:00<?, ?it/s]  0%|          | 1/624 [00:07<1:15:45,  7.30s/it]                                                 {'loss': 1.5148, 'grad_norm': 12.495073318481445, 'learning_rate': 1.0526315789473685e-06, 'epoch': 0.0}
  0%|          | 1/624 [00:07<1:15:45,  7.30s/it]  0%|          | 2/624 [00:12<1:02:23,  6.02s/it]                                                 {'loss': 1.7605, 'grad_norm': 10.800080299377441, 'learning_rate': 2.105263157894737e-06, 'epoch': 0.01}
  0%|          | 2/624 [00:12<1:02:23,  6.02s/it]  0%|          | 3/624 [00:17<58:24,  5.64s/it]                                                 {'loss': 1.5933, 'grad_norm': 25.183944702148438, 'learning_rate': 3.157894736842105e-06, 'epoch': 0.01}
  0%|          | 3/624 [00:17<58:24,  5.64s/it]  1%|          | 4/624 [00:22<56:11,  5.44s/it]                                               {'loss': 1.5672, 'grad_norm': 8.411199569702148, 'learning_rate': 4.210526315789474e-06, 'epoch': 0.02}
  1%|          | 4/624 [00:22<56:11,  5.44s/it]  1%|          | 5/624 [00:28<55:58,  5.43s/it]                                               {'loss': 1.5969, 'grad_norm': 9.326838493347168, 'learning_rate': 5.263157894736842e-06, 'epoch': 0.02}
  1%|          | 5/624 [00:28<55:58,  5.43s/it]  1%|          | 6/624 [00:33<55:21,  5.37s/it]                                               {'loss': 1.4737, 'grad_norm': 9.740287780761719, 'learning_rate': 6.31578947368421e-06, 'epoch': 0.03}
  1%|          | 6/624 [00:33<55:21,  5.37s/it]  1%|          | 7/624 [00:38<54:28,  5.30s/it]                                               {'loss': 1.3485, 'grad_norm': 5.691515922546387, 'learning_rate': 7.368421052631579e-06, 'epoch': 0.03}
  1%|          | 7/624 [00:38<54:28,  5.30s/it]  1%|▏         | 8/624 [00:43<53:36,  5.22s/it]                                               {'loss': 1.4755, 'grad_norm': 5.271927833557129, 'learning_rate': 8.421052631578948e-06, 'epoch': 0.04}
  1%|▏         | 8/624 [00:43<53:36,  5.22s/it]  1%|▏         | 9/624 [00:49<54:16,  5.30s/it]                                               {'loss': 1.4746, 'grad_norm': 5.3270440101623535, 'learning_rate': 9.473684210526315e-06, 'epoch': 0.04}
  1%|▏         | 9/624 [00:49<54:16,  5.30s/it]  2%|▏         | 10/624 [00:54<54:24,  5.32s/it]                                                {'loss': 1.4614, 'grad_norm': 5.135063648223877, 'learning_rate': 1.0526315789473684e-05, 'epoch': 0.05}
  2%|▏         | 10/624 [00:54<54:24,  5.32s/it]  2%|▏         | 11/624 [00:59<54:25,  5.33s/it]                                                {'loss': 1.6123, 'grad_norm': 5.676010608673096, 'learning_rate': 1.1578947368421053e-05, 'epoch': 0.05}
  2%|▏         | 11/624 [00:59<54:25,  5.33s/it]  2%|▏         | 12/624 [01:04<53:04,  5.20s/it]                                                {'loss': 1.3983, 'grad_norm': 5.748696804046631, 'learning_rate': 1.263157894736842e-05, 'epoch': 0.06}
  2%|▏         | 12/624 [01:04<53:04,  5.20s/it]  2%|▏         | 13/624 [01:10<53:21,  5.24s/it]                                                {'loss': 1.5384, 'grad_norm': 6.293537139892578, 'learning_rate': 1.3684210526315791e-05, 'epoch': 0.06}
  2%|▏         | 13/624 [01:10<53:21,  5.24s/it]  2%|▏         | 14/624 [01:15<52:55,  5.21s/it]                                                {'loss': 1.5904, 'grad_norm': 7.053627967834473, 'learning_rate': 1.4736842105263159e-05, 'epoch': 0.07}
  2%|▏         | 14/624 [01:15<52:55,  5.21s/it]  2%|▏         | 15/624 [01:20<53:38,  5.28s/it]                                                {'loss': 1.5146, 'grad_norm': 6.00828218460083, 'learning_rate': 1.578947368421053e-05, 'epoch': 0.07}
  2%|▏         | 15/624 [01:20<53:38,  5.28s/it]  3%|▎         | 16/624 [01:25<53:30,  5.28s/it]                                                {'loss': 1.3988, 'grad_norm': 5.988011360168457, 'learning_rate': 1.6842105263157896e-05, 'epoch': 0.08}
  3%|▎         | 16/624 [01:26<53:30,  5.28s/it]  3%|▎         | 17/624 [01:31<53:05,  5.25s/it]                                                {'loss': 1.4597, 'grad_norm': 5.154376029968262, 'learning_rate': 1.7894736842105264e-05, 'epoch': 0.08}
  3%|▎         | 17/624 [01:31<53:05,  5.25s/it]  3%|▎         | 18/624 [01:36<52:14,  5.17s/it]                                                {'loss': 1.5258, 'grad_norm': 5.918524742126465, 'learning_rate': 1.894736842105263e-05, 'epoch': 0.09}
  3%|▎         | 18/624 [01:36<52:14,  5.17s/it]  3%|▎         | 19/624 [01:41<52:01,  5.16s/it]                                                {'loss': 1.5292, 'grad_norm': 6.0544891357421875, 'learning_rate': 2e-05, 'epoch': 0.09}
  3%|▎         | 19/624 [01:41<52:01,  5.16s/it]  3%|▎         | 20/624 [01:46<52:34,  5.22s/it]                                                {'loss': 1.3857, 'grad_norm': 5.362462043762207, 'learning_rate': 1.9999865178850847e-05, 'epoch': 0.1}
  3%|▎         | 20/624 [01:46<52:34,  5.22s/it]  3%|▎         | 21/624 [01:51<52:21,  5.21s/it]                                                {'loss': 1.3768, 'grad_norm': 8.811148643493652, 'learning_rate': 1.999946071903873e-05, 'epoch': 0.1}
  3%|▎         | 21/624 [01:51<52:21,  5.21s/it]  4%|▎         | 22/624 [01:56<52:13,  5.21s/it]                                                {'loss': 1.4575, 'grad_norm': 5.644942283630371, 'learning_rate': 1.9998786631469602e-05, 'epoch': 0.11}
  4%|▎         | 22/624 [01:57<52:13,  5.21s/it]  4%|▎         | 23/624 [02:02<51:43,  5.16s/it]                                                {'loss': 1.5145, 'grad_norm': 5.473442077636719, 'learning_rate': 1.999784293431971e-05, 'epoch': 0.11}
  4%|▎         | 23/624 [02:02<51:43,  5.16s/it]  4%|▍         | 24/624 [02:07<52:18,  5.23s/it]                                                {'loss': 1.5063, 'grad_norm': 5.443707466125488, 'learning_rate': 1.9996629653035128e-05, 'epoch': 0.12}
  4%|▍         | 24/624 [02:07<52:18,  5.23s/it]  4%|▍         | 25/624 [02:12<51:42,  5.18s/it]                                                {'loss': 1.5326, 'grad_norm': 4.950822353363037, 'learning_rate': 1.999514682033104e-05, 'epoch': 0.12}
  4%|▍         | 25/624 [02:12<51:42,  5.18s/it]  4%|▍         | 26/624 [02:17<52:21,  5.25s/it]                                                {'loss': 1.5406, 'grad_norm': 5.2811970710754395, 'learning_rate': 1.99933944761909e-05, 'epoch': 0.12}
  4%|▍         | 26/624 [02:18<52:21,  5.25s/it]  4%|▍         | 27/624 [02:23<52:17,  5.26s/it]                                                {'loss': 1.6535, 'grad_norm': 5.41627836227417, 'learning_rate': 1.999137266786531e-05, 'epoch': 0.13}
  4%|▍         | 27/624 [02:23<52:17,  5.26s/it]  4%|▍         | 28/624 [02:28<52:55,  5.33s/it]                                                {'loss': 1.6028, 'grad_norm': 4.2735066413879395, 'learning_rate': 1.998908144987078e-05, 'epoch': 0.13}
  4%|▍         | 28/624 [02:28<52:55,  5.33s/it]  5%|▍         | 29/624 [02:33<51:31,  5.20s/it]                                                {'loss': 1.3282, 'grad_norm': 4.200089931488037, 'learning_rate': 1.9986520883988233e-05, 'epoch': 0.14}
  5%|▍         | 29/624 [02:33<51:31,  5.20s/it]  5%|▍         | 30/624 [02:38<51:27,  5.20s/it]                                                {'loss': 1.6066, 'grad_norm': 6.770906925201416, 'learning_rate': 1.9983691039261358e-05, 'epoch': 0.14}
  5%|▍         | 30/624 [02:38<51:27,  5.20s/it]  5%|▍         | 31/624 [02:44<51:45,  5.24s/it]                                                {'loss': 1.4564, 'grad_norm': 4.439695835113525, 'learning_rate': 1.998059199199474e-05, 'epoch': 0.15}
  5%|▍         | 31/624 [02:44<51:45,  5.24s/it]  5%|▌         | 32/624 [02:49<51:16,  5.20s/it]                                                {'loss': 1.659, 'grad_norm': 5.4714274406433105, 'learning_rate': 1.9977223825751802e-05, 'epoch': 0.15}
  5%|▌         | 32/624 [02:49<51:16,  5.20s/it]  5%|▌         | 33/624 [02:54<52:43,  5.35s/it]                                                {'loss': 1.4135, 'grad_norm': 5.632539749145508, 'learning_rate': 1.997358663135255e-05, 'epoch': 0.16}
  5%|▌         | 33/624 [02:55<52:43,  5.35s/it]  5%|▌         | 34/624 [03:00<52:52,  5.38s/it]                                                {'loss': 1.6988, 'grad_norm': 5.743926525115967, 'learning_rate': 1.9969680506871138e-05, 'epoch': 0.16}
  5%|▌         | 34/624 [03:00<52:52,  5.38s/it]  6%|▌         | 35/624 [03:05<52:36,  5.36s/it]                                                {'loss': 1.4525, 'grad_norm': 6.971195697784424, 'learning_rate': 1.9965505557633188e-05, 'epoch': 0.17}
  6%|▌         | 35/624 [03:05<52:36,  5.36s/it]  6%|▌         | 36/624 [03:10<52:10,  5.32s/it]                                                {'loss': 1.6962, 'grad_norm': 4.961376667022705, 'learning_rate': 1.9961061896213006e-05, 'epoch': 0.17}
  6%|▌         | 36/624 [03:11<52:10,  5.32s/it]  6%|▌         | 37/624 [03:15<50:40,  5.18s/it]                                                {'loss': 1.7556, 'grad_norm': 4.8596391677856445, 'learning_rate': 1.9956349642430494e-05, 'epoch': 0.18}
  6%|▌         | 37/624 [03:15<50:40,  5.18s/it]  6%|▌         | 38/624 [03:21<51:12,  5.24s/it]                                                {'loss': 1.6542, 'grad_norm': 4.867135524749756, 'learning_rate': 1.9951368923347945e-05, 'epoch': 0.18}
  6%|▌         | 38/624 [03:21<51:12,  5.24s/it]  6%|▋         | 39/624 [03:26<51:01,  5.23s/it]                                                {'loss': 1.5974, 'grad_norm': 5.562664031982422, 'learning_rate': 1.9946119873266615e-05, 'epoch': 0.19}
  6%|▋         | 39/624 [03:26<51:01,  5.23s/it]  6%|▋         | 40/624 [03:31<51:10,  5.26s/it]                                                {'loss': 1.5401, 'grad_norm': 4.6454644203186035, 'learning_rate': 1.9940602633723097e-05, 'epoch': 0.19}
  6%|▋         | 40/624 [03:31<51:10,  5.26s/it]  7%|▋         | 41/624 [03:36<50:28,  5.19s/it]                                                {'loss': 1.7588, 'grad_norm': 6.53574275970459, 'learning_rate': 1.99348173534855e-05, 'epoch': 0.2}
  7%|▋         | 41/624 [03:36<50:28,  5.19s/it]  7%|▋         | 42/624 [03:41<49:48,  5.14s/it]                                                {'loss': 1.5477, 'grad_norm': 6.025692939758301, 'learning_rate': 1.9928764188549462e-05, 'epoch': 0.2}
  7%|▋         | 42/624 [03:41<49:48,  5.14s/it]  7%|▋         | 43/624 [03:46<49:47,  5.14s/it]                                                {'loss': 1.5345, 'grad_norm': 7.570915222167969, 'learning_rate': 1.9922443302133906e-05, 'epoch': 0.21}
  7%|▋         | 43/624 [03:46<49:47,  5.14s/it]  7%|▋         | 44/624 [03:52<50:31,  5.23s/it]                                                {'loss': 1.6092, 'grad_norm': 4.588850498199463, 'learning_rate': 1.9915854864676665e-05, 'epoch': 0.21}
  7%|▋         | 44/624 [03:52<50:31,  5.23s/it]  7%|▋         | 45/624 [03:57<50:25,  5.23s/it]                                                {'loss': 1.4309, 'grad_norm': 6.499035835266113, 'learning_rate': 1.990899905382988e-05, 'epoch': 0.22}
  7%|▋         | 45/624 [03:57<50:25,  5.23s/it]  7%|▋         | 46/624 [04:02<49:44,  5.16s/it]                                                {'loss': 1.606, 'grad_norm': 4.886996746063232, 'learning_rate': 1.9901876054455217e-05, 'epoch': 0.22}
  7%|▋         | 46/624 [04:02<49:44,  5.16s/it]  8%|▊         | 47/624 [04:07<49:47,  5.18s/it]                                                {'loss': 1.5692, 'grad_norm': 4.599013805389404, 'learning_rate': 1.9894486058618863e-05, 'epoch': 0.23}
  8%|▊         | 47/624 [04:07<49:47,  5.18s/it]  8%|▊         | 48/624 [04:12<49:32,  5.16s/it]                                                {'loss': 1.2661, 'grad_norm': 5.611548900604248, 'learning_rate': 1.9886829265586368e-05, 'epoch': 0.23}
  8%|▊         | 48/624 [04:12<49:32,  5.16s/it]  8%|▊         | 49/624 [04:18<49:37,  5.18s/it]                                                {'loss': 1.4835, 'grad_norm': 4.735146522521973, 'learning_rate': 1.9878905881817254e-05, 'epoch': 0.24}
  8%|▊         | 49/624 [04:18<49:37,  5.18s/it]  8%|▊         | 50/624 [04:23<49:54,  5.22s/it]                                                {'loss': 1.734, 'grad_norm': 4.288096904754639, 'learning_rate': 1.9870716120959462e-05, 'epoch': 0.24}
  8%|▊         | 50/624 [04:23<49:54,  5.22s/it]  8%|▊         | 51/624 [04:28<49:22,  5.17s/it]                                                {'loss': 1.4794, 'grad_norm': 4.314927577972412, 'learning_rate': 1.986226020384359e-05, 'epoch': 0.24}
  8%|▊         | 51/624 [04:28<49:22,  5.17s/it]  8%|▊         | 52/624 [04:33<49:34,  5.20s/it]                                                {'loss': 1.8099, 'grad_norm': 7.306268215179443, 'learning_rate': 1.9853538358476933e-05, 'epoch': 0.25}
  8%|▊         | 52/624 [04:33<49:34,  5.20s/it]  8%|▊         | 53/624 [04:39<50:36,  5.32s/it]                                                {'loss': 1.4925, 'grad_norm': 5.209691047668457, 'learning_rate': 1.9844550820037326e-05, 'epoch': 0.25}
  8%|▊         | 53/624 [04:39<50:36,  5.32s/it]  9%|▊         | 54/624 [04:44<50:41,  5.34s/it]                                                {'loss': 1.5348, 'grad_norm': 5.333507537841797, 'learning_rate': 1.9835297830866827e-05, 'epoch': 0.26}
  9%|▊         | 54/624 [04:44<50:41,  5.34s/it]  9%|▉         | 55/624 [04:50<50:40,  5.34s/it]                                                {'loss': 1.5981, 'grad_norm': 5.025387763977051, 'learning_rate': 1.9825779640465157e-05, 'epoch': 0.26}
  9%|▉         | 55/624 [04:50<50:40,  5.34s/it]  9%|▉         | 56/624 [04:55<50:35,  5.34s/it]                                                {'loss': 1.4645, 'grad_norm': 5.342317581176758, 'learning_rate': 1.9815996505483e-05, 'epoch': 0.27}
  9%|▉         | 56/624 [04:55<50:35,  5.34s/it]  9%|▉         | 57/624 [05:00<49:34,  5.25s/it]                                                {'loss': 1.7636, 'grad_norm': 6.411173343658447, 'learning_rate': 1.9805948689715043e-05, 'epoch': 0.27}
  9%|▉         | 57/624 [05:00<49:34,  5.25s/it]  9%|▉         | 58/624 [05:05<49:34,  5.26s/it]                                                {'loss': 1.3195, 'grad_norm': 4.63899564743042, 'learning_rate': 1.979563646409291e-05, 'epoch': 0.28}
  9%|▉         | 58/624 [05:05<49:34,  5.26s/it]  9%|▉         | 59/624 [05:10<49:05,  5.21s/it]                                                {'loss': 1.4624, 'grad_norm': 6.351734638214111, 'learning_rate': 1.9785060106677818e-05, 'epoch': 0.28}
  9%|▉         | 59/624 [05:10<49:05,  5.21s/it] 10%|▉         | 60/624 [05:15<48:42,  5.18s/it]                                                {'loss': 1.4518, 'grad_norm': 4.862399101257324, 'learning_rate': 1.97742199026531e-05, 'epoch': 0.29}
 10%|▉         | 60/624 [05:16<48:42,  5.18s/it] 10%|▉         | 61/624 [05:21<49:04,  5.23s/it]                                                {'loss': 1.5438, 'grad_norm': 5.160120010375977, 'learning_rate': 1.9763116144316506e-05, 'epoch': 0.29}
 10%|▉         | 61/624 [05:21<49:04,  5.23s/it] 10%|▉         | 62/624 [05:26<49:14,  5.26s/it]                                                {'loss': 1.2218, 'grad_norm': 4.919559001922607, 'learning_rate': 1.9751749131072335e-05, 'epoch': 0.3}
 10%|▉         | 62/624 [05:26<49:14,  5.26s/it] 10%|█         | 63/624 [05:31<48:57,  5.24s/it]                                                {'loss': 1.4978, 'grad_norm': 4.742343425750732, 'learning_rate': 1.9740119169423337e-05, 'epoch': 0.3}
 10%|█         | 63/624 [05:31<48:57,  5.24s/it] 10%|█         | 64/624 [05:37<49:06,  5.26s/it]                                                {'loss': 1.7169, 'grad_norm': 4.493795871734619, 'learning_rate': 1.9728226572962474e-05, 'epoch': 0.31}
 10%|█         | 64/624 [05:37<49:06,  5.26s/it] 10%|█         | 65/624 [05:42<48:15,  5.18s/it]                                                {'loss': 1.5625, 'grad_norm': 5.415977478027344, 'learning_rate': 1.9716071662364454e-05, 'epoch': 0.31}
 10%|█         | 65/624 [05:42<48:15,  5.18s/it] 11%|█         | 66/624 [05:47<48:17,  5.19s/it]                                                {'loss': 1.5003, 'grad_norm': 4.367924690246582, 'learning_rate': 1.970365476537707e-05, 'epoch': 0.32}
 11%|█         | 66/624 [05:47<48:17,  5.19s/it] 11%|█         | 67/624 [05:52<48:29,  5.22s/it]                                                {'loss': 1.5407, 'grad_norm': 4.848803520202637, 'learning_rate': 1.9690976216812397e-05, 'epoch': 0.32}
 11%|█         | 67/624 [05:52<48:29,  5.22s/it] 11%|█         | 68/624 [05:58<49:28,  5.34s/it]                                                {'loss': 1.6058, 'grad_norm': 4.985437393188477, 'learning_rate': 1.9678036358537726e-05, 'epoch': 0.33}
 11%|█         | 68/624 [05:58<49:28,  5.34s/it] 11%|█         | 69/624 [06:03<49:47,  5.38s/it]                                                {'loss': 1.6164, 'grad_norm': 4.556461334228516, 'learning_rate': 1.966483553946637e-05, 'epoch': 0.33}
 11%|█         | 69/624 [06:03<49:47,  5.38s/it] 11%|█         | 70/624 [06:08<49:05,  5.32s/it]                                                {'loss': 1.5796, 'grad_norm': 4.75994873046875, 'learning_rate': 1.9651374115548255e-05, 'epoch': 0.34}
 11%|█         | 70/624 [06:08<49:05,  5.32s/it] 11%|█▏        | 71/624 [06:13<48:13,  5.23s/it]                                                {'loss': 1.4699, 'grad_norm': 4.729470252990723, 'learning_rate': 1.9637652449760297e-05, 'epoch': 0.34}
 11%|█▏        | 71/624 [06:13<48:13,  5.23s/it] 12%|█▏        | 72/624 [06:18<47:14,  5.13s/it]                                                {'loss': 1.8403, 'grad_norm': 4.393382549285889, 'learning_rate': 1.9623670912096656e-05, 'epoch': 0.35}
 12%|█▏        | 72/624 [06:18<47:14,  5.13s/it] 12%|█▏        | 73/624 [06:23<46:38,  5.08s/it]                                                {'loss': 1.4971, 'grad_norm': 3.8686740398406982, 'learning_rate': 1.9609429879558726e-05, 'epoch': 0.35}
 12%|█▏        | 73/624 [06:23<46:38,  5.08s/it] 12%|█▏        | 74/624 [06:29<47:31,  5.18s/it]                                                {'loss': 1.6907, 'grad_norm': 3.988621711730957, 'learning_rate': 1.9594929736144978e-05, 'epoch': 0.35}
 12%|█▏        | 74/624 [06:29<47:31,  5.18s/it] 12%|█▏        | 75/624 [06:34<47:53,  5.23s/it]                                                {'loss': 1.6669, 'grad_norm': 5.150786876678467, 'learning_rate': 1.958017087284061e-05, 'epoch': 0.36}
 12%|█▏        | 75/624 [06:34<47:53,  5.23s/it] 12%|█▏        | 76/624 [06:39<47:45,  5.23s/it]                                                {'loss': 1.5693, 'grad_norm': 4.566040992736816, 'learning_rate': 1.9565153687607006e-05, 'epoch': 0.36}
 12%|█▏        | 76/624 [06:39<47:45,  5.23s/it] 12%|█▏        | 77/624 [06:44<46:40,  5.12s/it]                                                {'loss': 1.5185, 'grad_norm': 5.207881927490234, 'learning_rate': 1.9549878585371006e-05, 'epoch': 0.37}
 12%|█▏        | 77/624 [06:44<46:40,  5.12s/it] 12%|█▎        | 78/624 [06:50<48:20,  5.31s/it]                                                {'loss': 1.5608, 'grad_norm': 5.463085651397705, 'learning_rate': 1.9534345978013972e-05, 'epoch': 0.37}
 12%|█▎        | 78/624 [06:50<48:20,  5.31s/it] 13%|█▎        | 79/624 [06:55<47:14,  5.20s/it]                                                {'loss': 1.6901, 'grad_norm': 4.828293323516846, 'learning_rate': 1.9518556284360696e-05, 'epoch': 0.38}
 13%|█▎        | 79/624 [06:55<47:14,  5.20s/it] 13%|█▎        | 80/624 [07:00<47:16,  5.21s/it]                                                {'loss': 1.3848, 'grad_norm': 6.206188201904297, 'learning_rate': 1.9502509930168113e-05, 'epoch': 0.38}
 13%|█▎        | 80/624 [07:00<47:16,  5.21s/it] 13%|█▎        | 81/624 [07:05<47:17,  5.23s/it]                                                {'loss': 1.5532, 'grad_norm': 7.258210182189941, 'learning_rate': 1.9486207348113803e-05, 'epoch': 0.39}
 13%|█▎        | 81/624 [07:05<47:17,  5.23s/it] 13%|█▎        | 82/624 [07:11<47:15,  5.23s/it]                                                {'loss': 1.5573, 'grad_norm': 5.768157005310059, 'learning_rate': 1.946964897778433e-05, 'epoch': 0.39}
 13%|█▎        | 82/624 [07:11<47:15,  5.23s/it] 13%|█▎        | 83/624 [07:15<46:22,  5.14s/it]                                                {'loss': 1.41, 'grad_norm': 5.046070575714111, 'learning_rate': 1.9452835265663404e-05, 'epoch': 0.4}
 13%|█▎        | 83/624 [07:16<46:22,  5.14s/it] 13%|█▎        | 84/624 [07:21<46:46,  5.20s/it]                                                {'loss': 1.6999, 'grad_norm': 4.594936370849609, 'learning_rate': 1.9435766665119823e-05, 'epoch': 0.4}
 13%|█▎        | 84/624 [07:21<46:46,  5.20s/it] 14%|█▎        | 85/624 [07:26<47:31,  5.29s/it]                                                {'loss': 1.5293, 'grad_norm': 5.056858062744141, 'learning_rate': 1.941844363639525e-05, 'epoch': 0.41}
 14%|█▎        | 85/624 [07:26<47:31,  5.29s/it] 14%|█▍        | 86/624 [07:31<47:13,  5.27s/it]                                                {'loss': 1.4125, 'grad_norm': 4.228383541107178, 'learning_rate': 1.9400866646591816e-05, 'epoch': 0.41}
 14%|█▍        | 86/624 [07:32<47:13,  5.27s/it] 14%|█▍        | 87/624 [07:37<46:42,  5.22s/it]                                                {'loss': 1.5907, 'grad_norm': 4.7683281898498535, 'learning_rate': 1.9383036169659513e-05, 'epoch': 0.42}
 14%|█▍        | 87/624 [07:37<46:42,  5.22s/it] 14%|█▍        | 88/624 [07:42<46:28,  5.20s/it]                                                {'loss': 1.4276, 'grad_norm': 4.96159029006958, 'learning_rate': 1.936495268638342e-05, 'epoch': 0.42}
 14%|█▍        | 88/624 [07:42<46:28,  5.20s/it] 14%|█▍        | 89/624 [07:47<45:43,  5.13s/it]                                                {'loss': 1.3006, 'grad_norm': 4.679100036621094, 'learning_rate': 1.934661668437073e-05, 'epoch': 0.43}
 14%|█▍        | 89/624 [07:47<45:43,  5.13s/it] 14%|█▍        | 90/624 [07:52<45:48,  5.15s/it]                                                {'loss': 1.5161, 'grad_norm': 5.001574993133545, 'learning_rate': 1.932802865803763e-05, 'epoch': 0.43}
 14%|█▍        | 90/624 [07:52<45:48,  5.15s/it] 15%|█▍        | 91/624 [07:57<46:37,  5.25s/it]                                                {'loss': 1.6053, 'grad_norm': 5.108554363250732, 'learning_rate': 1.930918910859592e-05, 'epoch': 0.44}
 15%|█▍        | 91/624 [07:58<46:37,  5.25s/it] 15%|█▍        | 92/624 [08:02<45:12,  5.10s/it]                                                {'loss': 1.6292, 'grad_norm': 4.6543097496032715, 'learning_rate': 1.9290098544039546e-05, 'epoch': 0.44}
 15%|█▍        | 92/624 [08:02<45:12,  5.10s/it] 15%|█▍        | 93/624 [08:07<45:26,  5.13s/it]                                                {'loss': 1.6005, 'grad_norm': 4.930142402648926, 'learning_rate': 1.927075747913088e-05, 'epoch': 0.45}
 15%|█▍        | 93/624 [08:07<45:26,  5.13s/it] 15%|█▌        | 94/624 [08:13<45:30,  5.15s/it]                                                {'loss': 1.6048, 'grad_norm': 4.741326332092285, 'learning_rate': 1.9251166435386837e-05, 'epoch': 0.45}
 15%|█▌        | 94/624 [08:13<45:30,  5.15s/it] 15%|█▌        | 95/624 [08:18<45:44,  5.19s/it]                                                {'loss': 1.5818, 'grad_norm': 4.9415507316589355, 'learning_rate': 1.923132594106483e-05, 'epoch': 0.46}
 15%|█▌        | 95/624 [08:18<45:44,  5.19s/it] 15%|█▌        | 96/624 [08:23<45:33,  5.18s/it]                                                {'loss': 1.7788, 'grad_norm': 3.9959371089935303, 'learning_rate': 1.92112365311485e-05, 'epoch': 0.46}
 15%|█▌        | 96/624 [08:23<45:33,  5.18s/it] 16%|█▌        | 97/624 [08:28<45:31,  5.18s/it]                                                {'loss': 1.7356, 'grad_norm': 5.491238117218018, 'learning_rate': 1.919089874733332e-05, 'epoch': 0.47}
 16%|█▌        | 97/624 [08:28<45:31,  5.18s/it] 16%|█▌        | 98/624 [08:33<44:53,  5.12s/it]                                                {'loss': 1.7595, 'grad_norm': 4.673900604248047, 'learning_rate': 1.9170313138011964e-05, 'epoch': 0.47}
 16%|█▌        | 98/624 [08:33<44:53,  5.12s/it] 16%|█▌        | 99/624 [08:39<45:26,  5.19s/it]                                                {'loss': 1.7251, 'grad_norm': 4.904769420623779, 'learning_rate': 1.9149480258259535e-05, 'epoch': 0.47}
 16%|█▌        | 99/624 [08:39<45:26,  5.19s/it] 16%|█▌        | 100/624 [08:44<45:55,  5.26s/it]                                                 {'loss': 1.4248, 'grad_norm': 5.3506693840026855, 'learning_rate': 1.9128400669818586e-05, 'epoch': 0.48}
 16%|█▌        | 100/624 [08:44<45:55,  5.26s/it] 16%|█▌        | 101/624 [08:49<45:21,  5.20s/it]                                                 {'loss': 1.4453, 'grad_norm': 4.67794132232666, 'learning_rate': 1.9107074941083987e-05, 'epoch': 0.48}
 16%|█▌        | 101/624 [08:49<45:21,  5.20s/it] 16%|█▋        | 102/624 [08:54<45:15,  5.20s/it]                                                 {'loss': 1.8222, 'grad_norm': 5.258531093597412, 'learning_rate': 1.9085503647087588e-05, 'epoch': 0.49}
 16%|█▋        | 102/624 [08:54<45:15,  5.20s/it] 17%|█▋        | 103/624 [08:59<44:29,  5.12s/it]                                                 {'loss': 1.5464, 'grad_norm': 8.042071342468262, 'learning_rate': 1.906368736948272e-05, 'epoch': 0.49}
 17%|█▋        | 103/624 [08:59<44:29,  5.12s/it] 17%|█▋        | 104/624 [09:04<44:54,  5.18s/it]                                                 {'loss': 1.5767, 'grad_norm': 4.4706339836120605, 'learning_rate': 1.9041626696528503e-05, 'epoch': 0.5}
 17%|█▋        | 104/624 [09:05<44:54,  5.18s/it] 17%|█▋        | 105/624 [09:10<45:15,  5.23s/it]                                                 {'loss': 1.3838, 'grad_norm': 4.469551086425781, 'learning_rate': 1.9019322223073997e-05, 'epoch': 0.5}
 17%|█▋        | 105/624 [09:10<45:15,  5.23s/it] 17%|█▋        | 106/624 [09:15<45:21,  5.25s/it]                                                 {'loss': 1.5141, 'grad_norm': 5.584957599639893, 'learning_rate': 1.899677455054215e-05, 'epoch': 0.51}
 17%|█▋        | 106/624 [09:15<45:21,  5.25s/it] 17%|█▋        | 107/624 [09:20<45:29,  5.28s/it]                                                 {'loss': 1.4752, 'grad_norm': 4.225976943969727, 'learning_rate': 1.8973984286913584e-05, 'epoch': 0.51}
 17%|█▋        | 107/624 [09:21<45:29,  5.28s/it] 17%|█▋        | 108/624 [09:26<45:41,  5.31s/it]                                                 {'loss': 1.5739, 'grad_norm': 6.238699913024902, 'learning_rate': 1.895095204671021e-05, 'epoch': 0.52}
 17%|█▋        | 108/624 [09:26<45:41,  5.31s/it] 17%|█▋        | 109/624 [09:31<45:27,  5.30s/it]                                                 {'loss': 1.633, 'grad_norm': 6.395109176635742, 'learning_rate': 1.892767845097864e-05, 'epoch': 0.52}
 17%|█▋        | 109/624 [09:31<45:27,  5.30s/it] 18%|█▊        | 110/624 [09:36<45:21,  5.29s/it]                                                 {'loss': 1.7025, 'grad_norm': 4.525211334228516, 'learning_rate': 1.890416412727346e-05, 'epoch': 0.53}
 18%|█▊        | 110/624 [09:37<45:21,  5.29s/it] 18%|█▊        | 111/624 [09:42<45:43,  5.35s/it]                                                 {'loss': 1.433, 'grad_norm': 5.383917808532715, 'learning_rate': 1.88804097096403e-05, 'epoch': 0.53}
 18%|█▊        | 111/624 [09:42<45:43,  5.35s/it] 18%|█▊        | 112/624 [09:47<45:28,  5.33s/it]                                                 {'loss': 1.4653, 'grad_norm': 4.906514644622803, 'learning_rate': 1.8856415838598738e-05, 'epoch': 0.54}
 18%|█▊        | 112/624 [09:47<45:28,  5.33s/it] 18%|█▊        | 113/624 [09:53<45:40,  5.36s/it]                                                 {'loss': 1.6242, 'grad_norm': 5.432185173034668, 'learning_rate': 1.8832183161125026e-05, 'epoch': 0.54}
 18%|█▊        | 113/624 [09:53<45:40,  5.36s/it] 18%|█▊        | 114/624 [09:58<45:37,  5.37s/it]                                                 {'loss': 1.7141, 'grad_norm': 3.7697913646698, 'learning_rate': 1.8807712330634645e-05, 'epoch': 0.55}
 18%|█▊        | 114/624 [09:58<45:37,  5.37s/it] 18%|█▊        | 115/624 [10:03<44:22,  5.23s/it]                                                 {'loss': 1.5802, 'grad_norm': 4.821393013000488, 'learning_rate': 1.87830040069647e-05, 'epoch': 0.55}
 18%|█▊        | 115/624 [10:03<44:22,  5.23s/it] 19%|█▊        | 116/624 [10:08<44:33,  5.26s/it]                                                 {'loss': 1.7838, 'grad_norm': 4.491559028625488, 'learning_rate': 1.87580588563561e-05, 'epoch': 0.56}
 19%|█▊        | 116/624 [10:08<44:33,  5.26s/it] 19%|█▉        | 117/624 [10:13<44:18,  5.24s/it]                                                 {'loss': 1.6241, 'grad_norm': 4.1144890785217285, 'learning_rate': 1.873287755143563e-05, 'epoch': 0.56}
 19%|█▉        | 117/624 [10:14<44:18,  5.24s/it] 19%|█▉        | 118/624 [10:19<44:08,  5.23s/it]                                                 {'loss': 1.5226, 'grad_norm': 4.240283489227295, 'learning_rate': 1.8707460771197773e-05, 'epoch': 0.57}
 19%|█▉        | 118/624 [10:19<44:08,  5.23s/it] 19%|█▉        | 119/624 [10:24<44:35,  5.30s/it]                                                 {'loss': 1.4897, 'grad_norm': 4.954909801483154, 'learning_rate': 1.868180920098644e-05, 'epoch': 0.57}
 19%|█▉        | 119/624 [10:24<44:35,  5.30s/it] 19%|█▉        | 120/624 [10:29<43:26,  5.17s/it]                                                 {'loss': 1.5658, 'grad_norm': 5.8972015380859375, 'learning_rate': 1.8655923532476463e-05, 'epoch': 0.58}
 19%|█▉        | 120/624 [10:29<43:26,  5.17s/it] 19%|█▉        | 121/624 [10:34<43:29,  5.19s/it]                                                 {'loss': 1.6522, 'grad_norm': 4.227372169494629, 'learning_rate': 1.8629804463654956e-05, 'epoch': 0.58}
 19%|█▉        | 121/624 [10:34<43:29,  5.19s/it] 20%|█▉        | 122/624 [10:39<43:04,  5.15s/it]                                                 {'loss': 1.57, 'grad_norm': 3.8576276302337646, 'learning_rate': 1.8603452698802498e-05, 'epoch': 0.59}
 20%|█▉        | 122/624 [10:39<43:04,  5.15s/it] 20%|█▉        | 123/624 [10:44<42:48,  5.13s/it]                                                 {'loss': 1.4556, 'grad_norm': 4.436267852783203, 'learning_rate': 1.857686894847413e-05, 'epoch': 0.59}
 20%|█▉        | 123/624 [10:44<42:48,  5.13s/it] 20%|█▉        | 124/624 [10:50<43:29,  5.22s/it]                                                 {'loss': 1.597, 'grad_norm': 6.779890060424805, 'learning_rate': 1.8550053929480202e-05, 'epoch': 0.59}
 20%|█▉        | 124/624 [10:50<43:29,  5.22s/it] 20%|██        | 125/624 [10:55<43:04,  5.18s/it]                                                 {'loss': 1.5316, 'grad_norm': 4.134687423706055, 'learning_rate': 1.8523008364867056e-05, 'epoch': 0.6}
 20%|██        | 125/624 [10:55<43:04,  5.18s/it] 20%|██        | 126/624 [11:00<43:53,  5.29s/it]                                                 {'loss': 1.6245, 'grad_norm': 4.441580772399902, 'learning_rate': 1.8495732983897504e-05, 'epoch': 0.6}
 20%|██        | 126/624 [11:01<43:53,  5.29s/it] 20%|██        | 127/624 [11:06<43:31,  5.26s/it]                                                 {'loss': 1.5323, 'grad_norm': 4.561964988708496, 'learning_rate': 1.8468228522031197e-05, 'epoch': 0.61}
 20%|██        | 127/624 [11:06<43:31,  5.26s/it] 21%|██        | 128/624 [11:11<43:43,  5.29s/it]                                                 {'loss': 1.709, 'grad_norm': 5.496191501617432, 'learning_rate': 1.8440495720904758e-05, 'epoch': 0.61}
 21%|██        | 128/624 [11:11<43:43,  5.29s/it] 21%|██        | 129/624 [11:16<43:55,  5.32s/it]                                                 {'loss': 1.5183, 'grad_norm': 4.231996059417725, 'learning_rate': 1.8412535328311813e-05, 'epoch': 0.62}
 21%|██        | 129/624 [11:16<43:55,  5.32s/it] 21%|██        | 130/624 [11:22<43:49,  5.32s/it]                                                 {'loss': 1.5364, 'grad_norm': 4.511932849884033, 'learning_rate': 1.8384348098182815e-05, 'epoch': 0.62}
 21%|██        | 130/624 [11:22<43:49,  5.32s/it] 21%|██        | 131/624 [11:27<43:04,  5.24s/it]                                                 {'loss': 1.6161, 'grad_norm': 5.088165760040283, 'learning_rate': 1.8355934790564718e-05, 'epoch': 0.63}
 21%|██        | 131/624 [11:27<43:04,  5.24s/it] 21%|██        | 132/624 [11:32<41:57,  5.12s/it]                                                 {'loss': 1.5479, 'grad_norm': 4.815494537353516, 'learning_rate': 1.832729617160047e-05, 'epoch': 0.63}
 21%|██        | 132/624 [11:32<41:57,  5.12s/it] 21%|██▏       | 133/624 [11:36<41:21,  5.05s/it]                                                 {'loss': 1.6469, 'grad_norm': 4.339644432067871, 'learning_rate': 1.8298433013508384e-05, 'epoch': 0.64}
 21%|██▏       | 133/624 [11:37<41:21,  5.05s/it] 21%|██▏       | 134/624 [11:42<42:15,  5.17s/it]                                                 {'loss': 1.5402, 'grad_norm': 5.10679817199707, 'learning_rate': 1.826934609456129e-05, 'epoch': 0.64}
 21%|██▏       | 134/624 [11:42<42:15,  5.17s/it] 22%|██▏       | 135/624 [11:47<42:17,  5.19s/it]                                                 {'loss': 1.7824, 'grad_norm': 4.332094192504883, 'learning_rate': 1.8240036199065546e-05, 'epoch': 0.65}
 22%|██▏       | 135/624 [11:47<42:17,  5.19s/it] 22%|██▏       | 136/624 [11:52<42:18,  5.20s/it]                                                 {'loss': 1.5348, 'grad_norm': 4.2459330558776855, 'learning_rate': 1.8210504117339917e-05, 'epoch': 0.65}
 22%|██▏       | 136/624 [11:52<42:18,  5.20s/it] 22%|██▏       | 137/624 [11:58<42:12,  5.20s/it]                                                 {'loss': 1.6666, 'grad_norm': 4.304036617279053, 'learning_rate': 1.8180750645694236e-05, 'epoch': 0.66}
 22%|██▏       | 137/624 [11:58<42:12,  5.20s/it] 22%|██▏       | 138/624 [12:03<41:48,  5.16s/it]                                                 {'loss': 1.7321, 'grad_norm': 8.13010025024414, 'learning_rate': 1.8150776586407957e-05, 'epoch': 0.66}
 22%|██▏       | 138/624 [12:03<41:48,  5.16s/it] 22%|██▏       | 139/624 [12:08<42:15,  5.23s/it]                                                 {'loss': 1.4917, 'grad_norm': 5.17850923538208, 'learning_rate': 1.8120582747708503e-05, 'epoch': 0.67}
 22%|██▏       | 139/624 [12:08<42:15,  5.23s/it] 22%|██▏       | 140/624 [12:13<42:33,  5.28s/it]                                                 {'loss': 1.5945, 'grad_norm': 5.485180377960205, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.67}
 22%|██▏       | 140/624 [12:14<42:33,  5.28s/it] 23%|██▎       | 141/624 [12:18<41:58,  5.21s/it]                                                 {'loss': 1.6107, 'grad_norm': 5.868346691131592, 'learning_rate': 1.8059538994588715e-05, 'epoch': 0.68}
 23%|██▎       | 141/624 [12:19<41:58,  5.21s/it] 23%|██▎       | 142/624 [12:24<42:15,  5.26s/it]                                                 {'loss': 1.6008, 'grad_norm': 4.778099060058594, 'learning_rate': 1.8028690726166172e-05, 'epoch': 0.68}
 23%|██▎       | 142/624 [12:24<42:15,  5.26s/it] 23%|██▎       | 143/624 [12:29<42:39,  5.32s/it]                                                 {'loss': 1.6711, 'grad_norm': 5.589781284332275, 'learning_rate': 1.7997625970281652e-05, 'epoch': 0.69}
 23%|██▎       | 143/624 [12:29<42:39,  5.32s/it] 23%|██▎       | 144/624 [12:34<41:17,  5.16s/it]                                                 {'loss': 1.6794, 'grad_norm': 5.459109306335449, 'learning_rate': 1.796634556457236e-05, 'epoch': 0.69}
 23%|██▎       | 144/624 [12:34<41:17,  5.16s/it] 23%|██▎       | 145/624 [12:39<41:38,  5.22s/it]                                                 {'loss': 1.1962, 'grad_norm': 4.970653057098389, 'learning_rate': 1.793485035249036e-05, 'epoch': 0.7}
 23%|██▎       | 145/624 [12:40<41:38,  5.22s/it] 23%|██▎       | 146/624 [12:45<41:47,  5.25s/it]                                                 {'loss': 1.4782, 'grad_norm': 4.569654941558838, 'learning_rate': 1.7903141183279776e-05, 'epoch': 0.7}
 23%|██▎       | 146/624 [12:45<41:47,  5.25s/it] 24%|██▎       | 147/624 [12:50<41:45,  5.25s/it]                                                 {'loss': 1.5687, 'grad_norm': 5.310946941375732, 'learning_rate': 1.7871218911953942e-05, 'epoch': 0.71}
 24%|██▎       | 147/624 [12:50<41:45,  5.25s/it] 24%|██▎       | 148/624 [12:55<41:50,  5.27s/it]                                                 {'loss': 1.624, 'grad_norm': 4.196695804595947, 'learning_rate': 1.7839084399272317e-05, 'epoch': 0.71}
 24%|██▎       | 148/624 [12:55<41:50,  5.27s/it] 24%|██▍       | 149/624 [13:00<41:10,  5.20s/it]                                                 {'loss': 1.7735, 'grad_norm': 5.409011363983154, 'learning_rate': 1.780673851171728e-05, 'epoch': 0.71}
 24%|██▍       | 149/624 [13:00<41:10,  5.20s/it] 24%|██▍       | 150/624 [13:06<41:03,  5.20s/it]                                                 {'loss': 1.7647, 'grad_norm': 5.938230037689209, 'learning_rate': 1.777418212147079e-05, 'epoch': 0.72}
 24%|██▍       | 150/624 [13:06<41:03,  5.20s/it] 24%|██▍       | 151/624 [13:11<40:50,  5.18s/it]                                                 {'loss': 1.5722, 'grad_norm': 5.067278861999512, 'learning_rate': 1.7741416106390828e-05, 'epoch': 0.72}
 24%|██▍       | 151/624 [13:11<40:50,  5.18s/it] 24%|██▍       | 152/624 [13:16<40:19,  5.13s/it]                                                 {'loss': 1.5068, 'grad_norm': 5.255152225494385, 'learning_rate': 1.7708441349987753e-05, 'epoch': 0.73}
 24%|██▍       | 152/624 [13:16<40:19,  5.13s/it] 25%|██▍       | 153/624 [13:21<40:39,  5.18s/it]                                                 {'loss': 1.6288, 'grad_norm': 4.534934997558594, 'learning_rate': 1.767525874140048e-05, 'epoch': 0.73}
 25%|██▍       | 153/624 [13:21<40:39,  5.18s/it] 25%|██▍       | 154/624 [13:26<40:40,  5.19s/it]                                                 {'loss': 1.6833, 'grad_norm': 4.313183307647705, 'learning_rate': 1.7641869175372493e-05, 'epoch': 0.74}
 25%|██▍       | 154/624 [13:26<40:40,  5.19s/it] 25%|██▍       | 155/624 [13:31<40:46,  5.22s/it]                                                 {'loss': 1.475, 'grad_norm': 11.900030136108398, 'learning_rate': 1.7608273552227723e-05, 'epoch': 0.74}
 25%|██▍       | 155/624 [13:32<40:46,  5.22s/it] 25%|██▌       | 156/624 [13:37<40:48,  5.23s/it]                                                 {'loss': 1.5225, 'grad_norm': 5.566695213317871, 'learning_rate': 1.7574472777846276e-05, 'epoch': 0.75}
 25%|██▌       | 156/624 [13:37<40:48,  5.23s/it] 25%|██▌       | 157/624 [13:42<40:37,  5.22s/it]                                                 {'loss': 1.7286, 'grad_norm': 5.115540504455566, 'learning_rate': 1.7540467763639994e-05, 'epoch': 0.75}
 25%|██▌       | 157/624 [13:42<40:37,  5.22s/it] 25%|██▌       | 158/624 [13:47<39:50,  5.13s/it]                                                 {'loss': 1.4397, 'grad_norm': 6.3475661277771, 'learning_rate': 1.7506259426527903e-05, 'epoch': 0.76}
 25%|██▌       | 158/624 [13:47<39:50,  5.13s/it] 25%|██▌       | 159/624 [13:52<40:10,  5.18s/it]                                                 {'loss': 1.7117, 'grad_norm': 6.793311595916748, 'learning_rate': 1.7471848688911465e-05, 'epoch': 0.76}
 25%|██▌       | 159/624 [13:52<40:10,  5.18s/it] 26%|██▌       | 160/624 [13:57<39:09,  5.06s/it]                                                 {'loss': 1.6769, 'grad_norm': 7.012084484100342, 'learning_rate': 1.7437236478649718e-05, 'epoch': 0.77}
 26%|██▌       | 160/624 [13:57<39:09,  5.06s/it] 26%|██▌       | 161/624 [14:02<39:06,  5.07s/it]                                                 {'loss': 1.4773, 'grad_norm': 4.563937664031982, 'learning_rate': 1.7402423729034252e-05, 'epoch': 0.77}
 26%|██▌       | 161/624 [14:02<39:06,  5.07s/it] 26%|██▌       | 162/624 [14:07<39:44,  5.16s/it]                                                 {'loss': 1.6134, 'grad_norm': 4.1062164306640625, 'learning_rate': 1.736741137876405e-05, 'epoch': 0.78}
 26%|██▌       | 162/624 [14:08<39:44,  5.16s/it] 26%|██▌       | 163/624 [14:13<41:16,  5.37s/it]                                                 {'loss': 1.5416, 'grad_norm': 7.648624420166016, 'learning_rate': 1.7332200371920173e-05, 'epoch': 0.78}
 26%|██▌       | 163/624 [14:13<41:16,  5.37s/it] 26%|██▋       | 164/624 [14:19<41:57,  5.47s/it]                                                 {'loss': 1.6954, 'grad_norm': 4.30778694152832, 'learning_rate': 1.72967916579403e-05, 'epoch': 0.79}
 26%|██▋       | 164/624 [14:19<41:57,  5.47s/it] 26%|██▋       | 165/624 [14:24<41:09,  5.38s/it]                                                 {'loss': 1.4346, 'grad_norm': 5.422671318054199, 'learning_rate': 1.7261186191593135e-05, 'epoch': 0.79}
 26%|██▋       | 165/624 [14:24<41:09,  5.38s/it] 27%|██▋       | 166/624 [14:29<40:24,  5.29s/it]                                                 {'loss': 1.5216, 'grad_norm': 4.768054008483887, 'learning_rate': 1.7225384932952655e-05, 'epoch': 0.8}
 27%|██▋       | 166/624 [14:29<40:24,  5.29s/it] 27%|██▋       | 167/624 [14:35<40:27,  5.31s/it]                                                 {'loss': 1.4851, 'grad_norm': 4.253408432006836, 'learning_rate': 1.7189388847372227e-05, 'epoch': 0.8}
 27%|██▋       | 167/624 [14:35<40:27,  5.31s/it] 27%|██▋       | 168/624 [14:40<40:12,  5.29s/it]                                                 {'loss': 1.6831, 'grad_norm': 5.019163131713867, 'learning_rate': 1.715319890545857e-05, 'epoch': 0.81}
 27%|██▋       | 168/624 [14:40<40:12,  5.29s/it] 27%|██▋       | 169/624 [14:45<40:04,  5.28s/it]                                                 {'loss': 1.5735, 'grad_norm': 4.726763725280762, 'learning_rate': 1.7116816083045603e-05, 'epoch': 0.81}
 27%|██▋       | 169/624 [14:45<40:04,  5.28s/it] 27%|██▋       | 170/624 [14:50<39:38,  5.24s/it]                                                 {'loss': 1.5688, 'grad_norm': 4.975773334503174, 'learning_rate': 1.7080241361168108e-05, 'epoch': 0.82}
 27%|██▋       | 170/624 [14:50<39:38,  5.24s/it] 27%|██▋       | 171/624 [14:56<39:59,  5.30s/it]                                                 {'loss': 1.5544, 'grad_norm': 3.738929033279419, 'learning_rate': 1.704347572603529e-05, 'epoch': 0.82}
 27%|██▋       | 171/624 [14:56<39:59,  5.30s/it] 28%|██▊       | 172/624 [15:01<39:57,  5.30s/it]                                                 {'loss': 1.8171, 'grad_norm': 4.373273849487305, 'learning_rate': 1.700652016900419e-05, 'epoch': 0.82}
 28%|██▊       | 172/624 [15:01<39:57,  5.30s/it] 28%|██▊       | 173/624 [15:06<40:05,  5.33s/it]                                                 {'loss': 1.6367, 'grad_norm': 5.255134105682373, 'learning_rate': 1.696937568655294e-05, 'epoch': 0.83}
 28%|██▊       | 173/624 [15:07<40:05,  5.33s/it] 28%|██▊       | 174/624 [15:12<39:37,  5.28s/it]                                                 {'loss': 1.438, 'grad_norm': 5.365942001342773, 'learning_rate': 1.6932043280253892e-05, 'epoch': 0.83}
 28%|██▊       | 174/624 [15:12<39:37,  5.28s/it] 28%|██▊       | 175/624 [15:16<38:44,  5.18s/it]                                                 {'loss': 1.6308, 'grad_norm': 3.9600064754486084, 'learning_rate': 1.689452395674664e-05, 'epoch': 0.84}
 28%|██▊       | 175/624 [15:17<38:44,  5.18s/it] 28%|██▊       | 176/624 [15:22<39:06,  5.24s/it]                                                 {'loss': 1.5734, 'grad_norm': 3.9960520267486572, 'learning_rate': 1.6856818727710847e-05, 'epoch': 0.84}
 28%|██▊       | 176/624 [15:22<39:06,  5.24s/it] 28%|██▊       | 177/624 [15:27<39:07,  5.25s/it]                                                 {'loss': 1.4414, 'grad_norm': 6.078994274139404, 'learning_rate': 1.6818928609838967e-05, 'epoch': 0.85}
 28%|██▊       | 177/624 [15:27<39:07,  5.25s/it] 29%|██▊       | 178/624 [15:33<39:16,  5.28s/it]                                                 {'loss': 1.6399, 'grad_norm': 5.101942539215088, 'learning_rate': 1.678085462480885e-05, 'epoch': 0.85}
 29%|██▊       | 178/624 [15:33<39:16,  5.28s/it] 29%|██▊       | 179/624 [15:38<38:34,  5.20s/it]                                                 {'loss': 1.4387, 'grad_norm': 4.9026103019714355, 'learning_rate': 1.6742597799256182e-05, 'epoch': 0.86}
 29%|██▊       | 179/624 [15:38<38:34,  5.20s/it] 29%|██▉       | 180/624 [15:43<38:39,  5.22s/it]                                                 {'loss': 1.5269, 'grad_norm': 4.8868489265441895, 'learning_rate': 1.6704159164746797e-05, 'epoch': 0.86}
 29%|██▉       | 180/624 [15:43<38:39,  5.22s/it] 29%|██▉       | 181/624 [15:48<38:17,  5.19s/it]                                                 {'loss': 1.6416, 'grad_norm': 4.787051677703857, 'learning_rate': 1.6665539757748866e-05, 'epoch': 0.87}
 29%|██▉       | 181/624 [15:48<38:17,  5.19s/it] 29%|██▉       | 182/624 [15:53<38:24,  5.21s/it]                                                 {'loss': 1.591, 'grad_norm': 3.8208630084991455, 'learning_rate': 1.6626740619604967e-05, 'epoch': 0.87}
 29%|██▉       | 182/624 [15:53<38:24,  5.21s/it] 29%|██▉       | 183/624 [15:58<38:13,  5.20s/it]                                                 {'loss': 1.777, 'grad_norm': 4.552206039428711, 'learning_rate': 1.658776279650397e-05, 'epoch': 0.88}
 29%|██▉       | 183/624 [15:58<38:13,  5.20s/it] 29%|██▉       | 184/624 [16:04<38:53,  5.30s/it]                                                 {'loss': 1.4634, 'grad_norm': 3.962655782699585, 'learning_rate': 1.6548607339452853e-05, 'epoch': 0.88}
 29%|██▉       | 184/624 [16:04<38:53,  5.30s/it] 30%|██▉       | 185/624 [16:09<38:53,  5.32s/it]                                                 {'loss': 1.5436, 'grad_norm': 5.143273830413818, 'learning_rate': 1.6509275304248366e-05, 'epoch': 0.89}
 30%|██▉       | 185/624 [16:09<38:53,  5.32s/it] 30%|██▉       | 186/624 [16:15<38:54,  5.33s/it]                                                 {'loss': 1.6425, 'grad_norm': 5.2025885581970215, 'learning_rate': 1.6469767751448538e-05, 'epoch': 0.89}
 30%|██▉       | 186/624 [16:15<38:54,  5.33s/it] 30%|██▉       | 187/624 [16:19<37:47,  5.19s/it]                                                 {'loss': 1.4911, 'grad_norm': 4.601143836975098, 'learning_rate': 1.6430085746344107e-05, 'epoch': 0.9}
 30%|██▉       | 187/624 [16:20<37:47,  5.19s/it] 30%|███       | 188/624 [16:25<37:46,  5.20s/it]                                                 {'loss': 1.5747, 'grad_norm': 4.609692573547363, 'learning_rate': 1.639023035892978e-05, 'epoch': 0.9}
 30%|███       | 188/624 [16:25<37:46,  5.20s/it] 30%|███       | 189/624 [16:30<37:12,  5.13s/it]                                                 {'loss': 1.5748, 'grad_norm': 5.104434013366699, 'learning_rate': 1.6350202663875385e-05, 'epoch': 0.91}
 30%|███       | 189/624 [16:30<37:12,  5.13s/it] 30%|███       | 190/624 [16:35<37:40,  5.21s/it]                                                 {'loss': 1.487, 'grad_norm': 4.7753801345825195, 'learning_rate': 1.6310003740496887e-05, 'epoch': 0.91}
 30%|███       | 190/624 [16:35<37:40,  5.21s/it] 31%|███       | 191/624 [16:40<37:46,  5.23s/it]                                                 {'loss': 1.7133, 'grad_norm': 4.622531890869141, 'learning_rate': 1.6269634672727296e-05, 'epoch': 0.92}
 31%|███       | 191/624 [16:40<37:46,  5.23s/it] 31%|███       | 192/624 [16:46<37:47,  5.25s/it]                                                 {'loss': 1.3052, 'grad_norm': 4.053213596343994, 'learning_rate': 1.6229096549087434e-05, 'epoch': 0.92}
 31%|███       | 192/624 [16:46<37:47,  5.25s/it] 31%|███       | 193/624 [16:51<37:46,  5.26s/it]                                                 {'loss': 1.5597, 'grad_norm': 4.837195873260498, 'learning_rate': 1.618839046265658e-05, 'epoch': 0.93}
 31%|███       | 193/624 [16:51<37:46,  5.26s/it] 31%|███       | 194/624 [16:56<37:27,  5.23s/it]                                                 {'loss': 1.6589, 'grad_norm': 4.198485374450684, 'learning_rate': 1.614751751104301e-05, 'epoch': 0.93}
 31%|███       | 194/624 [16:56<37:27,  5.23s/it] 31%|███▏      | 195/624 [17:01<37:50,  5.29s/it]                                                 {'loss': 1.624, 'grad_norm': 4.516870975494385, 'learning_rate': 1.6106478796354382e-05, 'epoch': 0.94}
 31%|███▏      | 195/624 [17:02<37:50,  5.29s/it] 31%|███▏      | 196/624 [17:07<37:28,  5.25s/it]                                                 {'loss': 1.5952, 'grad_norm': 3.9999890327453613, 'learning_rate': 1.6065275425168034e-05, 'epoch': 0.94}
 31%|███▏      | 196/624 [17:07<37:28,  5.25s/it] 32%|███▏      | 197/624 [17:12<37:32,  5.27s/it]                                                 {'loss': 1.6936, 'grad_norm': 5.627353668212891, 'learning_rate': 1.602390850850113e-05, 'epoch': 0.94}
 32%|███▏      | 197/624 [17:12<37:32,  5.27s/it] 32%|███▏      | 198/624 [17:17<37:22,  5.26s/it]                                                 {'loss': 1.4942, 'grad_norm': 4.528767108917236, 'learning_rate': 1.5982379161780722e-05, 'epoch': 0.95}
 32%|███▏      | 198/624 [17:17<37:22,  5.26s/it] 32%|███▏      | 199/624 [17:23<37:37,  5.31s/it]                                                 {'loss': 1.7365, 'grad_norm': 5.134439468383789, 'learning_rate': 1.5940688504813664e-05, 'epoch': 0.95}
 32%|███▏      | 199/624 [17:23<37:37,  5.31s/it] 32%|███▏      | 200/624 [17:28<37:14,  5.27s/it]                                                 {'loss': 1.5191, 'grad_norm': 4.014751434326172, 'learning_rate': 1.5898837661756405e-05, 'epoch': 0.96}
 32%|███▏      | 200/624 [17:28<37:14,  5.27s/it] 32%|███▏      | 201/624 [17:33<36:52,  5.23s/it]                                                 {'loss': 1.6082, 'grad_norm': 5.342032432556152, 'learning_rate': 1.5856827761084698e-05, 'epoch': 0.96}
 32%|███▏      | 201/624 [17:33<36:52,  5.23s/it] 32%|███▏      | 202/624 [17:38<36:31,  5.19s/it]                                                 {'loss': 1.3701, 'grad_norm': 3.8247900009155273, 'learning_rate': 1.5814659935563165e-05, 'epoch': 0.97}
 32%|███▏      | 202/624 [17:38<36:31,  5.19s/it] 33%|███▎      | 203/624 [17:44<37:12,  5.30s/it]                                                 {'loss': 1.5947, 'grad_norm': 3.8252923488616943, 'learning_rate': 1.577233532221474e-05, 'epoch': 0.97}
 33%|███▎      | 203/624 [17:44<37:12,  5.30s/it] 33%|███▎      | 204/624 [17:49<37:00,  5.29s/it]                                                 {'loss': 1.6258, 'grad_norm': 3.860952615737915, 'learning_rate': 1.5729855062290024e-05, 'epoch': 0.98}
 33%|███▎      | 204/624 [17:49<37:00,  5.29s/it] 33%|███▎      | 205/624 [17:54<36:54,  5.28s/it]                                                 {'loss': 1.5159, 'grad_norm': 6.243176460266113, 'learning_rate': 1.568722030123651e-05, 'epoch': 0.98}
 33%|███▎      | 205/624 [17:54<36:54,  5.28s/it] 33%|███▎      | 206/624 [17:59<36:06,  5.18s/it]                                                 {'loss': 1.462, 'grad_norm': 4.877236843109131, 'learning_rate': 1.5644432188667695e-05, 'epoch': 0.99}
 33%|███▎      | 206/624 [17:59<36:06,  5.18s/it] 33%|███▎      | 207/624 [18:04<35:28,  5.10s/it]                                                 {'loss': 1.5722, 'grad_norm': 4.048159599304199, 'learning_rate': 1.5601491878332077e-05, 'epoch': 0.99}
 33%|███▎      | 207/624 [18:04<35:28,  5.10s/it] 33%|███▎      | 208/624 [18:09<35:56,  5.18s/it]                                                 {'loss': 1.4338, 'grad_norm': 4.032680988311768, 'learning_rate': 1.5558400528082057e-05, 'epoch': 1.0}
 33%|███▎      | 208/624 [18:10<35:56,  5.18s/it] 33%|███▎      | 209/624 [18:14<35:30,  5.13s/it]                                                 {'loss': 1.1553, 'grad_norm': 5.06252908706665, 'learning_rate': 1.551515929984271e-05, 'epoch': 1.0}
 33%|███▎      | 209/624 [18:15<35:30,  5.13s/it] 34%|███▎      | 210/624 [18:20<35:44,  5.18s/it]                                                 {'loss': 0.8243, 'grad_norm': 4.022845268249512, 'learning_rate': 1.547176935958044e-05, 'epoch': 1.01}
 34%|███▎      | 210/624 [18:20<35:44,  5.18s/it] 34%|███▍      | 211/624 [18:25<35:52,  5.21s/it]                                                 {'loss': 0.7157, 'grad_norm': 4.2002997398376465, 'learning_rate': 1.5428231877271584e-05, 'epoch': 1.01}
 34%|███▍      | 211/624 [18:25<35:52,  5.21s/it] 34%|███▍      | 212/624 [18:30<35:49,  5.22s/it]                                                 {'loss': 0.845, 'grad_norm': 4.685903549194336, 'learning_rate': 1.538454802687081e-05, 'epoch': 1.02}
 34%|███▍      | 212/624 [18:30<35:49,  5.22s/it] 34%|███▍      | 213/624 [18:35<35:21,  5.16s/it]                                                 {'loss': 0.9071, 'grad_norm': 4.0984296798706055, 'learning_rate': 1.5340718986279505e-05, 'epoch': 1.02}
 34%|███▍      | 213/624 [18:35<35:21,  5.16s/it] 34%|███▍      | 214/624 [18:40<35:00,  5.12s/it]                                                 {'loss': 0.9025, 'grad_norm': 7.468338489532471, 'learning_rate': 1.529674593731399e-05, 'epoch': 1.03}
 34%|███▍      | 214/624 [18:40<35:00,  5.12s/it] 34%|███▍      | 215/624 [18:45<34:23,  5.05s/it]                                                 {'loss': 0.842, 'grad_norm': 5.77565336227417, 'learning_rate': 1.5252630065673662e-05, 'epoch': 1.03}
 34%|███▍      | 215/624 [18:45<34:23,  5.05s/it] 35%|███▍      | 216/624 [18:50<34:37,  5.09s/it]                                                 {'loss': 0.8058, 'grad_norm': 4.823929786682129, 'learning_rate': 1.5208372560909031e-05, 'epoch': 1.04}
 35%|███▍      | 216/624 [18:50<34:37,  5.09s/it] 35%|███▍      | 217/624 [18:56<35:00,  5.16s/it]                                                 {'loss': 0.8381, 'grad_norm': 5.620666980743408, 'learning_rate': 1.5163974616389621e-05, 'epoch': 1.04}
 35%|███▍      | 217/624 [18:56<35:00,  5.16s/it] 35%|███▍      | 218/624 [19:01<36:17,  5.36s/it]                                                 {'loss': 0.8168, 'grad_norm': 4.609112739562988, 'learning_rate': 1.5119437429271813e-05, 'epoch': 1.05}
 35%|███▍      | 218/624 [19:02<36:17,  5.36s/it] 35%|███▌      | 219/624 [19:07<35:47,  5.30s/it]                                                 {'loss': 0.81, 'grad_norm': 6.09754753112793, 'learning_rate': 1.5074762200466557e-05, 'epoch': 1.05}
 35%|███▌      | 219/624 [19:07<35:47,  5.30s/it] 35%|███▌      | 220/624 [19:12<35:57,  5.34s/it]                                                 {'loss': 0.8274, 'grad_norm': 4.0694756507873535, 'learning_rate': 1.5029950134606991e-05, 'epoch': 1.06}
 35%|███▌      | 220/624 [19:12<35:57,  5.34s/it] 35%|███▌      | 221/624 [19:17<35:19,  5.26s/it]                                                 {'loss': 0.8217, 'grad_norm': 4.285017013549805, 'learning_rate': 1.4985002440015959e-05, 'epoch': 1.06}
 35%|███▌      | 221/624 [19:17<35:19,  5.26s/it] 36%|███▌      | 222/624 [19:22<34:44,  5.18s/it]                                                 {'loss': 0.7168, 'grad_norm': 5.109531402587891, 'learning_rate': 1.4939920328673422e-05, 'epoch': 1.06}
 36%|███▌      | 222/624 [19:22<34:44,  5.18s/it] 36%|███▌      | 223/624 [19:28<35:07,  5.26s/it]                                                 {'loss': 0.5405, 'grad_norm': 4.387628078460693, 'learning_rate': 1.4894705016183803e-05, 'epoch': 1.07}
 36%|███▌      | 223/624 [19:28<35:07,  5.26s/it] 36%|███▌      | 224/624 [19:33<35:05,  5.26s/it]                                                 {'loss': 0.9484, 'grad_norm': 4.974968433380127, 'learning_rate': 1.4849357721743169e-05, 'epoch': 1.07}
 36%|███▌      | 224/624 [19:33<35:05,  5.26s/it] 36%|███▌      | 225/624 [19:38<35:18,  5.31s/it]                                                 {'loss': 0.8109, 'grad_norm': 4.637824535369873, 'learning_rate': 1.4803879668106393e-05, 'epoch': 1.08}
 36%|███▌      | 225/624 [19:38<35:18,  5.31s/it] 36%|███▌      | 226/624 [19:44<35:04,  5.29s/it]                                                 {'loss': 0.7102, 'grad_norm': 4.745955944061279, 'learning_rate': 1.4758272081554168e-05, 'epoch': 1.08}
 36%|███▌      | 226/624 [19:44<35:04,  5.29s/it] 36%|███▋      | 227/624 [19:49<34:43,  5.25s/it]                                                 {'loss': 0.874, 'grad_norm': 4.473702907562256, 'learning_rate': 1.4712536191859934e-05, 'epoch': 1.09}
 36%|███▋      | 227/624 [19:49<34:43,  5.25s/it] 37%|███▋      | 228/624 [19:54<35:18,  5.35s/it]                                                 {'loss': 0.6802, 'grad_norm': 4.783271312713623, 'learning_rate': 1.4666673232256738e-05, 'epoch': 1.09}
 37%|███▋      | 228/624 [19:54<35:18,  5.35s/it] 37%|███▋      | 229/624 [20:00<35:38,  5.41s/it]                                                 {'loss': 0.9402, 'grad_norm': 4.442485809326172, 'learning_rate': 1.4620684439403962e-05, 'epoch': 1.1}
 37%|███▋      | 229/624 [20:00<35:38,  5.41s/it] 37%|███▋      | 230/624 [20:05<34:25,  5.24s/it]                                                 {'loss': 0.976, 'grad_norm': 5.62580680847168, 'learning_rate': 1.4574571053353987e-05, 'epoch': 1.1}
 37%|███▋      | 230/624 [20:05<34:25,  5.24s/it] 37%|███▋      | 231/624 [20:10<33:34,  5.13s/it]                                                 {'loss': 0.7649, 'grad_norm': 4.968516826629639, 'learning_rate': 1.452833431751875e-05, 'epoch': 1.11}
 37%|███▋      | 231/624 [20:10<33:34,  5.13s/it] 37%|███▋      | 232/624 [20:15<33:39,  5.15s/it]                                                 {'loss': 0.7821, 'grad_norm': 4.3585944175720215, 'learning_rate': 1.448197547863622e-05, 'epoch': 1.11}
 37%|███▋      | 232/624 [20:15<33:39,  5.15s/it] 37%|███▋      | 233/624 [20:20<33:00,  5.07s/it]                                                 {'loss': 0.8059, 'grad_norm': 4.294806957244873, 'learning_rate': 1.4435495786736796e-05, 'epoch': 1.12}
 37%|███▋      | 233/624 [20:20<33:00,  5.07s/it] 38%|███▊      | 234/624 [20:25<33:09,  5.10s/it]                                                 {'loss': 0.8722, 'grad_norm': 5.249377727508545, 'learning_rate': 1.438889649510956e-05, 'epoch': 1.12}
 38%|███▊      | 234/624 [20:25<33:09,  5.10s/it] 38%|███▊      | 235/624 [20:30<32:52,  5.07s/it]                                                 {'loss': 0.9388, 'grad_norm': 4.458323955535889, 'learning_rate': 1.4342178860268523e-05, 'epoch': 1.13}
 38%|███▊      | 235/624 [20:30<32:52,  5.07s/it] 38%|███▊      | 236/624 [20:35<33:16,  5.15s/it]                                                 {'loss': 0.765, 'grad_norm': 4.618588924407959, 'learning_rate': 1.4295344141918734e-05, 'epoch': 1.13}
 38%|███▊      | 236/624 [20:35<33:16,  5.15s/it] 38%|███▊      | 237/624 [20:40<33:15,  5.16s/it]                                                 {'loss': 1.0364, 'grad_norm': 5.752800464630127, 'learning_rate': 1.4248393602922299e-05, 'epoch': 1.14}
 38%|███▊      | 237/624 [20:40<33:15,  5.16s/it] 38%|███▊      | 238/624 [20:46<33:45,  5.25s/it]                                                 {'loss': 0.6917, 'grad_norm': 4.500326633453369, 'learning_rate': 1.420132850926434e-05, 'epoch': 1.14}
 38%|███▊      | 238/624 [20:46<33:45,  5.25s/it] 38%|███▊      | 239/624 [20:51<33:55,  5.29s/it]                                                 {'loss': 0.689, 'grad_norm': 4.490303039550781, 'learning_rate': 1.4154150130018867e-05, 'epoch': 1.15}
 38%|███▊      | 239/624 [20:51<33:55,  5.29s/it] 38%|███▊      | 240/624 [20:56<33:30,  5.24s/it]                                                 {'loss': 0.8756, 'grad_norm': 4.34123420715332, 'learning_rate': 1.4106859737314532e-05, 'epoch': 1.15}
 38%|███▊      | 240/624 [20:56<33:30,  5.24s/it] 39%|███▊      | 241/624 [21:01<33:23,  5.23s/it]                                                 {'loss': 0.7043, 'grad_norm': 9.102226257324219, 'learning_rate': 1.4059458606300358e-05, 'epoch': 1.16}
 39%|███▊      | 241/624 [21:02<33:23,  5.23s/it] 39%|███▉      | 242/624 [21:07<33:41,  5.29s/it]                                                 {'loss': 0.753, 'grad_norm': 5.629117488861084, 'learning_rate': 1.4011948015111334e-05, 'epoch': 1.16}
 39%|███▉      | 242/624 [21:07<33:41,  5.29s/it] 39%|███▉      | 243/624 [21:12<32:59,  5.19s/it]                                                 {'loss': 0.8714, 'grad_norm': 4.721436500549316, 'learning_rate': 1.396432924483396e-05, 'epoch': 1.17}
 39%|███▉      | 243/624 [21:12<32:59,  5.19s/it] 39%|███▉      | 244/624 [21:17<32:51,  5.19s/it]                                                 {'loss': 0.8362, 'grad_norm': 5.671996116638184, 'learning_rate': 1.3916603579471705e-05, 'epoch': 1.17}
 39%|███▉      | 244/624 [21:17<32:51,  5.19s/it] 39%|███▉      | 245/624 [21:22<33:10,  5.25s/it]                                                 {'loss': 0.691, 'grad_norm': 5.757950782775879, 'learning_rate': 1.3868772305910376e-05, 'epoch': 1.18}
 39%|███▉      | 245/624 [21:23<33:10,  5.25s/it] 39%|███▉      | 246/624 [21:28<33:19,  5.29s/it]                                                 {'loss': 0.9437, 'grad_norm': 9.593981742858887, 'learning_rate': 1.3820836713883424e-05, 'epoch': 1.18}
 39%|███▉      | 246/624 [21:28<33:19,  5.29s/it] 40%|███▉      | 247/624 [21:33<33:15,  5.29s/it]                                                 {'loss': 0.8277, 'grad_norm': 7.427443027496338, 'learning_rate': 1.3772798095937172e-05, 'epoch': 1.18}
 40%|███▉      | 247/624 [21:33<33:15,  5.29s/it] 40%|███▉      | 248/624 [21:38<33:03,  5.28s/it]                                                 {'loss': 0.7097, 'grad_norm': 4.801572322845459, 'learning_rate': 1.3724657747395957e-05, 'epoch': 1.19}
 40%|███▉      | 248/624 [21:38<33:03,  5.28s/it] 40%|███▉      | 249/624 [21:44<33:02,  5.29s/it]                                                 {'loss': 0.7934, 'grad_norm': 4.128724575042725, 'learning_rate': 1.3676416966327201e-05, 'epoch': 1.19}
 40%|███▉      | 249/624 [21:44<33:02,  5.29s/it] 40%|████      | 250/624 [21:49<32:29,  5.21s/it]                                                 {'loss': 0.7697, 'grad_norm': 4.1867170333862305, 'learning_rate': 1.362807705350641e-05, 'epoch': 1.2}
 40%|████      | 250/624 [21:49<32:29,  5.21s/it] 40%|████      | 251/624 [21:54<32:15,  5.19s/it]                                                 {'loss': 0.7785, 'grad_norm': 4.231151103973389, 'learning_rate': 1.3579639312382105e-05, 'epoch': 1.2}
 40%|████      | 251/624 [21:54<32:15,  5.19s/it] 40%|████      | 252/624 [21:59<32:31,  5.25s/it]                                                 {'loss': 0.6466, 'grad_norm': 3.802166223526001, 'learning_rate': 1.3531105049040667e-05, 'epoch': 1.21}
 40%|████      | 252/624 [21:59<32:31,  5.25s/it] 41%|████      | 253/624 [22:05<32:53,  5.32s/it]                                                 {'loss': 0.666, 'grad_norm': 3.8602936267852783, 'learning_rate': 1.3482475572171132e-05, 'epoch': 1.21}
 41%|████      | 253/624 [22:05<32:53,  5.32s/it] 41%|████      | 254/624 [22:10<32:30,  5.27s/it]                                                 {'loss': 0.7856, 'grad_norm': 4.088754653930664, 'learning_rate': 1.3433752193029888e-05, 'epoch': 1.22}
 41%|████      | 254/624 [22:10<32:30,  5.27s/it] 41%|████      | 255/624 [22:15<32:31,  5.29s/it]                                                 {'loss': 0.6905, 'grad_norm': 3.722816228866577, 'learning_rate': 1.3384936225405326e-05, 'epoch': 1.22}
 41%|████      | 255/624 [22:15<32:31,  5.29s/it] 41%|████      | 256/624 [22:21<32:29,  5.30s/it]                                                 {'loss': 0.7636, 'grad_norm': 4.347720146179199, 'learning_rate': 1.333602898558242e-05, 'epoch': 1.23}
 41%|████      | 256/624 [22:21<32:29,  5.30s/it] 41%|████      | 257/624 [22:26<32:10,  5.26s/it]                                                 {'loss': 0.8197, 'grad_norm': 4.188772201538086, 'learning_rate': 1.3287031792307226e-05, 'epoch': 1.23}
 41%|████      | 257/624 [22:26<32:10,  5.26s/it] 41%|████▏     | 258/624 [22:31<32:00,  5.25s/it]                                                 {'loss': 0.766, 'grad_norm': 4.762589931488037, 'learning_rate': 1.323794596675132e-05, 'epoch': 1.24}
 41%|████▏     | 258/624 [22:31<32:00,  5.25s/it] 42%|████▏     | 259/624 [22:36<31:28,  5.17s/it]                                                 {'loss': 0.735, 'grad_norm': 4.1787028312683105, 'learning_rate': 1.318877283247619e-05, 'epoch': 1.24}
 42%|████▏     | 259/624 [22:36<31:28,  5.17s/it] 42%|████▏     | 260/624 [22:41<31:16,  5.16s/it]                                                 {'loss': 0.8423, 'grad_norm': 4.662001609802246, 'learning_rate': 1.3139513715397521e-05, 'epoch': 1.25}
 42%|████▏     | 260/624 [22:41<31:16,  5.16s/it] 42%|████▏     | 261/624 [22:46<31:08,  5.15s/it]                                                 {'loss': 0.7683, 'grad_norm': 4.345724582672119, 'learning_rate': 1.3090169943749475e-05, 'epoch': 1.25}
 42%|████▏     | 261/624 [22:46<31:08,  5.15s/it] 42%|████▏     | 262/624 [22:51<31:16,  5.18s/it]                                                 {'loss': 0.6552, 'grad_norm': 4.147659778594971, 'learning_rate': 1.304074284804885e-05, 'epoch': 1.26}
 42%|████▏     | 262/624 [22:52<31:16,  5.18s/it] 42%|████▏     | 263/624 [22:56<30:31,  5.07s/it]                                                 {'loss': 0.7297, 'grad_norm': 4.9245123863220215, 'learning_rate': 1.2991233761059214e-05, 'epoch': 1.26}
 42%|████▏     | 263/624 [22:56<30:31,  5.07s/it] 42%|████▏     | 264/624 [23:01<30:30,  5.08s/it]                                                 {'loss': 0.8172, 'grad_norm': 4.963655948638916, 'learning_rate': 1.2941644017754964e-05, 'epoch': 1.27}
 42%|████▏     | 264/624 [23:01<30:30,  5.08s/it] 42%|████▏     | 265/624 [23:06<30:27,  5.09s/it]                                                 {'loss': 0.774, 'grad_norm': 4.127607822418213, 'learning_rate': 1.289197495528534e-05, 'epoch': 1.27}
 42%|████▏     | 265/624 [23:07<30:27,  5.09s/it] 43%|████▎     | 266/624 [23:12<30:43,  5.15s/it]                                                 {'loss': 0.7319, 'grad_norm': 3.9170844554901123, 'learning_rate': 1.284222791293836e-05, 'epoch': 1.28}
 43%|████▎     | 266/624 [23:12<30:43,  5.15s/it] 43%|████▎     | 267/624 [23:17<30:45,  5.17s/it]                                                 {'loss': 0.7242, 'grad_norm': 4.60423469543457, 'learning_rate': 1.2792404232104699e-05, 'epoch': 1.28}
 43%|████▎     | 267/624 [23:17<30:45,  5.17s/it] 43%|████▎     | 268/624 [23:22<30:05,  5.07s/it]                                                 {'loss': 0.7061, 'grad_norm': 4.147988796234131, 'learning_rate': 1.2742505256241543e-05, 'epoch': 1.29}
 43%|████▎     | 268/624 [23:22<30:05,  5.07s/it] 43%|████▎     | 269/624 [23:27<30:12,  5.11s/it]                                                 {'loss': 0.7875, 'grad_norm': 3.962571859359741, 'learning_rate': 1.2692532330836346e-05, 'epoch': 1.29}
 43%|████▎     | 269/624 [23:27<30:12,  5.11s/it] 43%|████▎     | 270/624 [23:32<30:07,  5.11s/it]                                                 {'loss': 0.652, 'grad_norm': 4.566669940948486, 'learning_rate': 1.2642486803370553e-05, 'epoch': 1.29}
 43%|████▎     | 270/624 [23:32<30:07,  5.11s/it] 43%|████▎     | 271/624 [23:37<29:25,  5.00s/it]                                                 {'loss': 0.7644, 'grad_norm': 4.36200475692749, 'learning_rate': 1.2592370023283268e-05, 'epoch': 1.3}
 43%|████▎     | 271/624 [23:37<29:25,  5.00s/it] 44%|████▎     | 272/624 [23:42<30:01,  5.12s/it]                                                 {'loss': 0.738, 'grad_norm': 4.483165264129639, 'learning_rate': 1.2542183341934873e-05, 'epoch': 1.3}
 44%|████▎     | 272/624 [23:42<30:01,  5.12s/it] 44%|████▍     | 273/624 [23:48<30:34,  5.23s/it]                                                 {'loss': 0.8286, 'grad_norm': 5.2595648765563965, 'learning_rate': 1.2491928112570568e-05, 'epoch': 1.31}
 44%|████▍     | 273/624 [23:48<30:34,  5.23s/it] 44%|████▍     | 274/624 [23:53<30:44,  5.27s/it]                                                 {'loss': 0.812, 'grad_norm': 3.9981303215026855, 'learning_rate': 1.2441605690283915e-05, 'epoch': 1.31}
 44%|████▍     | 274/624 [23:53<30:44,  5.27s/it] 44%|████▍     | 275/624 [23:58<30:33,  5.25s/it]                                                 {'loss': 0.7294, 'grad_norm': 4.075860977172852, 'learning_rate': 1.2391217431980273e-05, 'epoch': 1.32}
 44%|████▍     | 275/624 [23:58<30:33,  5.25s/it] 44%|████▍     | 276/624 [24:03<30:21,  5.23s/it]                                                 {'loss': 0.7209, 'grad_norm': 4.242599964141846, 'learning_rate': 1.234076469634022e-05, 'epoch': 1.32}
 44%|████▍     | 276/624 [24:04<30:21,  5.23s/it] 44%|████▍     | 277/624 [24:09<30:11,  5.22s/it]                                                 {'loss': 0.831, 'grad_norm': 4.295252323150635, 'learning_rate': 1.2290248843782915e-05, 'epoch': 1.33}
 44%|████▍     | 277/624 [24:09<30:11,  5.22s/it] 45%|████▍     | 278/624 [24:14<29:50,  5.18s/it]                                                 {'loss': 0.6393, 'grad_norm': 3.8025496006011963, 'learning_rate': 1.2239671236429413e-05, 'epoch': 1.33}
 45%|████▍     | 278/624 [24:14<29:50,  5.18s/it] 45%|████▍     | 279/624 [24:19<29:35,  5.15s/it]                                                 {'loss': 0.6789, 'grad_norm': 4.992707252502441, 'learning_rate': 1.218903323806595e-05, 'epoch': 1.34}
 45%|████▍     | 279/624 [24:19<29:35,  5.15s/it] 45%|████▍     | 280/624 [24:24<29:43,  5.18s/it]                                                 {'loss': 0.7973, 'grad_norm': 3.860023260116577, 'learning_rate': 1.2138336214107148e-05, 'epoch': 1.34}
 45%|████▍     | 280/624 [24:24<29:43,  5.18s/it] 45%|████▌     | 281/624 [24:29<29:48,  5.21s/it]                                                 {'loss': 0.7918, 'grad_norm': 9.068103790283203, 'learning_rate': 1.2087581531559208e-05, 'epoch': 1.35}
 45%|████▌     | 281/624 [24:29<29:48,  5.21s/it] 45%|████▌     | 282/624 [24:35<29:53,  5.25s/it]                                                 {'loss': 0.8665, 'grad_norm': 4.1972174644470215, 'learning_rate': 1.2036770558983067e-05, 'epoch': 1.35}
 45%|████▌     | 282/624 [24:35<29:53,  5.25s/it] 45%|████▌     | 283/624 [24:40<29:57,  5.27s/it]                                                 {'loss': 0.8266, 'grad_norm': 4.8024582862854, 'learning_rate': 1.1985904666457455e-05, 'epoch': 1.36}
 45%|████▌     | 283/624 [24:40<29:57,  5.27s/it] 46%|████▌     | 284/624 [24:45<29:53,  5.28s/it]                                                 {'loss': 0.7606, 'grad_norm': 4.663908004760742, 'learning_rate': 1.1934985225541998e-05, 'epoch': 1.36}
 46%|████▌     | 284/624 [24:45<29:53,  5.28s/it] 46%|████▌     | 285/624 [24:50<29:29,  5.22s/it]                                                 {'loss': 0.8055, 'grad_norm': 4.508217811584473, 'learning_rate': 1.18840136092402e-05, 'epoch': 1.37}
 46%|████▌     | 285/624 [24:51<29:29,  5.22s/it] 46%|████▌     | 286/624 [24:55<28:59,  5.15s/it]                                                 {'loss': 0.763, 'grad_norm': 4.56742525100708, 'learning_rate': 1.1832991191962435e-05, 'epoch': 1.37}
 46%|████▌     | 286/624 [24:56<28:59,  5.15s/it] 46%|████▌     | 287/624 [25:01<28:57,  5.15s/it]                                                 {'loss': 0.8089, 'grad_norm': 4.425145149230957, 'learning_rate': 1.1781919349488894e-05, 'epoch': 1.38}
 46%|████▌     | 287/624 [25:01<28:57,  5.15s/it] 46%|████▌     | 288/624 [25:06<28:56,  5.17s/it]                                                 {'loss': 0.8353, 'grad_norm': 4.394044876098633, 'learning_rate': 1.1730799458932473e-05, 'epoch': 1.38}
 46%|████▌     | 288/624 [25:06<28:56,  5.17s/it] 46%|████▋     | 289/624 [25:11<29:09,  5.22s/it]                                                 {'loss': 0.8359, 'grad_norm': 4.599130630493164, 'learning_rate': 1.1679632898701649e-05, 'epoch': 1.39}
 46%|████▋     | 289/624 [25:11<29:09,  5.22s/it] 46%|████▋     | 290/624 [25:16<28:18,  5.08s/it]                                                 {'loss': 0.7701, 'grad_norm': 4.227880001068115, 'learning_rate': 1.1628421048463315e-05, 'epoch': 1.39}
 46%|████▋     | 290/624 [25:16<28:18,  5.08s/it] 47%|████▋     | 291/624 [25:21<28:32,  5.14s/it]                                                 {'loss': 0.8992, 'grad_norm': 5.213074684143066, 'learning_rate': 1.1577165289105565e-05, 'epoch': 1.4}
 47%|████▋     | 291/624 [25:21<28:32,  5.14s/it] 47%|████▋     | 292/624 [25:26<28:21,  5.12s/it]                                                 {'loss': 0.8294, 'grad_norm': 4.996181964874268, 'learning_rate': 1.1525867002700484e-05, 'epoch': 1.4}
 47%|████▋     | 292/624 [25:26<28:21,  5.12s/it] 47%|████▋     | 293/624 [25:31<28:04,  5.09s/it]                                                 {'loss': 0.8574, 'grad_norm': 4.346372604370117, 'learning_rate': 1.1474527572466847e-05, 'epoch': 1.41}
 47%|████▋     | 293/624 [25:31<28:04,  5.09s/it] 47%|████▋     | 294/624 [25:36<28:15,  5.14s/it]                                                 {'loss': 0.8378, 'grad_norm': 3.939822196960449, 'learning_rate': 1.1423148382732854e-05, 'epoch': 1.41}
 47%|████▋     | 294/624 [25:37<28:15,  5.14s/it] 47%|████▋     | 295/624 [25:42<28:16,  5.16s/it]                                                 {'loss': 0.8038, 'grad_norm': 3.8684017658233643, 'learning_rate': 1.1371730818898785e-05, 'epoch': 1.41}
 47%|████▋     | 295/624 [25:42<28:16,  5.16s/it] 47%|████▋     | 296/624 [25:47<28:32,  5.22s/it]                                                 {'loss': 0.6018, 'grad_norm': 4.060428619384766, 'learning_rate': 1.132027626739965e-05, 'epoch': 1.42}
 47%|████▋     | 296/624 [25:47<28:32,  5.22s/it] 48%|████▊     | 297/624 [25:52<28:27,  5.22s/it]                                                 {'loss': 0.7008, 'grad_norm': 4.07478666305542, 'learning_rate': 1.1268786115667798e-05, 'epoch': 1.42}
 48%|████▊     | 297/624 [25:52<28:27,  5.22s/it] 48%|████▊     | 298/624 [25:57<28:10,  5.19s/it]                                                 {'loss': 0.8132, 'grad_norm': 4.811229228973389, 'learning_rate': 1.1217261752095518e-05, 'epoch': 1.43}
 48%|████▊     | 298/624 [25:58<28:10,  5.19s/it] 48%|████▊     | 299/624 [26:03<28:14,  5.21s/it]                                                 {'loss': 0.9083, 'grad_norm': 3.9539833068847656, 'learning_rate': 1.1165704565997593e-05, 'epoch': 1.43}
 48%|████▊     | 299/624 [26:03<28:14,  5.21s/it] 48%|████▊     | 300/624 [26:08<27:58,  5.18s/it]                                                 {'loss': 0.7409, 'grad_norm': 4.231526851654053, 'learning_rate': 1.1114115947573834e-05, 'epoch': 1.44}
 48%|████▊     | 300/624 [26:08<27:58,  5.18s/it] 48%|████▊     | 301/624 [26:13<27:45,  5.16s/it]                                                 {'loss': 0.7462, 'grad_norm': 5.188995838165283, 'learning_rate': 1.1062497287871606e-05, 'epoch': 1.44}
 48%|████▊     | 301/624 [26:13<27:45,  5.16s/it] 48%|████▊     | 302/624 [26:18<28:04,  5.23s/it]                                                 {'loss': 0.807, 'grad_norm': 4.6133036613464355, 'learning_rate': 1.1010849978748314e-05, 'epoch': 1.45}
 48%|████▊     | 302/624 [26:18<28:04,  5.23s/it] 49%|████▊     | 303/624 [26:24<28:27,  5.32s/it]                                                 {'loss': 0.6491, 'grad_norm': 6.0128583908081055, 'learning_rate': 1.0959175412833869e-05, 'epoch': 1.45}
 49%|████▊     | 303/624 [26:24<28:27,  5.32s/it] 49%|████▊     | 304/624 [26:29<28:46,  5.39s/it]                                                 {'loss': 0.9596, 'grad_norm': 4.119927406311035, 'learning_rate': 1.0907474983493144e-05, 'epoch': 1.46}
 49%|████▊     | 304/624 [26:29<28:46,  5.39s/it] 49%|████▉     | 305/624 [26:35<28:17,  5.32s/it]                                                 {'loss': 0.8414, 'grad_norm': 5.6814799308776855, 'learning_rate': 1.08557500847884e-05, 'epoch': 1.46}
 49%|████▉     | 305/624 [26:35<28:17,  5.32s/it] 49%|████▉     | 306/624 [26:40<27:42,  5.23s/it]                                                 {'loss': 0.8637, 'grad_norm': 3.8846256732940674, 'learning_rate': 1.080400211144169e-05, 'epoch': 1.47}
 49%|████▉     | 306/624 [26:40<27:42,  5.23s/it] 49%|████▉     | 307/624 [26:45<27:41,  5.24s/it]                                                 {'loss': 0.6314, 'grad_norm': 4.412027835845947, 'learning_rate': 1.0752232458797262e-05, 'epoch': 1.47}
 49%|████▉     | 307/624 [26:45<27:41,  5.24s/it] 49%|████▉     | 308/624 [26:50<27:47,  5.28s/it]                                                 {'loss': 0.6455, 'grad_norm': 3.530766248703003, 'learning_rate': 1.070044252278393e-05, 'epoch': 1.48}
 49%|████▉     | 308/624 [26:50<27:47,  5.28s/it] 50%|████▉     | 309/624 [26:56<27:56,  5.32s/it]                                                 {'loss': 0.6494, 'grad_norm': 4.344031810760498, 'learning_rate': 1.064863369987743e-05, 'epoch': 1.48}
 50%|████▉     | 309/624 [26:56<27:56,  5.32s/it] 50%|████▉     | 310/624 [27:01<27:25,  5.24s/it]                                                 {'loss': 0.8432, 'grad_norm': 4.386435031890869, 'learning_rate': 1.0596807387062772e-05, 'epoch': 1.49}
 50%|████▉     | 310/624 [27:01<27:25,  5.24s/it] 50%|████▉     | 311/624 [27:06<26:59,  5.18s/it]                                                 {'loss': 0.8416, 'grad_norm': 3.4988975524902344, 'learning_rate': 1.0544964981796563e-05, 'epoch': 1.49}
 50%|████▉     | 311/624 [27:06<26:59,  5.18s/it] 50%|█████     | 312/624 [27:11<26:41,  5.13s/it]                                                 {'loss': 0.7805, 'grad_norm': 4.951017379760742, 'learning_rate': 1.0493107881969335e-05, 'epoch': 1.5}
 50%|█████     | 312/624 [27:11<26:41,  5.13s/it] 50%|█████     | 313/624 [27:16<26:19,  5.08s/it]                                                 {'loss': 0.8877, 'grad_norm': 4.788346767425537, 'learning_rate': 1.0441237485867845e-05, 'epoch': 1.5}
 50%|█████     | 313/624 [27:16<26:19,  5.08s/it] 50%|█████     | 314/624 [27:21<26:27,  5.12s/it]                                                 {'loss': 0.8264, 'grad_norm': 4.652549743652344, 'learning_rate': 1.0389355192137379e-05, 'epoch': 1.51}
 50%|█████     | 314/624 [27:21<26:27,  5.12s/it] 50%|█████     | 315/624 [27:26<26:12,  5.09s/it]                                                 {'loss': 0.7899, 'grad_norm': 4.5939531326293945, 'learning_rate': 1.0337462399744025e-05, 'epoch': 1.51}
 50%|█████     | 315/624 [27:26<26:12,  5.09s/it] 51%|█████     | 316/624 [27:31<26:22,  5.14s/it]                                                 {'loss': 0.8065, 'grad_norm': 4.082852840423584, 'learning_rate': 1.0285560507936962e-05, 'epoch': 1.52}
 51%|█████     | 316/624 [27:31<26:22,  5.14s/it] 51%|█████     | 317/624 [27:36<26:14,  5.13s/it]                                                 {'loss': 0.7954, 'grad_norm': 4.676595211029053, 'learning_rate': 1.0233650916210736e-05, 'epoch': 1.52}
 51%|█████     | 317/624 [27:36<26:14,  5.13s/it] 51%|█████     | 318/624 [27:42<26:36,  5.22s/it]                                                 {'loss': 0.7138, 'grad_norm': 4.445615291595459, 'learning_rate': 1.0181735024267504e-05, 'epoch': 1.53}
 51%|█████     | 318/624 [27:42<26:36,  5.22s/it] 51%|█████     | 319/624 [27:47<26:53,  5.29s/it]                                                 {'loss': 0.7732, 'grad_norm': 5.964245319366455, 'learning_rate': 1.012981423197931e-05, 'epoch': 1.53}
 51%|█████     | 319/624 [27:47<26:53,  5.29s/it] 51%|█████▏    | 320/624 [27:53<26:58,  5.32s/it]                                                 {'loss': 0.8449, 'grad_norm': 4.631807804107666, 'learning_rate': 1.007788993935033e-05, 'epoch': 1.53}
 51%|█████▏    | 320/624 [27:53<26:58,  5.32s/it] 51%|█████▏    | 321/624 [27:58<26:31,  5.25s/it]                                                 {'loss': 0.7131, 'grad_norm': 4.181155204772949, 'learning_rate': 1.002596354647912e-05, 'epoch': 1.54}
 51%|█████▏    | 321/624 [27:58<26:31,  5.25s/it] 52%|█████▏    | 322/624 [28:03<26:21,  5.24s/it]                                                 {'loss': 0.9089, 'grad_norm': 4.441815376281738, 'learning_rate': 9.974036453520881e-06, 'epoch': 1.54}
 52%|█████▏    | 322/624 [28:03<26:21,  5.24s/it] 52%|█████▏    | 323/624 [28:08<26:00,  5.18s/it]                                                 {'loss': 0.7504, 'grad_norm': 3.654690980911255, 'learning_rate': 9.922110060649672e-06, 'epoch': 1.55}
 52%|█████▏    | 323/624 [28:08<26:00,  5.18s/it] 52%|█████▏    | 324/624 [28:13<25:33,  5.11s/it]                                                 {'loss': 0.6677, 'grad_norm': 4.205193519592285, 'learning_rate': 9.870185768020694e-06, 'epoch': 1.55}
 52%|█████▏    | 324/624 [28:13<25:33,  5.11s/it] 52%|█████▏    | 325/624 [28:18<25:58,  5.21s/it]                                                 {'loss': 0.7858, 'grad_norm': 3.9391415119171143, 'learning_rate': 9.818264975732497e-06, 'epoch': 1.56}
 52%|█████▏    | 325/624 [28:18<25:58,  5.21s/it] 52%|█████▏    | 326/624 [28:24<26:04,  5.25s/it]                                                 {'loss': 0.7439, 'grad_norm': 3.8418092727661133, 'learning_rate': 9.766349083789266e-06, 'epoch': 1.56}
 52%|█████▏    | 326/624 [28:24<26:04,  5.25s/it] 52%|█████▏    | 327/624 [28:29<26:20,  5.32s/it]                                                 {'loss': 0.922, 'grad_norm': 3.857386589050293, 'learning_rate': 9.71443949206304e-06, 'epoch': 1.57}
 52%|█████▏    | 327/624 [28:29<26:20,  5.32s/it] 53%|█████▎    | 328/624 [28:35<26:43,  5.42s/it]                                                 {'loss': 0.6437, 'grad_norm': 4.184977054595947, 'learning_rate': 9.662537600255979e-06, 'epoch': 1.57}
 53%|█████▎    | 328/624 [28:35<26:43,  5.42s/it] 53%|█████▎    | 329/624 [28:40<26:33,  5.40s/it]                                                 {'loss': 0.7773, 'grad_norm': 4.270314693450928, 'learning_rate': 9.610644807862625e-06, 'epoch': 1.58}
 53%|█████▎    | 329/624 [28:40<26:33,  5.40s/it] 53%|█████▎    | 330/624 [28:46<26:34,  5.42s/it]                                                 {'loss': 0.9597, 'grad_norm': 3.9940025806427, 'learning_rate': 9.558762514132157e-06, 'epoch': 1.58}
 53%|█████▎    | 330/624 [28:46<26:34,  5.42s/it] 53%|█████▎    | 331/624 [28:51<25:58,  5.32s/it]                                                 {'loss': 0.8309, 'grad_norm': 4.194240570068359, 'learning_rate': 9.506892118030668e-06, 'epoch': 1.59}
 53%|█████▎    | 331/624 [28:51<25:58,  5.32s/it] 53%|█████▎    | 332/624 [28:56<25:14,  5.19s/it]                                                 {'loss': 0.7972, 'grad_norm': 4.4268293380737305, 'learning_rate': 9.455035018203439e-06, 'epoch': 1.59}
 53%|█████▎    | 332/624 [28:56<25:14,  5.19s/it] 53%|█████▎    | 333/624 [29:01<24:56,  5.14s/it]                                                 {'loss': 0.7615, 'grad_norm': 5.199631214141846, 'learning_rate': 9.40319261293723e-06, 'epoch': 1.6}
 53%|█████▎    | 333/624 [29:01<24:56,  5.14s/it] 54%|█████▎    | 334/624 [29:06<24:56,  5.16s/it]                                                 {'loss': 0.8677, 'grad_norm': 5.328347682952881, 'learning_rate': 9.351366300122569e-06, 'epoch': 1.6}
 54%|█████▎    | 334/624 [29:06<24:56,  5.16s/it] 54%|█████▎    | 335/624 [29:11<25:14,  5.24s/it]                                                 {'loss': 0.8144, 'grad_norm': 4.421981334686279, 'learning_rate': 9.299557477216073e-06, 'epoch': 1.61}
 54%|█████▎    | 335/624 [29:11<25:14,  5.24s/it] 54%|█████▍    | 336/624 [29:17<25:16,  5.26s/it]                                                 {'loss': 0.861, 'grad_norm': 3.7424073219299316, 'learning_rate': 9.247767541202738e-06, 'epoch': 1.61}
 54%|█████▍    | 336/624 [29:17<25:16,  5.26s/it] 54%|█████▍    | 337/624 [29:22<25:20,  5.30s/it]                                                 {'loss': 0.7985, 'grad_norm': 4.034987926483154, 'learning_rate': 9.195997888558312e-06, 'epoch': 1.62}
 54%|█████▍    | 337/624 [29:22<25:20,  5.30s/it] 54%|█████▍    | 338/624 [29:27<25:08,  5.27s/it]                                                 {'loss': 1.0041, 'grad_norm': 4.136049270629883, 'learning_rate': 9.144249915211605e-06, 'epoch': 1.62}
 54%|█████▍    | 338/624 [29:27<25:08,  5.27s/it] 54%|█████▍    | 339/624 [29:32<24:39,  5.19s/it]                                                 {'loss': 0.6539, 'grad_norm': 4.78134822845459, 'learning_rate': 9.092525016506858e-06, 'epoch': 1.63}
 54%|█████▍    | 339/624 [29:32<24:39,  5.19s/it] 54%|█████▍    | 340/624 [29:38<25:01,  5.29s/it]                                                 {'loss': 0.8178, 'grad_norm': 4.247229099273682, 'learning_rate': 9.040824587166136e-06, 'epoch': 1.63}
 54%|█████▍    | 340/624 [29:38<25:01,  5.29s/it] 55%|█████▍    | 341/624 [29:43<24:22,  5.17s/it]                                                 {'loss': 0.6198, 'grad_norm': 4.45266580581665, 'learning_rate': 8.98915002125169e-06, 'epoch': 1.64}
 55%|█████▍    | 341/624 [29:43<24:22,  5.17s/it] 55%|█████▍    | 342/624 [29:48<24:31,  5.22s/it]                                                 {'loss': 0.7666, 'grad_norm': 3.9398250579833984, 'learning_rate': 8.9375027121284e-06, 'epoch': 1.64}
 55%|█████▍    | 342/624 [29:48<24:31,  5.22s/it] 55%|█████▍    | 343/624 [29:53<24:35,  5.25s/it]                                                 {'loss': 0.7473, 'grad_norm': 4.547131061553955, 'learning_rate': 8.885884052426168e-06, 'epoch': 1.65}
 55%|█████▍    | 343/624 [29:53<24:35,  5.25s/it] 55%|█████▌    | 344/624 [29:58<24:33,  5.26s/it]                                                 {'loss': 0.7225, 'grad_norm': 3.8675551414489746, 'learning_rate': 8.83429543400241e-06, 'epoch': 1.65}
 55%|█████▌    | 344/624 [29:59<24:33,  5.26s/it] 55%|█████▌    | 345/624 [30:04<24:25,  5.25s/it]                                                 {'loss': 0.7187, 'grad_norm': 5.253139972686768, 'learning_rate': 8.78273824790448e-06, 'epoch': 1.65}
 55%|█████▌    | 345/624 [30:04<24:25,  5.25s/it] 55%|█████▌    | 346/624 [30:09<23:54,  5.16s/it]                                                 {'loss': 0.8238, 'grad_norm': 3.9173545837402344, 'learning_rate': 8.731213884332205e-06, 'epoch': 1.66}
 55%|█████▌    | 346/624 [30:09<23:54,  5.16s/it] 56%|█████▌    | 347/624 [30:14<24:07,  5.23s/it]                                                 {'loss': 0.9052, 'grad_norm': 4.040476322174072, 'learning_rate': 8.679723732600355e-06, 'epoch': 1.66}
 56%|█████▌    | 347/624 [30:14<24:07,  5.23s/it] 56%|█████▌    | 348/624 [30:19<24:02,  5.23s/it]                                                 {'loss': 0.7906, 'grad_norm': 4.4151458740234375, 'learning_rate': 8.628269181101216e-06, 'epoch': 1.67}
 56%|█████▌    | 348/624 [30:19<24:02,  5.23s/it] 56%|█████▌    | 349/624 [30:24<23:57,  5.23s/it]                                                 {'loss': 0.8223, 'grad_norm': 4.236024856567383, 'learning_rate': 8.576851617267151e-06, 'epoch': 1.67}
 56%|█████▌    | 349/624 [30:25<23:57,  5.23s/it] 56%|█████▌    | 350/624 [30:30<23:56,  5.24s/it]                                                 {'loss': 0.7561, 'grad_norm': 3.8517494201660156, 'learning_rate': 8.525472427533156e-06, 'epoch': 1.68}
 56%|█████▌    | 350/624 [30:30<23:56,  5.24s/it] 56%|█████▋    | 351/624 [30:35<23:55,  5.26s/it]                                                 {'loss': 0.7451, 'grad_norm': 4.319109916687012, 'learning_rate': 8.474132997299521e-06, 'epoch': 1.68}
 56%|█████▋    | 351/624 [30:35<23:55,  5.26s/it] 56%|█████▋    | 352/624 [30:40<24:03,  5.31s/it]                                                 {'loss': 0.6899, 'grad_norm': 4.869288921356201, 'learning_rate': 8.422834710894434e-06, 'epoch': 1.69}
 56%|█████▋    | 352/624 [30:41<24:03,  5.31s/it] 57%|█████▋    | 353/624 [30:46<23:51,  5.28s/it]                                                 {'loss': 0.7857, 'grad_norm': 4.624278545379639, 'learning_rate': 8.371578951536689e-06, 'epoch': 1.69}
 57%|█████▋    | 353/624 [30:46<23:51,  5.28s/it] 57%|█████▋    | 354/624 [30:51<24:00,  5.34s/it]                                                 {'loss': 0.773, 'grad_norm': 4.72633695602417, 'learning_rate': 8.320367101298351e-06, 'epoch': 1.7}
 57%|█████▋    | 354/624 [30:51<24:00,  5.34s/it] 57%|█████▋    | 355/624 [30:57<24:11,  5.40s/it]                                                 {'loss': 0.7024, 'grad_norm': 4.317216873168945, 'learning_rate': 8.26920054106753e-06, 'epoch': 1.7}
 57%|█████▋    | 355/624 [30:57<24:11,  5.40s/it] 57%|█████▋    | 356/624 [31:02<24:12,  5.42s/it]                                                 {'loss': 0.8076, 'grad_norm': 5.6110334396362305, 'learning_rate': 8.218080650511107e-06, 'epoch': 1.71}
 57%|█████▋    | 356/624 [31:02<24:12,  5.42s/it] 57%|█████▋    | 357/624 [31:07<23:19,  5.24s/it]                                                 {'loss': 0.7119, 'grad_norm': 4.290881156921387, 'learning_rate': 8.167008808037568e-06, 'epoch': 1.71}
 57%|█████▋    | 357/624 [31:07<23:19,  5.24s/it] 57%|█████▋    | 358/624 [31:12<22:52,  5.16s/it]                                                 {'loss': 0.7955, 'grad_norm': 5.479802131652832, 'learning_rate': 8.115986390759805e-06, 'epoch': 1.72}
 57%|█████▋    | 358/624 [31:12<22:52,  5.16s/it] 58%|█████▊    | 359/624 [31:17<22:40,  5.13s/it]                                                 {'loss': 0.8663, 'grad_norm': 4.39566707611084, 'learning_rate': 8.065014774458004e-06, 'epoch': 1.72}
 58%|█████▊    | 359/624 [31:17<22:40,  5.13s/it] 58%|█████▊    | 360/624 [31:22<22:51,  5.20s/it]                                                 {'loss': 0.7278, 'grad_norm': 4.162405967712402, 'learning_rate': 8.014095333542548e-06, 'epoch': 1.73}
 58%|█████▊    | 360/624 [31:22<22:51,  5.20s/it] 58%|█████▊    | 361/624 [31:28<23:00,  5.25s/it]                                                 {'loss': 0.8522, 'grad_norm': 4.073263168334961, 'learning_rate': 7.963229441016938e-06, 'epoch': 1.73}
 58%|█████▊    | 361/624 [31:28<23:00,  5.25s/it] 58%|█████▊    | 362/624 [31:33<22:56,  5.25s/it]                                                 {'loss': 0.6956, 'grad_norm': 4.333064079284668, 'learning_rate': 7.912418468440794e-06, 'epoch': 1.74}
 58%|█████▊    | 362/624 [31:33<22:56,  5.25s/it] 58%|█████▊    | 363/624 [31:38<22:46,  5.23s/it]                                                 {'loss': 0.7409, 'grad_norm': 3.9612224102020264, 'learning_rate': 7.861663785892857e-06, 'epoch': 1.74}
 58%|█████▊    | 363/624 [31:38<22:46,  5.23s/it] 58%|█████▊    | 364/624 [31:43<22:41,  5.23s/it]                                                 {'loss': 0.7082, 'grad_norm': 3.819762945175171, 'learning_rate': 7.810966761934053e-06, 'epoch': 1.75}
 58%|█████▊    | 364/624 [31:44<22:41,  5.23s/it] 58%|█████▊    | 365/624 [31:49<22:34,  5.23s/it]                                                 {'loss': 0.7056, 'grad_norm': 4.174880027770996, 'learning_rate': 7.760328763570589e-06, 'epoch': 1.75}
 58%|█████▊    | 365/624 [31:49<22:34,  5.23s/it] 59%|█████▊    | 366/624 [31:54<22:10,  5.16s/it]                                                 {'loss': 0.7498, 'grad_norm': 4.315208911895752, 'learning_rate': 7.709751156217088e-06, 'epoch': 1.76}
 59%|█████▊    | 366/624 [31:54<22:10,  5.16s/it] 59%|█████▉    | 367/624 [31:59<22:15,  5.20s/it]                                                 {'loss': 0.7633, 'grad_norm': 3.7854061126708984, 'learning_rate': 7.659235303659784e-06, 'epoch': 1.76}
 59%|█████▉    | 367/624 [31:59<22:15,  5.20s/it] 59%|█████▉    | 368/624 [32:04<22:17,  5.22s/it]                                                 {'loss': 0.6989, 'grad_norm': 4.419919490814209, 'learning_rate': 7.608782568019729e-06, 'epoch': 1.76}
 59%|█████▉    | 368/624 [32:04<22:17,  5.22s/it] 59%|█████▉    | 369/624 [32:09<21:24,  5.04s/it]                                                 {'loss': 0.6929, 'grad_norm': 3.7317070960998535, 'learning_rate': 7.558394309716088e-06, 'epoch': 1.77}
 59%|█████▉    | 369/624 [32:09<21:24,  5.04s/it] 59%|█████▉    | 370/624 [32:14<21:38,  5.11s/it]                                                 {'loss': 0.7442, 'grad_norm': 4.1093974113464355, 'learning_rate': 7.508071887429433e-06, 'epoch': 1.77}
 59%|█████▉    | 370/624 [32:14<21:38,  5.11s/it] 59%|█████▉    | 371/624 [32:19<21:32,  5.11s/it]                                                 {'loss': 0.8145, 'grad_norm': 3.920490026473999, 'learning_rate': 7.4578166580651335e-06, 'epoch': 1.78}
 59%|█████▉    | 371/624 [32:19<21:32,  5.11s/it] 60%|█████▉    | 372/624 [32:24<21:36,  5.15s/it]                                                 {'loss': 0.7728, 'grad_norm': 4.931862831115723, 'learning_rate': 7.4076299767167325e-06, 'epoch': 1.78}
 60%|█████▉    | 372/624 [32:25<21:36,  5.15s/it] 60%|█████▉    | 373/624 [32:30<21:39,  5.18s/it]                                                 {'loss': 0.7255, 'grad_norm': 3.892246723175049, 'learning_rate': 7.35751319662945e-06, 'epoch': 1.79}
 60%|█████▉    | 373/624 [32:30<21:39,  5.18s/it] 60%|█████▉    | 374/624 [32:35<21:40,  5.20s/it]                                                 {'loss': 0.9407, 'grad_norm': 4.3244733810424805, 'learning_rate': 7.307467669163655e-06, 'epoch': 1.79}
 60%|█████▉    | 374/624 [32:35<21:40,  5.20s/it] 60%|██████    | 375/624 [32:40<21:29,  5.18s/it]                                                 {'loss': 0.6298, 'grad_norm': 4.500171661376953, 'learning_rate': 7.25749474375846e-06, 'epoch': 1.8}
 60%|██████    | 375/624 [32:40<21:29,  5.18s/it] 60%|██████    | 376/624 [32:45<21:25,  5.18s/it]                                                 {'loss': 0.8362, 'grad_norm': 3.485419511795044, 'learning_rate': 7.207595767895303e-06, 'epoch': 1.8}
 60%|██████    | 376/624 [32:45<21:25,  5.18s/it] 60%|██████    | 377/624 [32:50<21:16,  5.17s/it]                                                 {'loss': 0.7053, 'grad_norm': 4.020944595336914, 'learning_rate': 7.157772087061645e-06, 'epoch': 1.81}
 60%|██████    | 377/624 [32:51<21:16,  5.17s/it] 61%|██████    | 378/624 [32:56<21:20,  5.20s/it]                                                 {'loss': 0.6798, 'grad_norm': 4.0235772132873535, 'learning_rate': 7.108025044714661e-06, 'epoch': 1.81}
 61%|██████    | 378/624 [32:56<21:20,  5.20s/it] 61%|██████    | 379/624 [33:01<21:08,  5.18s/it]                                                 {'loss': 0.8431, 'grad_norm': 4.9603657722473145, 'learning_rate': 7.058355982245038e-06, 'epoch': 1.82}
 61%|██████    | 379/624 [33:01<21:08,  5.18s/it] 61%|██████    | 380/624 [33:06<21:14,  5.23s/it]                                                 {'loss': 0.7403, 'grad_norm': 4.174317836761475, 'learning_rate': 7.00876623894079e-06, 'epoch': 1.82}
 61%|██████    | 380/624 [33:06<21:14,  5.23s/it] 61%|██████    | 381/624 [33:11<21:13,  5.24s/it]                                                 {'loss': 1.0231, 'grad_norm': 4.3483405113220215, 'learning_rate': 6.959257151951153e-06, 'epoch': 1.83}
 61%|██████    | 381/624 [33:12<21:13,  5.24s/it] 61%|██████    | 382/624 [33:17<21:23,  5.31s/it]                                                 {'loss': 0.6303, 'grad_norm': 3.574249267578125, 'learning_rate': 6.909830056250527e-06, 'epoch': 1.83}
 61%|██████    | 382/624 [33:17<21:23,  5.31s/it] 61%|██████▏   | 383/624 [33:22<21:37,  5.39s/it]                                                 {'loss': 0.9533, 'grad_norm': 4.286352634429932, 'learning_rate': 6.860486284602479e-06, 'epoch': 1.84}
 61%|██████▏   | 383/624 [33:23<21:37,  5.39s/it] 62%|██████▏   | 384/624 [33:28<21:20,  5.34s/it]                                                 {'loss': 0.8192, 'grad_norm': 4.251049041748047, 'learning_rate': 6.8112271675238154e-06, 'epoch': 1.84}
 62%|██████▏   | 384/624 [33:28<21:20,  5.34s/it] 62%|██████▏   | 385/624 [33:33<20:56,  5.26s/it]                                                 {'loss': 0.9109, 'grad_norm': 4.3242950439453125, 'learning_rate': 6.762054033248681e-06, 'epoch': 1.85}
 62%|██████▏   | 385/624 [33:33<20:56,  5.26s/it] 62%|██████▏   | 386/624 [33:38<21:11,  5.34s/it]                                                 {'loss': 0.7816, 'grad_norm': 5.527805328369141, 'learning_rate': 6.712968207692778e-06, 'epoch': 1.85}
 62%|██████▏   | 386/624 [33:38<21:11,  5.34s/it] 62%|██████▏   | 387/624 [33:44<20:59,  5.31s/it]                                                 {'loss': 0.7961, 'grad_norm': 4.397515773773193, 'learning_rate': 6.663971014417585e-06, 'epoch': 1.86}
 62%|██████▏   | 387/624 [33:44<20:59,  5.31s/it] 62%|██████▏   | 388/624 [33:49<20:39,  5.25s/it]                                                 {'loss': 0.7617, 'grad_norm': 4.091588973999023, 'learning_rate': 6.615063774594677e-06, 'epoch': 1.86}
 62%|██████▏   | 388/624 [33:49<20:39,  5.25s/it] 62%|██████▏   | 389/624 [33:54<20:47,  5.31s/it]                                                 {'loss': 0.8012, 'grad_norm': 4.413652420043945, 'learning_rate': 6.566247806970119e-06, 'epoch': 1.87}
 62%|██████▏   | 389/624 [33:54<20:47,  5.31s/it] 62%|██████▎   | 390/624 [34:00<20:50,  5.34s/it]                                                 {'loss': 0.7641, 'grad_norm': 4.112649917602539, 'learning_rate': 6.5175244278288705e-06, 'epoch': 1.87}
 62%|██████▎   | 390/624 [34:00<20:50,  5.34s/it] 63%|██████▎   | 391/624 [34:05<20:48,  5.36s/it]                                                 {'loss': 0.7995, 'grad_norm': 4.406943321228027, 'learning_rate': 6.468894950959336e-06, 'epoch': 1.88}
 63%|██████▎   | 391/624 [34:05<20:48,  5.36s/it] 63%|██████▎   | 392/624 [34:10<20:35,  5.33s/it]                                                 {'loss': 0.748, 'grad_norm': 4.304773807525635, 'learning_rate': 6.420360687617897e-06, 'epoch': 1.88}
 63%|██████▎   | 392/624 [34:10<20:35,  5.33s/it] 63%|██████▎   | 393/624 [34:16<20:41,  5.37s/it]                                                 {'loss': 0.777, 'grad_norm': 3.9147653579711914, 'learning_rate': 6.3719229464935915e-06, 'epoch': 1.88}
 63%|██████▎   | 393/624 [34:16<20:41,  5.37s/it] 63%|██████▎   | 394/624 [34:21<20:35,  5.37s/it]                                                 {'loss': 0.7229, 'grad_norm': 4.174623012542725, 'learning_rate': 6.323583033672799e-06, 'epoch': 1.89}
 63%|██████▎   | 394/624 [34:21<20:35,  5.37s/it] 63%|██████▎   | 395/624 [34:26<20:20,  5.33s/it]                                                 {'loss': 0.8808, 'grad_norm': 5.402329444885254, 'learning_rate': 6.275342252604044e-06, 'epoch': 1.89}
 63%|██████▎   | 395/624 [34:26<20:20,  5.33s/it] 63%|██████▎   | 396/624 [34:31<20:02,  5.28s/it]                                                 {'loss': 0.8147, 'grad_norm': 3.719343900680542, 'learning_rate': 6.22720190406283e-06, 'epoch': 1.9}
 63%|██████▎   | 396/624 [34:32<20:02,  5.28s/it] 64%|██████▎   | 397/624 [34:37<19:51,  5.25s/it]                                                 {'loss': 0.8503, 'grad_norm': 4.582362651824951, 'learning_rate': 6.179163286116581e-06, 'epoch': 1.9}
 64%|██████▎   | 397/624 [34:37<19:51,  5.25s/it] 64%|██████▍   | 398/624 [34:42<19:58,  5.30s/it]                                                 {'loss': 0.7437, 'grad_norm': 4.074776649475098, 'learning_rate': 6.13122769408963e-06, 'epoch': 1.91}
 64%|██████▍   | 398/624 [34:42<19:58,  5.30s/it] 64%|██████▍   | 399/624 [34:47<19:31,  5.20s/it]                                                 {'loss': 0.7573, 'grad_norm': 5.531382083892822, 'learning_rate': 6.083396420528298e-06, 'epoch': 1.91}
 64%|██████▍   | 399/624 [34:47<19:31,  5.20s/it] 64%|██████▍   | 400/624 [34:52<19:39,  5.27s/it]                                                 {'loss': 0.7448, 'grad_norm': 4.409023761749268, 'learning_rate': 6.0356707551660434e-06, 'epoch': 1.92}
 64%|██████▍   | 400/624 [34:53<19:39,  5.27s/it] 64%|██████▍   | 401/624 [34:57<19:23,  5.22s/it]                                                 {'loss': 0.7883, 'grad_norm': 3.9570791721343994, 'learning_rate': 5.988051984888668e-06, 'epoch': 1.92}
 64%|██████▍   | 401/624 [34:58<19:23,  5.22s/it] 64%|██████▍   | 402/624 [35:03<19:18,  5.22s/it]                                                 {'loss': 0.8316, 'grad_norm': 4.17433500289917, 'learning_rate': 5.940541393699646e-06, 'epoch': 1.93}
 64%|██████▍   | 402/624 [35:03<19:18,  5.22s/it] 65%|██████▍   | 403/624 [35:08<19:02,  5.17s/it]                                                 {'loss': 0.7295, 'grad_norm': 4.462696552276611, 'learning_rate': 5.893140262685469e-06, 'epoch': 1.93}
 65%|██████▍   | 403/624 [35:08<19:02,  5.17s/it] 65%|██████▍   | 404/624 [35:13<18:39,  5.09s/it]                                                 {'loss': 0.7239, 'grad_norm': 5.059907913208008, 'learning_rate': 5.845849869981137e-06, 'epoch': 1.94}
 65%|██████▍   | 404/624 [35:13<18:39,  5.09s/it] 65%|██████▍   | 405/624 [35:18<18:36,  5.10s/it]                                                 {'loss': 0.8551, 'grad_norm': 4.763144493103027, 'learning_rate': 5.7986714907356614e-06, 'epoch': 1.94}
 65%|██████▍   | 405/624 [35:18<18:36,  5.10s/it] 65%|██████▌   | 406/624 [35:23<18:47,  5.17s/it]                                                 {'loss': 0.8404, 'grad_norm': 3.9578940868377686, 'learning_rate': 5.751606397077703e-06, 'epoch': 1.95}
 65%|██████▌   | 406/624 [35:23<18:47,  5.17s/it] 65%|██████▌   | 407/624 [35:28<18:39,  5.16s/it]                                                 {'loss': 0.7964, 'grad_norm': 4.710572242736816, 'learning_rate': 5.704655858081268e-06, 'epoch': 1.95}
 65%|██████▌   | 407/624 [35:28<18:39,  5.16s/it] 65%|██████▌   | 408/624 [35:33<18:37,  5.17s/it]                                                 {'loss': 0.8388, 'grad_norm': 4.4754862785339355, 'learning_rate': 5.6578211397314765e-06, 'epoch': 1.96}
 65%|██████▌   | 408/624 [35:34<18:37,  5.17s/it] 66%|██████▌   | 409/624 [35:38<18:16,  5.10s/it]                                                 {'loss': 0.7003, 'grad_norm': 4.394439220428467, 'learning_rate': 5.611103504890444e-06, 'epoch': 1.96}
 66%|██████▌   | 409/624 [35:39<18:16,  5.10s/it] 66%|██████▌   | 410/624 [35:44<18:27,  5.17s/it]                                                 {'loss': 0.8517, 'grad_norm': 4.106986999511719, 'learning_rate': 5.564504213263205e-06, 'epoch': 1.97}
 66%|██████▌   | 410/624 [35:44<18:27,  5.17s/it] 66%|██████▌   | 411/624 [35:49<18:35,  5.24s/it]                                                 {'loss': 0.6826, 'grad_norm': 4.917578220367432, 'learning_rate': 5.5180245213637785e-06, 'epoch': 1.97}
 66%|██████▌   | 411/624 [35:49<18:35,  5.24s/it] 66%|██████▌   | 412/624 [35:54<18:35,  5.26s/it]                                                 {'loss': 0.8442, 'grad_norm': 4.793269634246826, 'learning_rate': 5.4716656824812505e-06, 'epoch': 1.98}
 66%|██████▌   | 412/624 [35:55<18:35,  5.26s/it] 66%|██████▌   | 413/624 [36:00<18:22,  5.23s/it]                                                 {'loss': 0.7593, 'grad_norm': 4.0319647789001465, 'learning_rate': 5.425428946646016e-06, 'epoch': 1.98}
 66%|██████▌   | 413/624 [36:00<18:22,  5.23s/it] 66%|██████▋   | 414/624 [36:05<18:12,  5.20s/it]                                                 {'loss': 0.7064, 'grad_norm': 5.20751428604126, 'learning_rate': 5.379315560596038e-06, 'epoch': 1.99}
 66%|██████▋   | 414/624 [36:05<18:12,  5.20s/it] 67%|██████▋   | 415/624 [36:10<18:10,  5.22s/it]                                                 {'loss': 0.7304, 'grad_norm': 4.4214043617248535, 'learning_rate': 5.333326767743263e-06, 'epoch': 1.99}
 67%|██████▋   | 415/624 [36:10<18:10,  5.22s/it] 67%|██████▋   | 416/624 [36:15<17:59,  5.19s/it]                                                 {'loss': 0.7455, 'grad_norm': 4.130659580230713, 'learning_rate': 5.287463808140069e-06, 'epoch': 2.0}
 67%|██████▋   | 416/624 [36:15<17:59,  5.19s/it] 67%|██████▋   | 417/624 [36:21<18:06,  5.25s/it]                                                 {'loss': 0.7714, 'grad_norm': 4.380447864532471, 'learning_rate': 5.241727918445836e-06, 'epoch': 2.0}
 67%|██████▋   | 417/624 [36:21<18:06,  5.25s/it] 67%|██████▋   | 418/624 [36:26<18:10,  5.29s/it]                                                 {'loss': 0.409, 'grad_norm': 4.520937442779541, 'learning_rate': 5.1961203318936116e-06, 'epoch': 2.0}
 67%|██████▋   | 418/624 [36:26<18:10,  5.29s/it] 67%|██████▋   | 419/624 [36:31<17:55,  5.25s/it]                                                 {'loss': 0.2888, 'grad_norm': 3.852587938308716, 'learning_rate': 5.1506422782568345e-06, 'epoch': 2.01}
 67%|██████▋   | 419/624 [36:31<17:55,  5.25s/it] 67%|██████▋   | 420/624 [36:36<17:57,  5.28s/it]                                                 {'loss': 0.3256, 'grad_norm': 3.8156821727752686, 'learning_rate': 5.105294983816203e-06, 'epoch': 2.01}
 67%|██████▋   | 420/624 [36:37<17:57,  5.28s/it] 67%|██████▋   | 421/624 [36:42<17:51,  5.28s/it]                                                 {'loss': 0.2483, 'grad_norm': 3.298682451248169, 'learning_rate': 5.060079671326577e-06, 'epoch': 2.02}
 67%|██████▋   | 421/624 [36:42<17:51,  5.28s/it] 68%|██████▊   | 422/624 [36:47<17:24,  5.17s/it]                                                 {'loss': 0.2839, 'grad_norm': 3.3857784271240234, 'learning_rate': 5.014997559984045e-06, 'epoch': 2.02}
 68%|██████▊   | 422/624 [36:47<17:24,  5.17s/it] 68%|██████▊   | 423/624 [36:52<17:23,  5.19s/it]                                                 {'loss': 0.2258, 'grad_norm': 4.160032749176025, 'learning_rate': 4.970049865393009e-06, 'epoch': 2.03}
 68%|██████▊   | 423/624 [36:52<17:23,  5.19s/it] 68%|██████▊   | 424/624 [36:57<17:18,  5.19s/it]                                                 {'loss': 0.2322, 'grad_norm': 5.156760215759277, 'learning_rate': 4.925237799533445e-06, 'epoch': 2.03}
 68%|██████▊   | 424/624 [36:57<17:18,  5.19s/it] 68%|██████▊   | 425/624 [37:02<16:57,  5.11s/it]                                                 {'loss': 0.2043, 'grad_norm': 5.592225074768066, 'learning_rate': 4.880562570728188e-06, 'epoch': 2.04}
 68%|██████▊   | 425/624 [37:02<16:57,  5.11s/it] 68%|██████▊   | 426/624 [37:07<16:50,  5.10s/it]                                                 {'loss': 0.3792, 'grad_norm': 9.521315574645996, 'learning_rate': 4.836025383610382e-06, 'epoch': 2.04}
 68%|██████▊   | 426/624 [37:07<16:50,  5.10s/it] 68%|██████▊   | 427/624 [37:12<16:59,  5.17s/it]                                                 {'loss': 0.3421, 'grad_norm': 7.5944294929504395, 'learning_rate': 4.791627439090975e-06, 'epoch': 2.05}
 68%|██████▊   | 427/624 [37:13<16:59,  5.17s/it] 69%|██████▊   | 428/624 [37:17<16:30,  5.06s/it]                                                 {'loss': 0.2496, 'grad_norm': 6.139709949493408, 'learning_rate': 4.74736993432634e-06, 'epoch': 2.05}
 69%|██████▊   | 428/624 [37:17<16:30,  5.06s/it] 69%|██████▉   | 429/624 [37:23<16:51,  5.19s/it]                                                 {'loss': 0.258, 'grad_norm': 4.466383934020996, 'learning_rate': 4.703254062686017e-06, 'epoch': 2.06}
 69%|██████▉   | 429/624 [37:23<16:51,  5.19s/it] 69%|██████▉   | 430/624 [37:28<16:41,  5.16s/it]                                                 {'loss': 0.2619, 'grad_norm': 5.383742332458496, 'learning_rate': 4.6592810137205e-06, 'epoch': 2.06}
 69%|██████▉   | 430/624 [37:28<16:41,  5.16s/it] 69%|██████▉   | 431/624 [37:33<16:24,  5.10s/it]                                                 {'loss': 0.2095, 'grad_norm': 3.658597946166992, 'learning_rate': 4.615451973129196e-06, 'epoch': 2.07}
 69%|██████▉   | 431/624 [37:33<16:24,  5.10s/it] 69%|██████▉   | 432/624 [37:38<16:15,  5.08s/it]                                                 {'loss': 0.2045, 'grad_norm': 3.5274176597595215, 'learning_rate': 4.571768122728421e-06, 'epoch': 2.07}
 69%|██████▉   | 432/624 [37:38<16:15,  5.08s/it] 69%|██████▉   | 433/624 [37:43<16:11,  5.09s/it]                                                 {'loss': 0.2264, 'grad_norm': 3.4570419788360596, 'learning_rate': 4.528230640419562e-06, 'epoch': 2.08}
 69%|██████▉   | 433/624 [37:43<16:11,  5.09s/it] 70%|██████▉   | 434/624 [37:48<16:33,  5.23s/it]                                                 {'loss': 0.315, 'grad_norm': 3.703155040740967, 'learning_rate': 4.4848407001572945e-06, 'epoch': 2.08}
 70%|██████▉   | 434/624 [37:49<16:33,  5.23s/it] 70%|██████▉   | 435/624 [37:53<16:18,  5.18s/it]                                                 {'loss': 0.2681, 'grad_norm': 3.6164941787719727, 'learning_rate': 4.441599471917946e-06, 'epoch': 2.09}
 70%|██████▉   | 435/624 [37:54<16:18,  5.18s/it] 70%|██████▉   | 436/624 [37:59<16:07,  5.14s/it]                                                 {'loss': 0.2766, 'grad_norm': 3.7651114463806152, 'learning_rate': 4.398508121667925e-06, 'epoch': 2.09}
 70%|██████▉   | 436/624 [37:59<16:07,  5.14s/it] 70%|███████   | 437/624 [38:04<16:26,  5.28s/it]                                                 {'loss': 0.3006, 'grad_norm': 4.8075151443481445, 'learning_rate': 4.355567811332311e-06, 'epoch': 2.1}
 70%|███████   | 437/624 [38:04<16:26,  5.28s/it] 70%|███████   | 438/624 [38:10<16:45,  5.41s/it]                                                 {'loss': 0.2558, 'grad_norm': 3.534662961959839, 'learning_rate': 4.312779698763493e-06, 'epoch': 2.1}
 70%|███████   | 438/624 [38:10<16:45,  5.41s/it] 70%|███████   | 439/624 [38:15<16:14,  5.27s/it]                                                 {'loss': 0.2064, 'grad_norm': 3.5513756275177, 'learning_rate': 4.270144937709981e-06, 'epoch': 2.11}
 70%|███████   | 439/624 [38:15<16:14,  5.27s/it] 71%|███████   | 440/624 [38:20<16:10,  5.28s/it]                                                 {'loss': 0.2774, 'grad_norm': 3.6343278884887695, 'learning_rate': 4.227664677785264e-06, 'epoch': 2.11}
 71%|███████   | 440/624 [38:20<16:10,  5.28s/it] 71%|███████   | 441/624 [38:25<15:58,  5.24s/it]                                                 {'loss': 0.2238, 'grad_norm': 4.1133880615234375, 'learning_rate': 4.1853400644368395e-06, 'epoch': 2.12}
 71%|███████   | 441/624 [38:25<15:58,  5.24s/it] 71%|███████   | 442/624 [38:30<15:52,  5.23s/it]                                                 {'loss': 0.2119, 'grad_norm': 4.067040920257568, 'learning_rate': 4.143172238915302e-06, 'epoch': 2.12}
 71%|███████   | 442/624 [38:31<15:52,  5.23s/it] 71%|███████   | 443/624 [38:36<15:48,  5.24s/it]                                                 {'loss': 0.2109, 'grad_norm': 4.122867584228516, 'learning_rate': 4.101162338243595e-06, 'epoch': 2.12}
 71%|███████   | 443/624 [38:36<15:48,  5.24s/it] 71%|███████   | 444/624 [38:41<15:42,  5.23s/it]                                                 {'loss': 0.2439, 'grad_norm': 3.8299593925476074, 'learning_rate': 4.059311495186338e-06, 'epoch': 2.13}
 71%|███████   | 444/624 [38:41<15:42,  5.23s/it] 71%|███████▏  | 445/624 [38:46<15:42,  5.27s/it]                                                 {'loss': 0.3097, 'grad_norm': 5.237226963043213, 'learning_rate': 4.017620838219276e-06, 'epoch': 2.13}
 71%|███████▏  | 445/624 [38:46<15:42,  5.27s/it] 71%|███████▏  | 446/624 [38:51<15:30,  5.23s/it]                                                 {'loss': 0.2791, 'grad_norm': 4.223593711853027, 'learning_rate': 3.9760914914988716e-06, 'epoch': 2.14}
 71%|███████▏  | 446/624 [38:52<15:30,  5.23s/it] 72%|███████▏  | 447/624 [38:56<15:15,  5.17s/it]                                                 {'loss': 0.2883, 'grad_norm': 4.8493757247924805, 'learning_rate': 3.93472457483197e-06, 'epoch': 2.14}
 72%|███████▏  | 447/624 [38:57<15:15,  5.17s/it] 72%|███████▏  | 448/624 [39:02<15:21,  5.24s/it]                                                 {'loss': 0.2212, 'grad_norm': 3.921936273574829, 'learning_rate': 3.893521203645618e-06, 'epoch': 2.15}
 72%|███████▏  | 448/624 [39:02<15:21,  5.24s/it] 72%|███████▏  | 449/624 [39:07<15:00,  5.15s/it]                                                 {'loss': 0.3311, 'grad_norm': 5.59843111038208, 'learning_rate': 3.852482488956992e-06, 'epoch': 2.15}
 72%|███████▏  | 449/624 [39:07<15:00,  5.15s/it] 72%|███████▏  | 450/624 [39:12<15:01,  5.18s/it]                                                 {'loss': 0.2552, 'grad_norm': 4.214529991149902, 'learning_rate': 3.8116095373434204e-06, 'epoch': 2.16}
 72%|███████▏  | 450/624 [39:12<15:01,  5.18s/it] 72%|███████▏  | 451/624 [39:17<14:53,  5.17s/it]                                                 {'loss': 0.3046, 'grad_norm': 4.228176116943359, 'learning_rate': 3.7709034509125706e-06, 'epoch': 2.16}
 72%|███████▏  | 451/624 [39:17<14:53,  5.17s/it] 72%|███████▏  | 452/624 [39:22<14:54,  5.20s/it]                                                 {'loss': 0.2989, 'grad_norm': 4.2126312255859375, 'learning_rate': 3.7303653272727057e-06, 'epoch': 2.17}
 72%|███████▏  | 452/624 [39:23<14:54,  5.20s/it] 73%|███████▎  | 453/624 [39:27<14:39,  5.14s/it]                                                 {'loss': 0.3089, 'grad_norm': 3.8184027671813965, 'learning_rate': 3.689996259503116e-06, 'epoch': 2.17}
 73%|███████▎  | 453/624 [39:28<14:39,  5.14s/it] 73%|███████▎  | 454/624 [39:33<14:49,  5.23s/it]                                                 {'loss': 0.284, 'grad_norm': 3.944885730743408, 'learning_rate': 3.6497973361246153e-06, 'epoch': 2.18}
 73%|███████▎  | 454/624 [39:33<14:49,  5.23s/it] 73%|███████▎  | 455/624 [39:38<14:48,  5.26s/it]                                                 {'loss': 0.2355, 'grad_norm': 3.4868435859680176, 'learning_rate': 3.609769641070221e-06, 'epoch': 2.18}
 73%|███████▎  | 455/624 [39:38<14:48,  5.26s/it] 73%|███████▎  | 456/624 [39:44<14:47,  5.28s/it]                                                 {'loss': 0.249, 'grad_norm': 3.442889451980591, 'learning_rate': 3.569914253655896e-06, 'epoch': 2.19}
 73%|███████▎  | 456/624 [39:44<14:47,  5.28s/it] 73%|███████▎  | 457/624 [39:49<14:52,  5.34s/it]                                                 {'loss': 0.2194, 'grad_norm': 3.296355724334717, 'learning_rate': 3.530232248551466e-06, 'epoch': 2.19}
 73%|███████▎  | 457/624 [39:49<14:52,  5.34s/it] 73%|███████▎  | 458/624 [39:54<14:44,  5.33s/it]                                                 {'loss': 0.3191, 'grad_norm': 4.297990322113037, 'learning_rate': 3.4907246957516416e-06, 'epoch': 2.2}
 73%|███████▎  | 458/624 [39:54<14:44,  5.33s/it] 74%|███████▎  | 459/624 [39:59<14:22,  5.23s/it]                                                 {'loss': 0.3065, 'grad_norm': 3.779092788696289, 'learning_rate': 3.4513926605471504e-06, 'epoch': 2.2}
 74%|███████▎  | 459/624 [39:59<14:22,  5.23s/it] 74%|███████▎  | 460/624 [40:05<14:17,  5.23s/it]                                                 {'loss': 0.297, 'grad_norm': 4.501718997955322, 'learning_rate': 3.412237203496036e-06, 'epoch': 2.21}
 74%|███████▎  | 460/624 [40:05<14:17,  5.23s/it] 74%|███████▍  | 461/624 [40:10<14:13,  5.24s/it]                                                 {'loss': 0.252, 'grad_norm': 4.315439701080322, 'learning_rate': 3.3732593803950354e-06, 'epoch': 2.21}
 74%|███████▍  | 461/624 [40:10<14:13,  5.24s/it] 74%|███████▍  | 462/624 [40:15<14:08,  5.24s/it]                                                 {'loss': 0.3256, 'grad_norm': 3.8469467163085938, 'learning_rate': 3.3344602422511343e-06, 'epoch': 2.22}
 74%|███████▍  | 462/624 [40:15<14:08,  5.24s/it] 74%|███████▍  | 463/624 [40:20<13:58,  5.21s/it]                                                 {'loss': 0.2542, 'grad_norm': 4.093044757843018, 'learning_rate': 3.2958408352532055e-06, 'epoch': 2.22}
 74%|███████▍  | 463/624 [40:20<13:58,  5.21s/it] 74%|███████▍  | 464/624 [40:26<13:58,  5.24s/it]                                                 {'loss': 0.3969, 'grad_norm': 5.279123306274414, 'learning_rate': 3.257402200743821e-06, 'epoch': 2.23}
 74%|███████▍  | 464/624 [40:26<13:58,  5.24s/it] 75%|███████▍  | 465/624 [40:31<13:55,  5.25s/it]                                                 {'loss': 0.2504, 'grad_norm': 3.9511263370513916, 'learning_rate': 3.2191453751911505e-06, 'epoch': 2.23}
 75%|███████▍  | 465/624 [40:31<13:55,  5.25s/it] 75%|███████▍  | 466/624 [40:36<13:51,  5.26s/it]                                                 {'loss': 0.2451, 'grad_norm': 3.7047736644744873, 'learning_rate': 3.1810713901610367e-06, 'epoch': 2.24}
 75%|███████▍  | 466/624 [40:36<13:51,  5.26s/it] 75%|███████▍  | 467/624 [40:41<13:38,  5.22s/it]                                                 {'loss': 0.1921, 'grad_norm': 3.874387741088867, 'learning_rate': 3.1431812722891598e-06, 'epoch': 2.24}
 75%|███████▍  | 467/624 [40:41<13:38,  5.22s/it] 75%|███████▌  | 468/624 [40:47<13:42,  5.27s/it]                                                 {'loss': 0.2177, 'grad_norm': 3.5695409774780273, 'learning_rate': 3.1054760432533626e-06, 'epoch': 2.24}
 75%|███████▌  | 468/624 [40:47<13:42,  5.27s/it] 75%|███████▌  | 469/624 [40:52<13:28,  5.22s/it]                                                 {'loss': 0.2899, 'grad_norm': 4.698369026184082, 'learning_rate': 3.0679567197461135e-06, 'epoch': 2.25}
 75%|███████▌  | 469/624 [40:52<13:28,  5.22s/it] 75%|███████▌  | 470/624 [40:57<13:36,  5.31s/it]                                                 {'loss': 0.233, 'grad_norm': 3.974428176879883, 'learning_rate': 3.0306243134470668e-06, 'epoch': 2.25}
 75%|███████▌  | 470/624 [40:57<13:36,  5.31s/it] 75%|███████▌  | 471/624 [41:03<13:33,  5.32s/it]                                                 {'loss': 0.2688, 'grad_norm': 3.2134323120117188, 'learning_rate': 2.993479830995815e-06, 'epoch': 2.26}
 75%|███████▌  | 471/624 [41:03<13:33,  5.32s/it] 76%|███████▌  | 472/624 [41:08<13:26,  5.31s/it]                                                 {'loss': 0.218, 'grad_norm': 3.7025551795959473, 'learning_rate': 2.9565242739647115e-06, 'epoch': 2.26}
 76%|███████▌  | 472/624 [41:08<13:26,  5.31s/it] 76%|███████▌  | 473/624 [41:13<13:21,  5.31s/it]                                                 {'loss': 0.2146, 'grad_norm': 3.291374921798706, 'learning_rate': 2.919758638831893e-06, 'epoch': 2.27}
 76%|███████▌  | 473/624 [41:13<13:21,  5.31s/it] 76%|███████▌  | 474/624 [41:19<13:24,  5.36s/it]                                                 {'loss': 0.2609, 'grad_norm': 4.584666728973389, 'learning_rate': 2.8831839169543998e-06, 'epoch': 2.27}
 76%|███████▌  | 474/624 [41:19<13:24,  5.36s/it] 76%|███████▌  | 475/624 [41:24<13:17,  5.35s/it]                                                 {'loss': 0.2453, 'grad_norm': 3.5421206951141357, 'learning_rate': 2.84680109454143e-06, 'epoch': 2.28}
 76%|███████▌  | 475/624 [41:24<13:17,  5.35s/it] 76%|███████▋  | 476/624 [41:29<13:12,  5.35s/it]                                                 {'loss': 0.2852, 'grad_norm': 3.5238311290740967, 'learning_rate': 2.810611152627777e-06, 'epoch': 2.28}
 76%|███████▋  | 476/624 [41:29<13:12,  5.35s/it] 76%|███████▋  | 477/624 [41:34<12:58,  5.30s/it]                                                 {'loss': 0.2996, 'grad_norm': 4.047956943511963, 'learning_rate': 2.774615067047346e-06, 'epoch': 2.29}
 76%|███████▋  | 477/624 [41:35<12:58,  5.30s/it] 77%|███████▋  | 478/624 [41:40<12:45,  5.25s/it]                                                 {'loss': 0.2649, 'grad_norm': 3.485034704208374, 'learning_rate': 2.738813808406866e-06, 'epoch': 2.29}
 77%|███████▋  | 478/624 [41:40<12:45,  5.25s/it] 77%|███████▋  | 479/624 [41:45<12:28,  5.16s/it]                                                 {'loss': 0.2812, 'grad_norm': 4.036945343017578, 'learning_rate': 2.7032083420597e-06, 'epoch': 2.3}
 77%|███████▋  | 479/624 [41:45<12:28,  5.16s/it] 77%|███████▋  | 480/624 [41:50<12:22,  5.15s/it]                                                 {'loss': 0.206, 'grad_norm': 3.580986738204956, 'learning_rate': 2.667799628079829e-06, 'epoch': 2.3}
 77%|███████▋  | 480/624 [41:50<12:22,  5.15s/it] 77%|███████▋  | 481/624 [41:55<12:11,  5.12s/it]                                                 {'loss': 0.2371, 'grad_norm': 3.7750062942504883, 'learning_rate': 2.6325886212359496e-06, 'epoch': 2.31}
 77%|███████▋  | 481/624 [41:55<12:11,  5.12s/it] 77%|███████▋  | 482/624 [42:00<12:17,  5.19s/it]                                                 {'loss': 0.2888, 'grad_norm': 4.25986385345459, 'learning_rate': 2.5975762709657506e-06, 'epoch': 2.31}
 77%|███████▋  | 482/624 [42:00<12:17,  5.19s/it] 77%|███████▋  | 483/624 [42:05<12:01,  5.11s/it]                                                 {'loss': 0.2621, 'grad_norm': 4.1291422843933105, 'learning_rate': 2.5627635213502832e-06, 'epoch': 2.32}
 77%|███████▋  | 483/624 [42:05<12:01,  5.11s/it] 78%|███████▊  | 484/624 [42:10<12:04,  5.17s/it]                                                 {'loss': 0.2662, 'grad_norm': 3.5414655208587646, 'learning_rate': 2.528151311088537e-06, 'epoch': 2.32}
 78%|███████▊  | 484/624 [42:10<12:04,  5.17s/it] 78%|███████▊  | 485/624 [42:16<12:19,  5.32s/it]                                                 {'loss': 0.1667, 'grad_norm': 3.2379536628723145, 'learning_rate': 2.4937405734720964e-06, 'epoch': 2.33}
 78%|███████▊  | 485/624 [42:16<12:19,  5.32s/it] 78%|███████▊  | 486/624 [42:21<12:12,  5.31s/it]                                                 {'loss': 0.1943, 'grad_norm': 5.0521745681762695, 'learning_rate': 2.459532236360007e-06, 'epoch': 2.33}
 78%|███████▊  | 486/624 [42:21<12:12,  5.31s/it] 78%|███████▊  | 487/624 [42:26<12:02,  5.27s/it]                                                 {'loss': 0.2017, 'grad_norm': 3.355663299560547, 'learning_rate': 2.4255272221537295e-06, 'epoch': 2.34}
 78%|███████▊  | 487/624 [42:27<12:02,  5.27s/it] 78%|███████▊  | 488/624 [42:32<11:48,  5.21s/it]                                                 {'loss': 0.2854, 'grad_norm': 4.7340288162231445, 'learning_rate': 2.391726447772279e-06, 'epoch': 2.34}
 78%|███████▊  | 488/624 [42:32<11:48,  5.21s/it] 78%|███████▊  | 489/624 [42:37<11:38,  5.18s/it]                                                 {'loss': 0.2864, 'grad_norm': 4.412981033325195, 'learning_rate': 2.3581308246275103e-06, 'epoch': 2.35}
 78%|███████▊  | 489/624 [42:37<11:38,  5.18s/it] 79%|███████▊  | 490/624 [42:42<11:33,  5.17s/it]                                                 {'loss': 0.274, 'grad_norm': 4.027647495269775, 'learning_rate': 2.324741258599521e-06, 'epoch': 2.35}
 79%|███████▊  | 490/624 [42:42<11:33,  5.17s/it] 79%|███████▊  | 491/624 [42:47<11:34,  5.22s/it]                                                 {'loss': 0.2529, 'grad_norm': 4.191167831420898, 'learning_rate': 2.29155865001225e-06, 'epoch': 2.35}
 79%|███████▊  | 491/624 [42:47<11:34,  5.22s/it] 79%|███████▉  | 492/624 [42:52<11:33,  5.26s/it]                                                 {'loss': 0.3369, 'grad_norm': 4.269003868103027, 'learning_rate': 2.2585838936091753e-06, 'epoch': 2.36}
 79%|███████▉  | 492/624 [42:53<11:33,  5.26s/it] 79%|███████▉  | 493/624 [42:58<11:26,  5.24s/it]                                                 {'loss': 0.3216, 'grad_norm': 4.231429100036621, 'learning_rate': 2.225817878529214e-06, 'epoch': 2.36}
 79%|███████▉  | 493/624 [42:58<11:26,  5.24s/it] 79%|███████▉  | 494/624 [43:03<11:20,  5.24s/it]                                                 {'loss': 0.1967, 'grad_norm': 4.119096755981445, 'learning_rate': 2.1932614882827196e-06, 'epoch': 2.37}
 79%|███████▉  | 494/624 [43:03<11:20,  5.24s/it] 79%|███████▉  | 495/624 [43:08<11:13,  5.22s/it]                                                 {'loss': 0.2697, 'grad_norm': 4.2872724533081055, 'learning_rate': 2.160915600727688e-06, 'epoch': 2.37}
 79%|███████▉  | 495/624 [43:08<11:13,  5.22s/it] 79%|███████▉  | 496/624 [43:13<10:59,  5.15s/it]                                                 {'loss': 0.3083, 'grad_norm': 4.127578258514404, 'learning_rate': 2.1287810880460636e-06, 'epoch': 2.38}
 79%|███████▉  | 496/624 [43:13<10:59,  5.15s/it] 80%|███████▉  | 497/624 [43:18<10:58,  5.18s/it]                                                 {'loss': 0.3395, 'grad_norm': 4.138027191162109, 'learning_rate': 2.0968588167202265e-06, 'epoch': 2.38}
 80%|███████▉  | 497/624 [43:18<10:58,  5.18s/it] 80%|███████▉  | 498/624 [43:24<10:58,  5.23s/it]                                                 {'loss': 0.3086, 'grad_norm': 3.6903090476989746, 'learning_rate': 2.0651496475096455e-06, 'epoch': 2.39}
 80%|███████▉  | 498/624 [43:24<10:58,  5.23s/it] 80%|███████▉  | 499/624 [43:29<10:47,  5.18s/it]                                                 {'loss': 0.2722, 'grad_norm': 3.9197850227355957, 'learning_rate': 2.03365443542764e-06, 'epoch': 2.39}
 80%|███████▉  | 499/624 [43:29<10:47,  5.18s/it] 80%|████████  | 500/624 [43:34<10:47,  5.22s/it]                                                 {'loss': 0.2545, 'grad_norm': 3.9000630378723145, 'learning_rate': 2.0023740297183536e-06, 'epoch': 2.4}
 80%|████████  | 500/624 [43:34<10:47,  5.22s/it] 80%|████████  | 501/624 [43:39<10:31,  5.13s/it]                                                 {'loss': 0.2275, 'grad_norm': 3.2359514236450195, 'learning_rate': 1.971309273833828e-06, 'epoch': 2.4}
 80%|████████  | 501/624 [43:39<10:31,  5.13s/it] 80%|████████  | 502/624 [43:44<10:28,  5.15s/it]                                                 {'loss': 0.3047, 'grad_norm': 4.128240585327148, 'learning_rate': 1.940461005411288e-06, 'epoch': 2.41}
 80%|████████  | 502/624 [43:44<10:28,  5.15s/it] 81%|████████  | 503/624 [43:49<10:20,  5.13s/it]                                                 {'loss': 0.3705, 'grad_norm': 4.434199333190918, 'learning_rate': 1.9098300562505266e-06, 'epoch': 2.41}
 81%|████████  | 503/624 [43:49<10:20,  5.13s/it] 81%|████████  | 504/624 [43:54<10:16,  5.13s/it]                                                 {'loss': 0.2174, 'grad_norm': 3.5118234157562256, 'learning_rate': 1.8794172522915022e-06, 'epoch': 2.42}
 81%|████████  | 504/624 [43:55<10:16,  5.13s/it] 81%|████████  | 505/624 [44:00<10:22,  5.23s/it]                                                 {'loss': 0.2229, 'grad_norm': 3.178978204727173, 'learning_rate': 1.849223413592046e-06, 'epoch': 2.42}
 81%|████████  | 505/624 [44:00<10:22,  5.23s/it] 81%|████████  | 506/624 [44:05<10:24,  5.29s/it]                                                 {'loss': 0.2777, 'grad_norm': 4.086475372314453, 'learning_rate': 1.8192493543057676e-06, 'epoch': 2.43}
 81%|████████  | 506/624 [44:05<10:24,  5.29s/it] 81%|████████▏ | 507/624 [44:11<10:20,  5.30s/it]                                                 {'loss': 0.3291, 'grad_norm': 4.120648384094238, 'learning_rate': 1.7894958826600884e-06, 'epoch': 2.43}
 81%|████████▏ | 507/624 [44:11<10:20,  5.30s/it] 81%|████████▏ | 508/624 [44:16<10:11,  5.27s/it]                                                 {'loss': 0.199, 'grad_norm': 4.123744010925293, 'learning_rate': 1.7599638009344566e-06, 'epoch': 2.44}
 81%|████████▏ | 508/624 [44:16<10:11,  5.27s/it] 82%|████████▏ | 509/624 [44:21<10:03,  5.25s/it]                                                 {'loss': 0.2463, 'grad_norm': 4.766903400421143, 'learning_rate': 1.730653905438714e-06, 'epoch': 2.44}
 82%|████████▏ | 509/624 [44:21<10:03,  5.25s/it] 82%|████████▏ | 510/624 [44:26<10:01,  5.28s/it]                                                 {'loss': 0.2132, 'grad_norm': 3.1404576301574707, 'learning_rate': 1.701566986491614e-06, 'epoch': 2.45}
 82%|████████▏ | 510/624 [44:26<10:01,  5.28s/it] 82%|████████▏ | 511/624 [44:31<09:48,  5.20s/it]                                                 {'loss': 0.3541, 'grad_norm': 3.6744565963745117, 'learning_rate': 1.672703828399529e-06, 'epoch': 2.45}
 82%|████████▏ | 511/624 [44:32<09:48,  5.20s/it] 82%|████████▏ | 512/624 [44:37<09:48,  5.25s/it]                                                 {'loss': 0.2008, 'grad_norm': 2.9511592388153076, 'learning_rate': 1.6440652094352838e-06, 'epoch': 2.46}
 82%|████████▏ | 512/624 [44:37<09:48,  5.25s/it] 82%|████████▏ | 513/624 [44:42<09:40,  5.23s/it]                                                 {'loss': 0.3026, 'grad_norm': 4.237091064453125, 'learning_rate': 1.6156519018171856e-06, 'epoch': 2.46}
 82%|████████▏ | 513/624 [44:42<09:40,  5.23s/it] 82%|████████▏ | 514/624 [44:47<09:26,  5.15s/it]                                                 {'loss': 0.2838, 'grad_norm': 3.8135547637939453, 'learning_rate': 1.587464671688187e-06, 'epoch': 2.47}
 82%|████████▏ | 514/624 [44:47<09:26,  5.15s/it] 83%|████████▎ | 515/624 [44:52<09:27,  5.21s/it]                                                 {'loss': 0.2366, 'grad_norm': 3.6363000869750977, 'learning_rate': 1.5595042790952442e-06, 'epoch': 2.47}
 83%|████████▎ | 515/624 [44:52<09:27,  5.21s/it] 83%|████████▎ | 516/624 [44:58<09:26,  5.24s/it]                                                 {'loss': 0.2632, 'grad_norm': 3.4902873039245605, 'learning_rate': 1.5317714779688076e-06, 'epoch': 2.47}
 83%|████████▎ | 516/624 [44:58<09:26,  5.24s/it] 83%|████████▎ | 517/624 [45:02<09:07,  5.11s/it]                                                 {'loss': 0.1759, 'grad_norm': 3.936453104019165, 'learning_rate': 1.5042670161024975e-06, 'epoch': 2.48}
 83%|████████▎ | 517/624 [45:03<09:07,  5.11s/it] 83%|████████▎ | 518/624 [45:08<09:12,  5.21s/it]                                                 {'loss': 0.3069, 'grad_norm': 3.546053409576416, 'learning_rate': 1.4769916351329495e-06, 'epoch': 2.48}
 83%|████████▎ | 518/624 [45:08<09:12,  5.21s/it] 83%|████████▎ | 519/624 [45:13<09:04,  5.19s/it]                                                 {'loss': 0.3175, 'grad_norm': 4.3092265129089355, 'learning_rate': 1.4499460705198e-06, 'epoch': 2.49}
 83%|████████▎ | 519/624 [45:13<09:04,  5.19s/it] 83%|████████▎ | 520/624 [45:18<09:03,  5.23s/it]                                                 {'loss': 0.236, 'grad_norm': 4.1155104637146, 'learning_rate': 1.4231310515258745e-06, 'epoch': 2.49}
 83%|████████▎ | 520/624 [45:18<09:03,  5.23s/it] 83%|████████▎ | 521/624 [45:24<09:06,  5.30s/it]                                                 {'loss': 0.2436, 'grad_norm': 3.7661001682281494, 'learning_rate': 1.396547301197504e-06, 'epoch': 2.5}
 83%|████████▎ | 521/624 [45:24<09:06,  5.30s/it] 84%|████████▎ | 522/624 [45:29<08:57,  5.27s/it]                                                 {'loss': 0.2285, 'grad_norm': 4.048346042633057, 'learning_rate': 1.3701955363450447e-06, 'epoch': 2.5}
 84%|████████▎ | 522/624 [45:29<08:57,  5.27s/it] 84%|████████▍ | 523/624 [45:34<08:50,  5.25s/it]                                                 {'loss': 0.1945, 'grad_norm': 3.3979804515838623, 'learning_rate': 1.3440764675235384e-06, 'epoch': 2.51}
 84%|████████▍ | 523/624 [45:34<08:50,  5.25s/it] 84%|████████▍ | 524/624 [45:40<08:50,  5.31s/it]                                                 {'loss': 0.3932, 'grad_norm': 3.7261598110198975, 'learning_rate': 1.3181907990135624e-06, 'epoch': 2.51}
 84%|████████▍ | 524/624 [45:40<08:50,  5.31s/it] 84%|████████▍ | 525/624 [45:45<08:51,  5.37s/it]                                                 {'loss': 0.2621, 'grad_norm': 4.056080341339111, 'learning_rate': 1.2925392288022299e-06, 'epoch': 2.52}
 84%|████████▍ | 525/624 [45:45<08:51,  5.37s/it] 84%|████████▍ | 526/624 [45:50<08:39,  5.30s/it]                                                 {'loss': 0.2315, 'grad_norm': 3.986126661300659, 'learning_rate': 1.267122448564374e-06, 'epoch': 2.52}
 84%|████████▍ | 526/624 [45:50<08:39,  5.30s/it] 84%|████████▍ | 527/624 [45:55<08:32,  5.29s/it]                                                 {'loss': 0.2931, 'grad_norm': 3.7216334342956543, 'learning_rate': 1.2419411436439021e-06, 'epoch': 2.53}
 84%|████████▍ | 527/624 [45:56<08:32,  5.29s/it] 85%|████████▍ | 528/624 [46:00<08:17,  5.18s/it]                                                 {'loss': 0.239, 'grad_norm': 3.8388447761535645, 'learning_rate': 1.2169959930353049e-06, 'epoch': 2.53}
 85%|████████▍ | 528/624 [46:01<08:17,  5.18s/it] 85%|████████▍ | 529/624 [46:06<08:13,  5.20s/it]                                                 {'loss': 0.2621, 'grad_norm': 4.590907573699951, 'learning_rate': 1.1922876693653584e-06, 'epoch': 2.54}
 85%|████████▍ | 529/624 [46:06<08:13,  5.20s/it] 85%|████████▍ | 530/624 [46:11<08:07,  5.18s/it]                                                 {'loss': 0.2053, 'grad_norm': 3.9238929748535156, 'learning_rate': 1.1678168388749788e-06, 'epoch': 2.54}
 85%|████████▍ | 530/624 [46:11<08:07,  5.18s/it] 85%|████████▌ | 531/624 [46:16<08:02,  5.19s/it]                                                 {'loss': 0.2507, 'grad_norm': 4.458306312561035, 'learning_rate': 1.1435841614012666e-06, 'epoch': 2.55}
 85%|████████▌ | 531/624 [46:16<08:02,  5.19s/it] 85%|████████▌ | 532/624 [46:21<08:05,  5.28s/it]                                                 {'loss': 0.174, 'grad_norm': 3.34944748878479, 'learning_rate': 1.1195902903597023e-06, 'epoch': 2.55}
 85%|████████▌ | 532/624 [46:22<08:05,  5.28s/it] 85%|████████▌ | 533/624 [46:27<07:56,  5.24s/it]                                                 {'loss': 0.2444, 'grad_norm': 3.7996599674224854, 'learning_rate': 1.0958358727265438e-06, 'epoch': 2.56}
 85%|████████▌ | 533/624 [46:27<07:56,  5.24s/it] 86%|████████▌ | 534/624 [46:32<07:57,  5.30s/it]                                                 {'loss': 0.2562, 'grad_norm': 4.132776737213135, 'learning_rate': 1.0723215490213635e-06, 'epoch': 2.56}
 86%|████████▌ | 534/624 [46:32<07:57,  5.30s/it] 86%|████████▌ | 535/624 [46:38<07:58,  5.38s/it]                                                 {'loss': 0.3143, 'grad_norm': 4.185009002685547, 'learning_rate': 1.0490479532897946e-06, 'epoch': 2.57}
 86%|████████▌ | 535/624 [46:38<07:58,  5.38s/it] 86%|████████▌ | 536/624 [46:43<07:50,  5.35s/it]                                                 {'loss': 0.2982, 'grad_norm': 3.9141244888305664, 'learning_rate': 1.0260157130864178e-06, 'epoch': 2.57}
 86%|████████▌ | 536/624 [46:43<07:50,  5.35s/it] 86%|████████▌ | 537/624 [46:48<07:45,  5.35s/it]                                                 {'loss': 0.2332, 'grad_norm': 3.6331429481506348, 'learning_rate': 1.0032254494578519e-06, 'epoch': 2.58}
 86%|████████▌ | 537/624 [46:48<07:45,  5.35s/it] 86%|████████▌ | 538/624 [46:53<07:33,  5.27s/it]                                                 {'loss': 0.3242, 'grad_norm': 4.362876892089844, 'learning_rate': 9.806777769260034e-07, 'epoch': 2.58}
 86%|████████▌ | 538/624 [46:53<07:33,  5.27s/it] 86%|████████▋ | 539/624 [46:59<07:26,  5.25s/it]                                                 {'loss': 0.4339, 'grad_norm': 6.190732479095459, 'learning_rate': 9.583733034714982e-07, 'epoch': 2.59}
 86%|████████▋ | 539/624 [46:59<07:26,  5.25s/it] 87%|████████▋ | 540/624 [47:04<07:19,  5.23s/it]                                                 {'loss': 0.1981, 'grad_norm': 3.6061460971832275, 'learning_rate': 9.363126305172831e-07, 'epoch': 2.59}
 87%|████████▋ | 540/624 [47:04<07:19,  5.23s/it] 87%|████████▋ | 541/624 [47:09<07:19,  5.30s/it]                                                 {'loss': 0.3193, 'grad_norm': 4.624227523803711, 'learning_rate': 9.144963529124163e-07, 'epoch': 2.59}
 87%|████████▋ | 541/624 [47:09<07:19,  5.30s/it] 87%|████████▋ | 542/624 [47:15<07:15,  5.32s/it]                                                 {'loss': 0.3026, 'grad_norm': 4.996110916137695, 'learning_rate': 8.929250589160166e-07, 'epoch': 2.6}
 87%|████████▋ | 542/624 [47:15<07:15,  5.32s/it] 87%|████████▋ | 543/624 [47:20<07:08,  5.28s/it]                                                 {'loss': 0.2345, 'grad_norm': 3.675525188446045, 'learning_rate': 8.715993301814174e-07, 'epoch': 2.6}
 87%|████████▋ | 543/624 [47:20<07:08,  5.28s/it] 87%|████████▋ | 544/624 [47:25<06:58,  5.24s/it]                                                 {'loss': 0.1888, 'grad_norm': 4.085870265960693, 'learning_rate': 8.505197417404687e-07, 'epoch': 2.61}
 87%|████████▋ | 544/624 [47:25<06:58,  5.24s/it] 87%|████████▋ | 545/624 [47:30<06:52,  5.23s/it]                                                 {'loss': 0.254, 'grad_norm': 3.9332432746887207, 'learning_rate': 8.296868619880372e-07, 'epoch': 2.61}
 87%|████████▋ | 545/624 [47:30<06:52,  5.23s/it] 88%|████████▊ | 546/624 [47:36<06:56,  5.34s/it]                                                 {'loss': 0.2744, 'grad_norm': 3.8183939456939697, 'learning_rate': 8.091012526666797e-07, 'epoch': 2.62}
 88%|████████▊ | 546/624 [47:36<06:56,  5.34s/it] 88%|████████▊ | 547/624 [47:41<06:51,  5.34s/it]                                                 {'loss': 0.3376, 'grad_norm': 4.293550968170166, 'learning_rate': 7.887634688515e-07, 'epoch': 2.62}
 88%|████████▊ | 547/624 [47:41<06:51,  5.34s/it] 88%|████████▊ | 548/624 [47:47<06:49,  5.39s/it]                                                 {'loss': 0.2929, 'grad_norm': 3.6050546169281006, 'learning_rate': 7.686740589351704e-07, 'epoch': 2.63}
 88%|████████▊ | 548/624 [47:47<06:49,  5.39s/it] 88%|████████▊ | 549/624 [47:52<06:43,  5.38s/it]                                                 {'loss': 0.265, 'grad_norm': 3.5802693367004395, 'learning_rate': 7.488335646131628e-07, 'epoch': 2.63}
 88%|████████▊ | 549/624 [47:52<06:43,  5.38s/it] 88%|████████▊ | 550/624 [47:57<06:40,  5.41s/it]                                                 {'loss': 0.1786, 'grad_norm': 3.9629592895507812, 'learning_rate': 7.292425208691212e-07, 'epoch': 2.64}
 88%|████████▊ | 550/624 [47:58<06:40,  5.41s/it] 88%|████████▊ | 551/624 [48:03<06:33,  5.38s/it]                                                 {'loss': 0.2695, 'grad_norm': 4.286652565002441, 'learning_rate': 7.099014559604556e-07, 'epoch': 2.64}
 88%|████████▊ | 551/624 [48:03<06:33,  5.38s/it] 88%|████████▊ | 552/624 [48:08<06:24,  5.34s/it]                                                 {'loss': 0.2573, 'grad_norm': 3.6557605266571045, 'learning_rate': 6.908108914040823e-07, 'epoch': 2.65}
 88%|████████▊ | 552/624 [48:08<06:24,  5.34s/it] 89%|████████▊ | 553/624 [48:13<06:12,  5.25s/it]                                                 {'loss': 0.2703, 'grad_norm': 4.142439365386963, 'learning_rate': 6.71971341962373e-07, 'epoch': 2.65}
 89%|████████▊ | 553/624 [48:13<06:12,  5.25s/it] 89%|████████▉ | 554/624 [48:18<06:05,  5.22s/it]                                                 {'loss': 0.2211, 'grad_norm': 3.585878372192383, 'learning_rate': 6.53383315629268e-07, 'epoch': 2.66}
 89%|████████▉ | 554/624 [48:18<06:05,  5.22s/it] 89%|████████▉ | 555/624 [48:24<06:06,  5.31s/it]                                                 {'loss': 0.2312, 'grad_norm': 3.5340027809143066, 'learning_rate': 6.350473136165836e-07, 'epoch': 2.66}
 89%|████████▉ | 555/624 [48:24<06:06,  5.31s/it] 89%|████████▉ | 556/624 [48:29<05:59,  5.29s/it]                                                 {'loss': 0.1928, 'grad_norm': 4.432037353515625, 'learning_rate': 6.169638303404912e-07, 'epoch': 2.67}
 89%|████████▉ | 556/624 [48:29<05:59,  5.29s/it] 89%|████████▉ | 557/624 [48:34<05:47,  5.19s/it]                                                 {'loss': 0.2184, 'grad_norm': 3.9048688411712646, 'learning_rate': 5.991333534081878e-07, 'epoch': 2.67}
 89%|████████▉ | 557/624 [48:34<05:47,  5.19s/it] 89%|████████▉ | 558/624 [48:39<05:44,  5.23s/it]                                                 {'loss': 0.2936, 'grad_norm': 3.974231481552124, 'learning_rate': 5.815563636047539e-07, 'epoch': 2.68}
 89%|████████▉ | 558/624 [48:39<05:44,  5.23s/it] 90%|████████▉ | 559/624 [48:45<05:42,  5.27s/it]                                                 {'loss': 0.3007, 'grad_norm': 3.5064384937286377, 'learning_rate': 5.64233334880181e-07, 'epoch': 2.68}
 90%|████████▉ | 559/624 [48:45<05:42,  5.27s/it] 90%|████████▉ | 560/624 [48:50<05:42,  5.35s/it]                                                 {'loss': 0.2022, 'grad_norm': 3.5663492679595947, 'learning_rate': 5.471647343365982e-07, 'epoch': 2.69}
 90%|████████▉ | 560/624 [48:50<05:42,  5.35s/it] 90%|████████▉ | 561/624 [48:55<05:38,  5.37s/it]                                                 {'loss': 0.2144, 'grad_norm': 3.527778387069702, 'learning_rate': 5.303510222156716e-07, 'epoch': 2.69}
 90%|████████▉ | 561/624 [48:56<05:38,  5.37s/it] 90%|█████████ | 562/624 [49:01<05:27,  5.28s/it]                                                 {'loss': 0.2376, 'grad_norm': 3.976153612136841, 'learning_rate': 5.137926518862013e-07, 'epoch': 2.7}
 90%|█████████ | 562/624 [49:01<05:27,  5.28s/it] 90%|█████████ | 563/624 [49:06<05:20,  5.25s/it]                                                 {'loss': 0.3216, 'grad_norm': 4.52720832824707, 'learning_rate': 4.974900698318885e-07, 'epoch': 2.7}
 90%|█████████ | 563/624 [49:06<05:20,  5.25s/it] 90%|█████████ | 564/624 [49:11<05:10,  5.17s/it]                                                 {'loss': 0.2597, 'grad_norm': 3.7097573280334473, 'learning_rate': 4.814437156393048e-07, 'epoch': 2.71}
 90%|█████████ | 564/624 [49:11<05:10,  5.17s/it] 91%|█████████ | 565/624 [49:16<05:04,  5.16s/it]                                                 {'loss': 0.2312, 'grad_norm': 3.6234970092773438, 'learning_rate': 4.656540219860317e-07, 'epoch': 2.71}
 91%|█████████ | 565/624 [49:16<05:04,  5.16s/it] 91%|█████████ | 566/624 [49:21<04:58,  5.15s/it]                                                 {'loss': 0.2359, 'grad_norm': 3.5597164630889893, 'learning_rate': 4.501214146289956e-07, 'epoch': 2.71}
 91%|█████████ | 566/624 [49:21<04:58,  5.15s/it] 91%|█████████ | 567/624 [49:26<04:54,  5.16s/it]                                                 {'loss': 0.2475, 'grad_norm': 3.678253650665283, 'learning_rate': 4.3484631239299356e-07, 'epoch': 2.72}
 91%|█████████ | 567/624 [49:26<04:54,  5.16s/it] 91%|█████████ | 568/624 [49:31<04:48,  5.16s/it]                                                 {'loss': 0.2605, 'grad_norm': 4.30472469329834, 'learning_rate': 4.198291271593924e-07, 'epoch': 2.72}
 91%|█████████ | 568/624 [49:31<04:48,  5.16s/it] 91%|█████████ | 569/624 [49:36<04:38,  5.06s/it]                                                 {'loss': 0.2696, 'grad_norm': 4.357523441314697, 'learning_rate': 4.0507026385502747e-07, 'epoch': 2.73}
 91%|█████████ | 569/624 [49:36<04:38,  5.06s/it] 91%|█████████▏| 570/624 [49:41<04:36,  5.11s/it]                                                 {'loss': 0.2265, 'grad_norm': 3.6248230934143066, 'learning_rate': 3.9057012044127817e-07, 'epoch': 2.73}
 91%|█████████▏| 570/624 [49:42<04:36,  5.11s/it] 92%|█████████▏| 571/624 [49:47<04:35,  5.20s/it]                                                 {'loss': 0.209, 'grad_norm': 4.045997142791748, 'learning_rate': 3.7632908790334656e-07, 'epoch': 2.74}
 92%|█████████▏| 571/624 [49:47<04:35,  5.20s/it] 92%|█████████▏| 572/624 [49:52<04:36,  5.31s/it]                                                 {'loss': 0.1808, 'grad_norm': 3.0692758560180664, 'learning_rate': 3.6234755023970447e-07, 'epoch': 2.74}
 92%|█████████▏| 572/624 [49:52<04:36,  5.31s/it] 92%|█████████▏| 573/624 [49:58<04:31,  5.33s/it]                                                 {'loss': 0.2528, 'grad_norm': 3.626429557800293, 'learning_rate': 3.4862588445174985e-07, 'epoch': 2.75}
 92%|█████████▏| 573/624 [49:58<04:31,  5.33s/it] 92%|█████████▏| 574/624 [50:03<04:20,  5.20s/it]                                                 {'loss': 0.2444, 'grad_norm': 4.115317344665527, 'learning_rate': 3.3516446053363015e-07, 'epoch': 2.75}
 92%|█████████▏| 574/624 [50:03<04:20,  5.20s/it] 92%|█████████▏| 575/624 [50:08<04:15,  5.21s/it]                                                 {'loss': 0.2296, 'grad_norm': 3.135648250579834, 'learning_rate': 3.219636414622751e-07, 'epoch': 2.76}
 92%|█████████▏| 575/624 [50:08<04:15,  5.21s/it] 92%|█████████▏| 576/624 [50:13<04:11,  5.25s/it]                                                 {'loss': 0.2363, 'grad_norm': 3.3695993423461914, 'learning_rate': 3.090237831876053e-07, 'epoch': 2.76}
 92%|█████████▏| 576/624 [50:13<04:11,  5.25s/it] 92%|█████████▏| 577/624 [50:18<04:05,  5.22s/it]                                                 {'loss': 0.272, 'grad_norm': 3.866271495819092, 'learning_rate': 2.9634523462293005e-07, 'epoch': 2.77}
 92%|█████████▏| 577/624 [50:18<04:05,  5.22s/it] 93%|█████████▎| 578/624 [50:23<03:56,  5.13s/it]                                                 {'loss': 0.197, 'grad_norm': 3.5596909523010254, 'learning_rate': 2.839283376355506e-07, 'epoch': 2.77}
 93%|█████████▎| 578/624 [50:23<03:56,  5.13s/it] 93%|█████████▎| 579/624 [50:28<03:51,  5.15s/it]                                                 {'loss': 0.1822, 'grad_norm': 3.446732997894287, 'learning_rate': 2.717734270375272e-07, 'epoch': 2.78}
 93%|█████████▎| 579/624 [50:29<03:51,  5.15s/it] 93%|█████████▎| 580/624 [50:34<03:47,  5.18s/it]                                                 {'loss': 0.1701, 'grad_norm': 3.5638182163238525, 'learning_rate': 2.5988083057666534e-07, 'epoch': 2.78}
 93%|█████████▎| 580/624 [50:34<03:47,  5.18s/it] 93%|█████████▎| 581/624 [50:39<03:42,  5.17s/it]                                                 {'loss': 0.2788, 'grad_norm': 3.9239611625671387, 'learning_rate': 2.4825086892766745e-07, 'epoch': 2.79}
 93%|█████████▎| 581/624 [50:39<03:42,  5.17s/it] 93%|█████████▎| 582/624 [50:44<03:38,  5.20s/it]                                                 {'loss': 0.2354, 'grad_norm': 3.9083826541900635, 'learning_rate': 2.3688385568349515e-07, 'epoch': 2.79}
 93%|█████████▎| 582/624 [50:44<03:38,  5.20s/it] 93%|█████████▎| 583/624 [50:50<03:37,  5.30s/it]                                                 {'loss': 0.2362, 'grad_norm': 3.9395227432250977, 'learning_rate': 2.2578009734690264e-07, 'epoch': 2.8}
 93%|█████████▎| 583/624 [50:50<03:37,  5.30s/it] 94%|█████████▎| 584/624 [50:55<03:32,  5.31s/it]                                                 {'loss': 0.1778, 'grad_norm': 4.323983669281006, 'learning_rate': 2.1493989332218468e-07, 'epoch': 2.8}
 94%|█████████▎| 584/624 [50:55<03:32,  5.31s/it] 94%|█████████▍| 585/624 [51:00<03:27,  5.31s/it]                                                 {'loss': 0.2218, 'grad_norm': 6.79594087600708, 'learning_rate': 2.043635359070928e-07, 'epoch': 2.81}
 94%|█████████▍| 585/624 [51:00<03:27,  5.31s/it] 94%|█████████▍| 586/624 [51:06<03:24,  5.38s/it]                                                 {'loss': 0.2523, 'grad_norm': 3.6855525970458984, 'learning_rate': 1.9405131028495838e-07, 'epoch': 2.81}
 94%|█████████▍| 586/624 [51:06<03:24,  5.38s/it] 94%|█████████▍| 587/624 [51:11<03:17,  5.33s/it]                                                 {'loss': 0.2539, 'grad_norm': 3.681825876235962, 'learning_rate': 1.8400349451700438e-07, 'epoch': 2.82}
 94%|█████████▍| 587/624 [51:11<03:17,  5.33s/it] 94%|█████████▍| 588/624 [51:16<03:11,  5.32s/it]                                                 {'loss': 0.2766, 'grad_norm': 3.5623927116394043, 'learning_rate': 1.742203595348435e-07, 'epoch': 2.82}
 94%|█████████▍| 588/624 [51:17<03:11,  5.32s/it] 94%|█████████▍| 589/624 [51:22<03:05,  5.31s/it]                                                 {'loss': 0.2338, 'grad_norm': 3.781377077102661, 'learning_rate': 1.6470216913317628e-07, 'epoch': 2.82}
 94%|█████████▍| 589/624 [51:22<03:05,  5.31s/it] 95%|█████████▍| 590/624 [51:26<02:55,  5.15s/it]                                                 {'loss': 0.2569, 'grad_norm': 3.9854509830474854, 'learning_rate': 1.5544917996267562e-07, 'epoch': 2.83}
 95%|█████████▍| 590/624 [51:27<02:55,  5.15s/it] 95%|█████████▍| 591/624 [51:32<02:55,  5.32s/it]                                                 {'loss': 0.2734, 'grad_norm': 3.649864673614502, 'learning_rate': 1.464616415230702e-07, 'epoch': 2.83}
 95%|█████████▍| 591/624 [51:32<02:55,  5.32s/it] 95%|█████████▍| 592/624 [51:37<02:46,  5.21s/it]                                                 {'loss': 0.1829, 'grad_norm': 4.154232025146484, 'learning_rate': 1.3773979615640976e-07, 'epoch': 2.84}
 95%|█████████▍| 592/624 [51:37<02:46,  5.21s/it] 95%|█████████▌| 593/624 [51:42<02:40,  5.17s/it]                                                 {'loss': 0.1781, 'grad_norm': 4.007417678833008, 'learning_rate': 1.292838790405393e-07, 'epoch': 2.84}
 95%|█████████▌| 593/624 [51:42<02:40,  5.17s/it] 95%|█████████▌| 594/624 [51:47<02:33,  5.11s/it]                                                 {'loss': 0.2896, 'grad_norm': 5.079823970794678, 'learning_rate': 1.2109411818274851e-07, 'epoch': 2.85}
 95%|█████████▌| 594/624 [51:47<02:33,  5.11s/it] 95%|█████████▌| 595/624 [51:53<02:31,  5.22s/it]                                                 {'loss': 0.2972, 'grad_norm': 3.7699801921844482, 'learning_rate': 1.1317073441363458e-07, 'epoch': 2.85}
 95%|█████████▌| 595/624 [51:53<02:31,  5.22s/it] 96%|█████████▌| 596/624 [51:58<02:25,  5.18s/it]                                                 {'loss': 0.2593, 'grad_norm': 4.726772308349609, 'learning_rate': 1.055139413811379e-07, 'epoch': 2.86}
 96%|█████████▌| 596/624 [51:58<02:25,  5.18s/it] 96%|█████████▌| 597/624 [52:03<02:22,  5.29s/it]                                                 {'loss': 0.1846, 'grad_norm': 4.0091938972473145, 'learning_rate': 9.812394554478355e-08, 'epoch': 2.86}
 96%|█████████▌| 597/624 [52:03<02:22,  5.29s/it] 96%|█████████▌| 598/624 [52:08<02:15,  5.22s/it]                                                 {'loss': 0.1841, 'grad_norm': 3.766064167022705, 'learning_rate': 9.10009461701189e-08, 'epoch': 2.87}
 96%|█████████▌| 598/624 [52:08<02:15,  5.22s/it] 96%|█████████▌| 599/624 [52:14<02:10,  5.23s/it]                                                 {'loss': 0.2222, 'grad_norm': 3.824420928955078, 'learning_rate': 8.41451353233369e-08, 'epoch': 2.87}
 96%|█████████▌| 599/624 [52:14<02:10,  5.23s/it] 96%|█████████▌| 600/624 [52:19<02:05,  5.21s/it]                                                 {'loss': 0.2191, 'grad_norm': 4.015270233154297, 'learning_rate': 7.755669786609688e-08, 'epoch': 2.88}
 96%|█████████▌| 600/624 [52:19<02:05,  5.21s/it] 96%|█████████▋| 601/624 [52:24<02:01,  5.26s/it]                                                 {'loss': 0.228, 'grad_norm': 3.995633125305176, 'learning_rate': 7.123581145053849e-08, 'epoch': 2.88}
 96%|█████████▋| 601/624 [52:24<02:01,  5.26s/it] 96%|█████████▋| 602/624 [52:29<01:55,  5.25s/it]                                                 {'loss': 0.3065, 'grad_norm': 4.308324337005615, 'learning_rate': 6.51826465144978e-08, 'epoch': 2.89}
 96%|█████████▋| 602/624 [52:29<01:55,  5.25s/it] 97%|█████████▋| 603/624 [52:35<01:51,  5.33s/it]                                                 {'loss': 0.2751, 'grad_norm': 3.4027748107910156, 'learning_rate': 5.93973662769054e-08, 'epoch': 2.89}
 97%|█████████▋| 603/624 [52:35<01:51,  5.33s/it] 97%|█████████▋| 604/624 [52:40<01:46,  5.32s/it]                                                 {'loss': 0.3122, 'grad_norm': 3.685825824737549, 'learning_rate': 5.388012673338661e-08, 'epoch': 2.9}
 97%|█████████▋| 604/624 [52:40<01:46,  5.32s/it] 97%|█████████▋| 605/624 [52:46<01:41,  5.35s/it]                                                 {'loss': 0.2185, 'grad_norm': 4.064706802368164, 'learning_rate': 4.863107665205702e-08, 'epoch': 2.9}
 97%|█████████▋| 605/624 [52:46<01:41,  5.35s/it] 97%|█████████▋| 606/624 [52:51<01:35,  5.33s/it]                                                 {'loss': 0.2607, 'grad_norm': 4.096783638000488, 'learning_rate': 4.365035756950797e-08, 'epoch': 2.91}
 97%|█████████▋| 606/624 [52:51<01:35,  5.33s/it] 97%|█████████▋| 607/624 [52:56<01:29,  5.29s/it]                                                 {'loss': 0.2551, 'grad_norm': 4.084891319274902, 'learning_rate': 3.8938103786995144e-08, 'epoch': 2.91}
 97%|█████████▋| 607/624 [52:56<01:29,  5.29s/it] 97%|█████████▋| 608/624 [53:01<01:23,  5.25s/it]                                                 {'loss': 0.2127, 'grad_norm': 3.7422094345092773, 'learning_rate': 3.449444236681254e-08, 'epoch': 2.92}
 97%|█████████▋| 608/624 [53:01<01:23,  5.25s/it] 98%|█████████▊| 609/624 [53:06<01:16,  5.12s/it]                                                 {'loss': 0.2856, 'grad_norm': 4.384604454040527, 'learning_rate': 3.03194931288664e-08, 'epoch': 2.92}
 98%|█████████▊| 609/624 [53:06<01:16,  5.12s/it] 98%|█████████▊| 610/624 [53:11<01:12,  5.18s/it]                                                 {'loss': 0.3848, 'grad_norm': 3.7960398197174072, 'learning_rate': 2.641336864744992e-08, 'epoch': 2.93}
 98%|█████████▊| 610/624 [53:11<01:12,  5.18s/it] 98%|█████████▊| 611/624 [53:16<01:06,  5.12s/it]                                                 {'loss': 0.275, 'grad_norm': 3.9831974506378174, 'learning_rate': 2.2776174248199114e-08, 'epoch': 2.93}
 98%|█████████▊| 611/624 [53:16<01:06,  5.12s/it] 98%|█████████▊| 612/624 [53:22<01:02,  5.18s/it]                                                 {'loss': 0.1968, 'grad_norm': 3.343097686767578, 'learning_rate': 1.9408008005260548e-08, 'epoch': 2.94}
 98%|█████████▊| 612/624 [53:22<01:02,  5.18s/it] 98%|█████████▊| 613/624 [53:27<00:56,  5.17s/it]                                                 {'loss': 0.3195, 'grad_norm': 3.631580352783203, 'learning_rate': 1.630896073864352e-08, 'epoch': 2.94}
 98%|█████████▊| 613/624 [53:27<00:56,  5.17s/it] 98%|█████████▊| 614/624 [53:32<00:52,  5.28s/it]                                                 {'loss': 0.352, 'grad_norm': 4.559237003326416, 'learning_rate': 1.3479116011769766e-08, 'epoch': 2.94}
 98%|█████████▊| 614/624 [53:32<00:52,  5.28s/it] 99%|█████████▊| 615/624 [53:37<00:47,  5.23s/it]                                                 {'loss': 0.2573, 'grad_norm': 3.301088333129883, 'learning_rate': 1.0918550129223049e-08, 'epoch': 2.95}
 99%|█████████▊| 615/624 [53:38<00:47,  5.23s/it] 99%|█████████▊| 616/624 [53:43<00:42,  5.37s/it]                                                 {'loss': 0.2704, 'grad_norm': 3.874393939971924, 'learning_rate': 8.627332134690802e-09, 'epoch': 2.95}
 99%|█████████▊| 616/624 [53:43<00:42,  5.37s/it] 99%|█████████▉| 617/624 [53:48<00:37,  5.31s/it]                                                 {'loss': 0.2202, 'grad_norm': 3.7907965183258057, 'learning_rate': 6.605523809102288e-09, 'epoch': 2.96}
 99%|█████████▉| 617/624 [53:48<00:37,  5.31s/it] 99%|█████████▉| 618/624 [53:54<00:31,  5.30s/it]                                                 {'loss': 0.2617, 'grad_norm': 4.344546318054199, 'learning_rate': 4.853179668959928e-09, 'epoch': 2.96}
 99%|█████████▉| 618/624 [53:54<00:31,  5.30s/it] 99%|█████████▉| 619/624 [53:59<00:26,  5.32s/it]                                                 {'loss': 0.2574, 'grad_norm': 3.9767582416534424, 'learning_rate': 3.3703469648760367e-09, 'epoch': 2.97}
 99%|█████████▉| 619/624 [53:59<00:26,  5.32s/it] 99%|█████████▉| 620/624 [54:04<00:21,  5.34s/it]                                                 {'loss': 0.2284, 'grad_norm': 4.093214988708496, 'learning_rate': 2.1570656802905042e-09, 'epoch': 2.97}
 99%|█████████▉| 620/624 [54:04<00:21,  5.34s/it]100%|█████████▉| 621/624 [54:09<00:15,  5.27s/it]                                                 {'loss': 0.1691, 'grad_norm': 3.3703558444976807, 'learning_rate': 1.213368530399439e-09, 'epoch': 2.98}
100%|█████████▉| 621/624 [54:10<00:15,  5.27s/it]100%|█████████▉| 622/624 [54:15<00:10,  5.24s/it]                                                 {'loss': 0.2852, 'grad_norm': 4.079092025756836, 'learning_rate': 5.392809612703165e-10, 'epoch': 2.98}
100%|█████████▉| 622/624 [54:15<00:10,  5.24s/it]100%|█████████▉| 623/624 [54:20<00:05,  5.18s/it]                                                 {'loss': 0.2226, 'grad_norm': 3.9115443229675293, 'learning_rate': 1.3482114915475132e-10, 'epoch': 2.99}
100%|█████████▉| 623/624 [54:20<00:05,  5.18s/it]100%|██████████| 624/624 [54:25<00:00,  5.30s/it]                                                 {'loss': 0.2125, 'grad_norm': 4.959707260131836, 'learning_rate': 0.0, 'epoch': 2.99}
100%|██████████| 624/624 [54:25<00:00,  5.30s/it]/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3012, in _save_checkpoint
[rank0]:     self._save_optimizer_and_scheduler(output_dir)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3167, in _save_optimizer_and_scheduler
[rank0]:     torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank0]:     return container(name_or_buffer)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank0]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank0]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_story/checkpoint-624/scheduler.pt cannot be opened.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3012, in _save_checkpoint
[rank0]:     self._save_optimizer_and_scheduler(output_dir)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3167, in _save_optimizer_and_scheduler
[rank0]:     torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank0]:     return container(name_or_buffer)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank0]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank0]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_story/checkpoint-624/scheduler.pt cannot be opened.
[1;34mwandb[0m: 🚀 View run [33m/nlp/scr/qinanyu/model_cache/result_story[0m at: [34mhttps://wandb.ai/limber/huggingface/runs/dg775ki2[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../../sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_133520-dg775ki2/logs[0m
W1010 14:49:52.636000 140714473116032 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 506740 closing signal SIGTERM
W1010 14:49:52.637000 140714473116032 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 506741 closing signal SIGTERM
E1010 14:49:53.262000 140714473116032 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 506739) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-10_14:49:52
  host      : sphinx6.stanford.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 506739)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
SLURM_JOBID=8747403
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1010 15:24:46.545000 140001791881600 torch/distributed/run.py:779] 
W1010 15:24:46.545000 140001791881600 torch/distributed/run.py:779] *****************************************
W1010 15:24:46.545000 140001791881600 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1010 15:24:46.545000 140001791881600 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:59, 19.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:00, 20.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:00, 20.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:42, 21.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:42, 21.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:43, 21.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:02<00:20, 20.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:02<00:20, 20.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:02<00:20, 20.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:07<00:00, 14.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:07<00:00, 16.94s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:07<00:00, 14.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:07<00:00, 16.94s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:07<00:00, 14.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:07<00:00, 16.99s/it]
loaded
loaded
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
load trainer
load trainer
load trainer
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_152707-2o539gkm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /nlp/scr/qinanyu/model_cache/result_dolly
wandb: ⭐️ View project at https://wandb.ai/limber/huggingface
wandb: 🚀 View run at https://wandb.ai/limber/huggingface/runs/2o539gkm
  0%|          | 0/624 [00:00<?, ?it/s]  0%|          | 1/624 [00:06<1:05:20,  6.29s/it]                                                 {'loss': 1.5148, 'grad_norm': 12.495073318481445, 'learning_rate': 1.0526315789473685e-06, 'epoch': 0.0}
  0%|          | 1/624 [00:06<1:05:20,  6.29s/it]  0%|          | 2/624 [00:11<58:26,  5.64s/it]                                                 {'loss': 1.7605, 'grad_norm': 10.800081253051758, 'learning_rate': 2.105263157894737e-06, 'epoch': 0.01}
  0%|          | 2/624 [00:11<58:26,  5.64s/it]  0%|          | 3/624 [00:16<56:16,  5.44s/it]                                               {'loss': 1.593, 'grad_norm': 25.1744327545166, 'learning_rate': 3.157894736842105e-06, 'epoch': 0.01}
  0%|          | 3/624 [00:16<56:16,  5.44s/it]  1%|          | 4/624 [00:22<55:49,  5.40s/it]                                               {'loss': 1.5664, 'grad_norm': 8.40312385559082, 'learning_rate': 4.210526315789474e-06, 'epoch': 0.02}
  1%|          | 4/624 [00:22<55:49,  5.40s/it]  1%|          | 5/624 [00:27<55:32,  5.38s/it]                                               {'loss': 1.5968, 'grad_norm': 9.327375411987305, 'learning_rate': 5.263157894736842e-06, 'epoch': 0.02}
  1%|          | 5/624 [00:27<55:32,  5.38s/it]  1%|          | 6/624 [00:32<54:40,  5.31s/it]                                               {'loss': 1.4737, 'grad_norm': 9.80016040802002, 'learning_rate': 6.31578947368421e-06, 'epoch': 0.03}
  1%|          | 6/624 [00:32<54:40,  5.31s/it]  1%|          | 7/624 [00:37<54:04,  5.26s/it]                                               {'loss': 1.3491, 'grad_norm': 5.733725070953369, 'learning_rate': 7.368421052631579e-06, 'epoch': 0.03}
  1%|          | 7/624 [00:37<54:04,  5.26s/it]  1%|▏         | 8/624 [00:42<53:27,  5.21s/it]                                               {'loss': 1.475, 'grad_norm': 5.300696849822998, 'learning_rate': 8.421052631578948e-06, 'epoch': 0.04}
  1%|▏         | 8/624 [00:42<53:27,  5.21s/it]  1%|▏         | 9/624 [00:48<54:21,  5.30s/it]                                               {'loss': 1.4756, 'grad_norm': 5.30349588394165, 'learning_rate': 9.473684210526315e-06, 'epoch': 0.04}
  1%|▏         | 9/624 [00:48<54:21,  5.30s/it]  2%|▏         | 10/624 [00:53<54:10,  5.29s/it]                                                {'loss': 1.4619, 'grad_norm': 5.120640277862549, 'learning_rate': 1.0526315789473684e-05, 'epoch': 0.05}
  2%|▏         | 10/624 [00:53<54:10,  5.29s/it]  2%|▏         | 11/624 [00:59<54:31,  5.34s/it]                                                {'loss': 1.6118, 'grad_norm': 5.695831775665283, 'learning_rate': 1.1578947368421053e-05, 'epoch': 0.05}
  2%|▏         | 11/624 [00:59<54:31,  5.34s/it]  2%|▏         | 12/624 [01:03<53:18,  5.23s/it]                                                {'loss': 1.3981, 'grad_norm': 5.787429332733154, 'learning_rate': 1.263157894736842e-05, 'epoch': 0.06}
  2%|▏         | 12/624 [01:04<53:18,  5.23s/it]  2%|▏         | 13/624 [01:09<53:35,  5.26s/it]                                                {'loss': 1.539, 'grad_norm': 5.780490875244141, 'learning_rate': 1.3684210526315791e-05, 'epoch': 0.06}
  2%|▏         | 13/624 [01:09<53:35,  5.26s/it]  2%|▏         | 14/624 [01:14<53:01,  5.21s/it]                                                {'loss': 1.5908, 'grad_norm': 7.201350688934326, 'learning_rate': 1.4736842105263159e-05, 'epoch': 0.07}
  2%|▏         | 14/624 [01:14<53:01,  5.21s/it]  2%|▏         | 15/624 [01:19<53:19,  5.25s/it]                                                {'loss': 1.514, 'grad_norm': 6.013665199279785, 'learning_rate': 1.578947368421053e-05, 'epoch': 0.07}
  2%|▏         | 15/624 [01:19<53:19,  5.25s/it]  3%|▎         | 16/624 [01:25<53:18,  5.26s/it]                                                {'loss': 1.3985, 'grad_norm': 5.882689952850342, 'learning_rate': 1.6842105263157896e-05, 'epoch': 0.08}
  3%|▎         | 16/624 [01:25<53:18,  5.26s/it]  3%|▎         | 17/624 [01:30<53:01,  5.24s/it]                                                {'loss': 1.4608, 'grad_norm': 5.199213027954102, 'learning_rate': 1.7894736842105264e-05, 'epoch': 0.08}
  3%|▎         | 17/624 [01:30<53:01,  5.24s/it]  3%|▎         | 18/624 [01:35<52:51,  5.23s/it]                                                {'loss': 1.5255, 'grad_norm': 5.885075569152832, 'learning_rate': 1.894736842105263e-05, 'epoch': 0.09}
  3%|▎         | 18/624 [01:35<52:51,  5.23s/it]  3%|▎         | 19/624 [01:40<52:14,  5.18s/it]                                                {'loss': 1.529, 'grad_norm': 6.171816349029541, 'learning_rate': 2e-05, 'epoch': 0.09}
  3%|▎         | 19/624 [01:40<52:14,  5.18s/it]  3%|▎         | 20/624 [01:45<52:30,  5.22s/it]                                                {'loss': 1.3851, 'grad_norm': 5.334774017333984, 'learning_rate': 1.9999865178850847e-05, 'epoch': 0.1}
  3%|▎         | 20/624 [01:45<52:30,  5.22s/it]  3%|▎         | 21/624 [01:51<52:33,  5.23s/it]                                                {'loss': 1.3755, 'grad_norm': 9.0061674118042, 'learning_rate': 1.999946071903873e-05, 'epoch': 0.1}
  3%|▎         | 21/624 [01:51<52:33,  5.23s/it]  4%|▎         | 22/624 [01:56<52:43,  5.25s/it]                                                {'loss': 1.458, 'grad_norm': 5.713691711425781, 'learning_rate': 1.9998786631469602e-05, 'epoch': 0.11}
  4%|▎         | 22/624 [01:56<52:43,  5.25s/it]  4%|▎         | 23/624 [02:01<52:10,  5.21s/it]                                                {'loss': 1.5141, 'grad_norm': 5.216008186340332, 'learning_rate': 1.999784293431971e-05, 'epoch': 0.11}
  4%|▎         | 23/624 [02:01<52:10,  5.21s/it]  4%|▍         | 24/624 [02:06<52:35,  5.26s/it]                                                {'loss': 1.5017, 'grad_norm': 4.727444171905518, 'learning_rate': 1.9996629653035128e-05, 'epoch': 0.12}
  4%|▍         | 24/624 [02:07<52:35,  5.26s/it]  4%|▍         | 25/624 [02:11<51:59,  5.21s/it]                                                {'loss': 1.5332, 'grad_norm': 5.0234880447387695, 'learning_rate': 1.999514682033104e-05, 'epoch': 0.12}
  4%|▍         | 25/624 [02:12<51:59,  5.21s/it]  4%|▍         | 26/624 [02:17<52:35,  5.28s/it]                                                {'loss': 1.5522, 'grad_norm': 6.392561912536621, 'learning_rate': 1.99933944761909e-05, 'epoch': 0.12}
  4%|▍         | 26/624 [02:17<52:35,  5.28s/it]  4%|▍         | 27/624 [02:22<52:09,  5.24s/it]                                                {'loss': 1.6573, 'grad_norm': 5.161737442016602, 'learning_rate': 1.999137266786531e-05, 'epoch': 0.13}
  4%|▍         | 27/624 [02:22<52:09,  5.24s/it]  4%|▍         | 28/624 [02:27<52:06,  5.25s/it]                                                {'loss': 1.6042, 'grad_norm': 4.227275371551514, 'learning_rate': 1.998908144987078e-05, 'epoch': 0.13}
  4%|▍         | 28/624 [02:27<52:06,  5.25s/it]  5%|▍         | 29/624 [02:32<50:46,  5.12s/it]                                                {'loss': 1.3302, 'grad_norm': 4.214003086090088, 'learning_rate': 1.9986520883988233e-05, 'epoch': 0.14}
  5%|▍         | 29/624 [02:32<50:46,  5.12s/it]  5%|▍         | 30/624 [02:37<51:11,  5.17s/it]                                                {'loss': 1.6112, 'grad_norm': 6.799961090087891, 'learning_rate': 1.9983691039261358e-05, 'epoch': 0.14}
  5%|▍         | 30/624 [02:38<51:11,  5.17s/it]  5%|▍         | 31/624 [02:43<51:59,  5.26s/it]                                                {'loss': 1.4545, 'grad_norm': 4.46565580368042, 'learning_rate': 1.998059199199474e-05, 'epoch': 0.15}
  5%|▍         | 31/624 [02:43<51:59,  5.26s/it]  5%|▌         | 32/624 [02:48<51:10,  5.19s/it]                                                {'loss': 1.6642, 'grad_norm': 5.6018877029418945, 'learning_rate': 1.9977223825751802e-05, 'epoch': 0.15}
  5%|▌         | 32/624 [02:48<51:10,  5.19s/it]  5%|▌         | 33/624 [02:53<52:02,  5.28s/it]                                                {'loss': 1.421, 'grad_norm': 5.961602210998535, 'learning_rate': 1.997358663135255e-05, 'epoch': 0.16}
  5%|▌         | 33/624 [02:54<52:02,  5.28s/it]  5%|▌         | 34/624 [02:59<52:18,  5.32s/it]                                                {'loss': 1.7102, 'grad_norm': 6.345980644226074, 'learning_rate': 1.9969680506871138e-05, 'epoch': 0.16}
  5%|▌         | 34/624 [02:59<52:18,  5.32s/it]  6%|▌         | 35/624 [03:04<52:02,  5.30s/it]                                                {'loss': 1.4501, 'grad_norm': 6.551489353179932, 'learning_rate': 1.9965505557633188e-05, 'epoch': 0.17}
  6%|▌         | 35/624 [03:04<52:02,  5.30s/it]  6%|▌         | 36/624 [03:09<51:45,  5.28s/it]                                                {'loss': 1.7015, 'grad_norm': 4.9708638191223145, 'learning_rate': 1.9961061896213006e-05, 'epoch': 0.17}
  6%|▌         | 36/624 [03:09<51:45,  5.28s/it]  6%|▌         | 37/624 [03:14<50:08,  5.12s/it]                                                {'loss': 1.7599, 'grad_norm': 5.113049030303955, 'learning_rate': 1.9956349642430494e-05, 'epoch': 0.18}
  6%|▌         | 37/624 [03:14<50:08,  5.12s/it]  6%|▌         | 38/624 [03:19<50:39,  5.19s/it]                                                {'loss': 1.6544, 'grad_norm': 4.912349700927734, 'learning_rate': 1.9951368923347945e-05, 'epoch': 0.18}
  6%|▌         | 38/624 [03:20<50:39,  5.19s/it]  6%|▋         | 39/624 [03:25<51:12,  5.25s/it]                                                {'loss': 1.5983, 'grad_norm': 5.540974140167236, 'learning_rate': 1.9946119873266615e-05, 'epoch': 0.19}
  6%|▋         | 39/624 [03:25<51:12,  5.25s/it]  6%|▋         | 40/624 [03:30<51:07,  5.25s/it]                                                {'loss': 1.5397, 'grad_norm': 4.597101211547852, 'learning_rate': 1.9940602633723097e-05, 'epoch': 0.19}
  6%|▋         | 40/624 [03:30<51:07,  5.25s/it]  7%|▋         | 41/624 [03:35<50:27,  5.19s/it]                                                {'loss': 1.7606, 'grad_norm': 6.671167850494385, 'learning_rate': 1.99348173534855e-05, 'epoch': 0.2}
  7%|▋         | 41/624 [03:35<50:27,  5.19s/it]  7%|▋         | 42/624 [03:40<49:13,  5.07s/it]                                                {'loss': 1.5493, 'grad_norm': 5.8237504959106445, 'learning_rate': 1.9928764188549462e-05, 'epoch': 0.2}
  7%|▋         | 42/624 [03:40<49:13,  5.07s/it]  7%|▋         | 43/624 [03:45<49:13,  5.08s/it]                                                {'loss': 1.5293, 'grad_norm': 6.534775733947754, 'learning_rate': 1.9922443302133906e-05, 'epoch': 0.21}
  7%|▋         | 43/624 [03:45<49:13,  5.08s/it]  7%|▋         | 44/624 [03:50<49:33,  5.13s/it]                                                {'loss': 1.6071, 'grad_norm': 4.654382228851318, 'learning_rate': 1.9915854864676665e-05, 'epoch': 0.21}
  7%|▋         | 44/624 [03:50<49:33,  5.13s/it]  7%|▋         | 45/624 [03:56<49:49,  5.16s/it]                                                {'loss': 1.4279, 'grad_norm': 6.569234848022461, 'learning_rate': 1.990899905382988e-05, 'epoch': 0.22}
  7%|▋         | 45/624 [03:56<49:49,  5.16s/it]  7%|▋         | 46/624 [04:01<49:36,  5.15s/it]                                                {'loss': 1.6049, 'grad_norm': 4.902775764465332, 'learning_rate': 1.9901876054455217e-05, 'epoch': 0.22}
  7%|▋         | 46/624 [04:01<49:36,  5.15s/it]  8%|▊         | 47/624 [04:06<49:49,  5.18s/it]                                                {'loss': 1.5719, 'grad_norm': 4.819836139678955, 'learning_rate': 1.9894486058618863e-05, 'epoch': 0.23}
  8%|▊         | 47/624 [04:06<49:49,  5.18s/it]  8%|▊         | 48/624 [04:11<49:50,  5.19s/it]                                                {'loss': 1.2668, 'grad_norm': 7.229400634765625, 'learning_rate': 1.9886829265586368e-05, 'epoch': 0.23}
  8%|▊         | 48/624 [04:11<49:50,  5.19s/it]  8%|▊         | 49/624 [04:16<49:44,  5.19s/it]                                                {'loss': 1.482, 'grad_norm': 4.943150997161865, 'learning_rate': 1.9878905881817254e-05, 'epoch': 0.24}
  8%|▊         | 49/624 [04:16<49:44,  5.19s/it]  8%|▊         | 50/624 [04:22<49:48,  5.21s/it]                                                {'loss': 1.736, 'grad_norm': 4.418599605560303, 'learning_rate': 1.9870716120959462e-05, 'epoch': 0.24}
  8%|▊         | 50/624 [04:22<49:48,  5.21s/it]  8%|▊         | 51/624 [04:27<49:31,  5.19s/it]                                                {'loss': 1.4797, 'grad_norm': 4.472428321838379, 'learning_rate': 1.986226020384359e-05, 'epoch': 0.24}
  8%|▊         | 51/624 [04:27<49:31,  5.19s/it]  8%|▊         | 52/624 [04:32<49:50,  5.23s/it]                                                {'loss': 1.8128, 'grad_norm': 6.605550289154053, 'learning_rate': 1.9853538358476933e-05, 'epoch': 0.25}
  8%|▊         | 52/624 [04:32<49:50,  5.23s/it]  8%|▊         | 53/624 [04:38<50:34,  5.31s/it]                                                {'loss': 1.4878, 'grad_norm': 5.259258270263672, 'learning_rate': 1.9844550820037326e-05, 'epoch': 0.25}
  8%|▊         | 53/624 [04:38<50:34,  5.31s/it]  9%|▊         | 54/624 [04:43<50:30,  5.32s/it]                                                {'loss': 1.5337, 'grad_norm': 5.312291145324707, 'learning_rate': 1.9835297830866827e-05, 'epoch': 0.26}
  9%|▊         | 54/624 [04:43<50:30,  5.32s/it]  9%|▉         | 55/624 [04:48<50:57,  5.37s/it]                                                {'loss': 1.5905, 'grad_norm': 4.844367504119873, 'learning_rate': 1.9825779640465157e-05, 'epoch': 0.26}
  9%|▉         | 55/624 [04:48<50:57,  5.37s/it]  9%|▉         | 56/624 [04:54<50:52,  5.37s/it]                                                {'loss': 1.4648, 'grad_norm': 4.515657424926758, 'learning_rate': 1.9815996505483e-05, 'epoch': 0.27}
  9%|▉         | 56/624 [04:54<50:52,  5.37s/it]  9%|▉         | 57/624 [04:59<49:37,  5.25s/it]                                                {'loss': 1.7515, 'grad_norm': 6.083053112030029, 'learning_rate': 1.9805948689715043e-05, 'epoch': 0.27}
  9%|▉         | 57/624 [04:59<49:37,  5.25s/it]  9%|▉         | 58/624 [05:04<49:45,  5.27s/it]                                                {'loss': 1.3161, 'grad_norm': 4.4880523681640625, 'learning_rate': 1.979563646409291e-05, 'epoch': 0.28}
  9%|▉         | 58/624 [05:04<49:45,  5.27s/it]  9%|▉         | 59/624 [05:09<49:30,  5.26s/it]                                                {'loss': 1.4559, 'grad_norm': 6.222674369812012, 'learning_rate': 1.9785060106677818e-05, 'epoch': 0.28}
  9%|▉         | 59/624 [05:09<49:30,  5.26s/it] 10%|▉         | 60/624 [05:14<49:25,  5.26s/it]                                                {'loss': 1.4541, 'grad_norm': 4.832564830780029, 'learning_rate': 1.97742199026531e-05, 'epoch': 0.29}
 10%|▉         | 60/624 [05:15<49:25,  5.26s/it] 10%|▉         | 61/624 [05:20<49:43,  5.30s/it]                                                {'loss': 1.5429, 'grad_norm': 5.5308146476745605, 'learning_rate': 1.9763116144316506e-05, 'epoch': 0.29}
 10%|▉         | 61/624 [05:20<49:43,  5.30s/it] 10%|▉         | 62/624 [05:25<50:10,  5.36s/it]                                                {'loss': 1.2238, 'grad_norm': 5.201312065124512, 'learning_rate': 1.9751749131072335e-05, 'epoch': 0.3}
 10%|▉         | 62/624 [05:26<50:10,  5.36s/it] 10%|█         | 63/624 [05:30<49:14,  5.27s/it]                                                {'loss': 1.4938, 'grad_norm': 4.877923011779785, 'learning_rate': 1.9740119169423337e-05, 'epoch': 0.3}
 10%|█         | 63/624 [05:31<49:14,  5.27s/it] 10%|█         | 64/624 [05:36<49:17,  5.28s/it]                                                {'loss': 1.718, 'grad_norm': 4.657564163208008, 'learning_rate': 1.9728226572962474e-05, 'epoch': 0.31}
 10%|█         | 64/624 [05:36<49:17,  5.28s/it] 10%|█         | 65/624 [05:41<48:39,  5.22s/it]                                                {'loss': 1.5629, 'grad_norm': 5.914402961730957, 'learning_rate': 1.9716071662364454e-05, 'epoch': 0.31}
 10%|█         | 65/624 [05:41<48:39,  5.22s/it] 11%|█         | 66/624 [05:46<48:29,  5.21s/it]                                                {'loss': 1.5009, 'grad_norm': 4.327115058898926, 'learning_rate': 1.970365476537707e-05, 'epoch': 0.32}
 11%|█         | 66/624 [05:46<48:29,  5.21s/it] 11%|█         | 67/624 [05:51<48:23,  5.21s/it]                                                {'loss': 1.5381, 'grad_norm': 4.418532371520996, 'learning_rate': 1.9690976216812397e-05, 'epoch': 0.32}
 11%|█         | 67/624 [05:51<48:23,  5.21s/it] 11%|█         | 68/624 [05:57<49:04,  5.30s/it]                                                {'loss': 1.6116, 'grad_norm': 4.679203987121582, 'learning_rate': 1.9678036358537726e-05, 'epoch': 0.33}
 11%|█         | 68/624 [05:57<49:04,  5.30s/it] 11%|█         | 69/624 [06:02<49:32,  5.36s/it]                                                {'loss': 1.6198, 'grad_norm': 4.8026041984558105, 'learning_rate': 1.966483553946637e-05, 'epoch': 0.33}
 11%|█         | 69/624 [06:02<49:32,  5.36s/it] 11%|█         | 70/624 [06:07<48:49,  5.29s/it]                                                {'loss': 1.5723, 'grad_norm': 4.777866840362549, 'learning_rate': 1.9651374115548255e-05, 'epoch': 0.34}
 11%|█         | 70/624 [06:07<48:49,  5.29s/it] 11%|█▏        | 71/624 [06:13<48:31,  5.27s/it]                                                {'loss': 1.4678, 'grad_norm': 4.817140102386475, 'learning_rate': 1.9637652449760297e-05, 'epoch': 0.34}
 11%|█▏        | 71/624 [06:13<48:31,  5.27s/it] 12%|█▏        | 72/624 [06:18<47:36,  5.17s/it]                                                {'loss': 1.8387, 'grad_norm': 4.417056083679199, 'learning_rate': 1.9623670912096656e-05, 'epoch': 0.35}
 12%|█▏        | 72/624 [06:18<47:36,  5.17s/it] 12%|█▏        | 73/624 [06:23<47:03,  5.13s/it]                                                {'loss': 1.5009, 'grad_norm': 3.7893049716949463, 'learning_rate': 1.9609429879558726e-05, 'epoch': 0.35}
 12%|█▏        | 73/624 [06:23<47:03,  5.13s/it] 12%|█▏        | 74/624 [06:28<47:35,  5.19s/it]                                                {'loss': 1.69, 'grad_norm': 3.9411895275115967, 'learning_rate': 1.9594929736144978e-05, 'epoch': 0.35}
 12%|█▏        | 74/624 [06:28<47:35,  5.19s/it] 12%|█▏        | 75/624 [06:33<47:50,  5.23s/it]                                                {'loss': 1.6777, 'grad_norm': 5.162856578826904, 'learning_rate': 1.958017087284061e-05, 'epoch': 0.36}
 12%|█▏        | 75/624 [06:33<47:50,  5.23s/it] 12%|█▏        | 76/624 [06:38<47:38,  5.22s/it]                                                {'loss': 1.5634, 'grad_norm': 5.143144607543945, 'learning_rate': 1.9565153687607006e-05, 'epoch': 0.36}
 12%|█▏        | 76/624 [06:39<47:38,  5.22s/it] 12%|█▏        | 77/624 [06:43<46:34,  5.11s/it]                                                {'loss': 1.5241, 'grad_norm': 5.563735008239746, 'learning_rate': 1.9549878585371006e-05, 'epoch': 0.37}
 12%|█▏        | 77/624 [06:43<46:34,  5.11s/it] 12%|█▎        | 78/624 [06:49<48:07,  5.29s/it]                                                {'loss': 1.554, 'grad_norm': 5.185113906860352, 'learning_rate': 1.9534345978013972e-05, 'epoch': 0.37}
 12%|█▎        | 78/624 [06:49<48:07,  5.29s/it] 13%|█▎        | 79/624 [06:54<47:07,  5.19s/it]                                                {'loss': 1.6926, 'grad_norm': 4.635666370391846, 'learning_rate': 1.9518556284360696e-05, 'epoch': 0.38}
 13%|█▎        | 79/624 [06:54<47:07,  5.19s/it] 13%|█▎        | 80/624 [06:59<47:45,  5.27s/it]                                                {'loss': 1.3871, 'grad_norm': 6.125580310821533, 'learning_rate': 1.9502509930168113e-05, 'epoch': 0.38}
 13%|█▎        | 80/624 [06:59<47:45,  5.27s/it] 13%|█▎        | 81/624 [07:05<47:41,  5.27s/it]                                                {'loss': 1.5555, 'grad_norm': 7.875296115875244, 'learning_rate': 1.9486207348113803e-05, 'epoch': 0.39}
 13%|█▎        | 81/624 [07:05<47:41,  5.27s/it] 13%|█▎        | 82/624 [07:10<47:21,  5.24s/it]                                                {'loss': 1.5514, 'grad_norm': 5.721121311187744, 'learning_rate': 1.946964897778433e-05, 'epoch': 0.39}
 13%|█▎        | 82/624 [07:10<47:21,  5.24s/it] 13%|█▎        | 83/624 [07:15<46:39,  5.17s/it]                                                {'loss': 1.4114, 'grad_norm': 4.65606164932251, 'learning_rate': 1.9452835265663404e-05, 'epoch': 0.4}
 13%|█▎        | 83/624 [07:15<46:39,  5.17s/it] 13%|█▎        | 84/624 [07:20<46:47,  5.20s/it]                                                {'loss': 1.698, 'grad_norm': 4.723322868347168, 'learning_rate': 1.9435766665119823e-05, 'epoch': 0.4}
 13%|█▎        | 84/624 [07:20<46:47,  5.20s/it] 14%|█▎        | 85/624 [07:25<47:07,  5.25s/it]                                                {'loss': 1.5267, 'grad_norm': 4.854816436767578, 'learning_rate': 1.941844363639525e-05, 'epoch': 0.41}
 14%|█▎        | 85/624 [07:26<47:07,  5.25s/it] 14%|█▍        | 86/624 [07:31<47:11,  5.26s/it]                                                {'loss': 1.4115, 'grad_norm': 4.970335006713867, 'learning_rate': 1.9400866646591816e-05, 'epoch': 0.41}
 14%|█▍        | 86/624 [07:31<47:11,  5.26s/it] 14%|█▍        | 87/624 [07:36<46:36,  5.21s/it]                                                {'loss': 1.5961, 'grad_norm': 4.718233108520508, 'learning_rate': 1.9383036169659513e-05, 'epoch': 0.42}
 14%|█▍        | 87/624 [07:36<46:36,  5.21s/it] 14%|█▍        | 88/624 [07:41<46:37,  5.22s/it]                                                {'loss': 1.4275, 'grad_norm': 5.214118003845215, 'learning_rate': 1.936495268638342e-05, 'epoch': 0.42}
 14%|█▍        | 88/624 [07:41<46:37,  5.22s/it] 14%|█▍        | 89/624 [07:46<46:06,  5.17s/it]                                                {'loss': 1.3086, 'grad_norm': 4.6633195877075195, 'learning_rate': 1.934661668437073e-05, 'epoch': 0.43}
 14%|█▍        | 89/624 [07:46<46:06,  5.17s/it] 14%|█▍        | 90/624 [07:51<46:24,  5.21s/it]                                                {'loss': 1.5059, 'grad_norm': 8.320481300354004, 'learning_rate': 1.932802865803763e-05, 'epoch': 0.43}
 14%|█▍        | 90/624 [07:52<46:24,  5.21s/it] 15%|█▍        | 91/624 [07:57<47:02,  5.30s/it]                                                {'loss': 1.6045, 'grad_norm': 5.024181842803955, 'learning_rate': 1.930918910859592e-05, 'epoch': 0.44}
 15%|█▍        | 91/624 [07:57<47:02,  5.30s/it] 15%|█▍        | 92/624 [08:02<45:41,  5.15s/it]                                                {'loss': 1.6339, 'grad_norm': 4.724009037017822, 'learning_rate': 1.9290098544039546e-05, 'epoch': 0.44}
 15%|█▍        | 92/624 [08:02<45:41,  5.15s/it] 15%|█▍        | 93/624 [08:07<46:00,  5.20s/it]                                                {'loss': 1.6039, 'grad_norm': 4.7972211837768555, 'learning_rate': 1.927075747913088e-05, 'epoch': 0.45}
 15%|█▍        | 93/624 [08:07<46:00,  5.20s/it] 15%|█▌        | 94/624 [08:12<45:54,  5.20s/it]                                                {'loss': 1.6062, 'grad_norm': 4.812315464019775, 'learning_rate': 1.9251166435386837e-05, 'epoch': 0.45}
 15%|█▌        | 94/624 [08:12<45:54,  5.20s/it] 15%|█▌        | 95/624 [08:18<46:05,  5.23s/it]                                                {'loss': 1.5845, 'grad_norm': 5.038823127746582, 'learning_rate': 1.923132594106483e-05, 'epoch': 0.46}
 15%|█▌        | 95/624 [08:18<46:05,  5.23s/it] 15%|█▌        | 96/624 [08:23<45:39,  5.19s/it]                                                {'loss': 1.7781, 'grad_norm': 3.9619598388671875, 'learning_rate': 1.92112365311485e-05, 'epoch': 0.46}
 15%|█▌        | 96/624 [08:23<45:39,  5.19s/it] 16%|█▌        | 97/624 [08:28<45:38,  5.20s/it]                                                {'loss': 1.732, 'grad_norm': 5.667612552642822, 'learning_rate': 1.919089874733332e-05, 'epoch': 0.47}
 16%|█▌        | 97/624 [08:28<45:38,  5.20s/it] 16%|█▌        | 98/624 [08:33<45:06,  5.15s/it]                                                {'loss': 1.7573, 'grad_norm': 4.7296462059021, 'learning_rate': 1.9170313138011964e-05, 'epoch': 0.47}
 16%|█▌        | 98/624 [08:33<45:06,  5.15s/it] 16%|█▌        | 99/624 [08:38<45:46,  5.23s/it]                                                {'loss': 1.7184, 'grad_norm': 4.897082805633545, 'learning_rate': 1.9149480258259535e-05, 'epoch': 0.47}
 16%|█▌        | 99/624 [08:38<45:46,  5.23s/it] 16%|█▌        | 100/624 [08:44<45:42,  5.23s/it]                                                 {'loss': 1.4238, 'grad_norm': 5.375049114227295, 'learning_rate': 1.9128400669818586e-05, 'epoch': 0.48}
 16%|█▌        | 100/624 [08:44<45:42,  5.23s/it] 16%|█▌        | 101/624 [08:49<45:04,  5.17s/it]                                                 {'loss': 1.436, 'grad_norm': 4.559322357177734, 'learning_rate': 1.9107074941083987e-05, 'epoch': 0.48}
 16%|█▌        | 101/624 [08:49<45:04,  5.17s/it] 16%|█▋        | 102/624 [08:54<45:15,  5.20s/it]                                                 {'loss': 1.8267, 'grad_norm': 5.4802565574646, 'learning_rate': 1.9085503647087588e-05, 'epoch': 0.49}
 16%|█▋        | 102/624 [08:54<45:15,  5.20s/it] 17%|█▋        | 103/624 [08:59<44:08,  5.08s/it]                                                 {'loss': 1.5385, 'grad_norm': 8.280632019042969, 'learning_rate': 1.906368736948272e-05, 'epoch': 0.49}
 17%|█▋        | 103/624 [08:59<44:08,  5.08s/it] 17%|█▋        | 104/624 [09:04<44:40,  5.15s/it]                                                 {'loss': 1.5836, 'grad_norm': 5.971827030181885, 'learning_rate': 1.9041626696528503e-05, 'epoch': 0.5}
 17%|█▋        | 104/624 [09:04<44:40,  5.15s/it] 17%|█▋        | 105/624 [09:09<44:52,  5.19s/it]                                                 {'loss': 1.3788, 'grad_norm': 4.449219703674316, 'learning_rate': 1.9019322223073997e-05, 'epoch': 0.5}
 17%|█▋        | 105/624 [09:09<44:52,  5.19s/it] 17%|█▋        | 106/624 [09:15<45:10,  5.23s/it]                                                 {'loss': 1.5193, 'grad_norm': 5.765255451202393, 'learning_rate': 1.899677455054215e-05, 'epoch': 0.51}
 17%|█▋        | 106/624 [09:15<45:10,  5.23s/it] 17%|█▋        | 107/624 [09:20<45:16,  5.26s/it]                                                 {'loss': 1.4716, 'grad_norm': 4.378997325897217, 'learning_rate': 1.8973984286913584e-05, 'epoch': 0.51}
 17%|█▋        | 107/624 [09:20<45:16,  5.26s/it] 17%|█▋        | 108/624 [09:25<45:23,  5.28s/it]                                                 {'loss': 1.5708, 'grad_norm': 7.147302150726318, 'learning_rate': 1.895095204671021e-05, 'epoch': 0.52}
 17%|█▋        | 108/624 [09:25<45:23,  5.28s/it] 17%|█▋        | 109/624 [09:30<45:15,  5.27s/it]                                                 {'loss': 1.6335, 'grad_norm': 9.189751625061035, 'learning_rate': 1.892767845097864e-05, 'epoch': 0.52}
 17%|█▋        | 109/624 [09:31<45:15,  5.27s/it] 18%|█▊        | 110/624 [09:36<45:06,  5.27s/it]                                                 {'loss': 1.7038, 'grad_norm': 4.560429096221924, 'learning_rate': 1.890416412727346e-05, 'epoch': 0.53}
 18%|█▊        | 110/624 [09:36<45:06,  5.27s/it] 18%|█▊        | 111/624 [09:41<45:15,  5.29s/it]                                                 {'loss': 1.4325, 'grad_norm': 5.359573841094971, 'learning_rate': 1.88804097096403e-05, 'epoch': 0.53}
 18%|█▊        | 111/624 [09:41<45:15,  5.29s/it] 18%|█▊        | 112/624 [09:46<45:05,  5.28s/it]                                                 {'loss': 1.4538, 'grad_norm': 4.7132463455200195, 'learning_rate': 1.8856415838598738e-05, 'epoch': 0.54}
 18%|█▊        | 112/624 [09:46<45:05,  5.28s/it] 18%|█▊        | 113/624 [09:52<45:36,  5.36s/it]                                                 {'loss': 1.6205, 'grad_norm': 4.914677619934082, 'learning_rate': 1.8832183161125026e-05, 'epoch': 0.54}
 18%|█▊        | 113/624 [09:52<45:36,  5.36s/it] 18%|█▊        | 114/624 [09:57<45:34,  5.36s/it]                                                 {'loss': 1.7148, 'grad_norm': 3.8677706718444824, 'learning_rate': 1.8807712330634645e-05, 'epoch': 0.55}
 18%|█▊        | 114/624 [09:57<45:34,  5.36s/it] 18%|█▊        | 115/624 [10:02<44:15,  5.22s/it]                                                 {'loss': 1.5743, 'grad_norm': 4.935403823852539, 'learning_rate': 1.87830040069647e-05, 'epoch': 0.55}
 18%|█▊        | 115/624 [10:02<44:15,  5.22s/it] 19%|█▊        | 116/624 [10:08<44:41,  5.28s/it]                                                 {'loss': 1.7818, 'grad_norm': 4.609832286834717, 'learning_rate': 1.87580588563561e-05, 'epoch': 0.56}
 19%|█▊        | 116/624 [10:08<44:41,  5.28s/it] 19%|█▉        | 117/624 [10:13<44:41,  5.29s/it]                                                 {'loss': 1.6252, 'grad_norm': 4.104889392852783, 'learning_rate': 1.873287755143563e-05, 'epoch': 0.56}
 19%|█▉        | 117/624 [10:13<44:41,  5.29s/it] 19%|█▉        | 118/624 [10:18<44:17,  5.25s/it]                                                 {'loss': 1.5122, 'grad_norm': 4.0991530418396, 'learning_rate': 1.8707460771197773e-05, 'epoch': 0.57}
 19%|█▉        | 118/624 [10:18<44:17,  5.25s/it] 19%|█▉        | 119/624 [10:23<44:30,  5.29s/it]                                                 {'loss': 1.4951, 'grad_norm': 5.00514554977417, 'learning_rate': 1.868180920098644e-05, 'epoch': 0.57}
 19%|█▉        | 119/624 [10:24<44:30,  5.29s/it] 19%|█▉        | 120/624 [10:28<43:40,  5.20s/it]                                                 {'loss': 1.5634, 'grad_norm': 6.2210540771484375, 'learning_rate': 1.8655923532476463e-05, 'epoch': 0.58}
 19%|█▉        | 120/624 [10:29<43:40,  5.20s/it] 19%|█▉        | 121/624 [10:34<43:32,  5.19s/it]                                                 {'loss': 1.6518, 'grad_norm': 4.412484169006348, 'learning_rate': 1.8629804463654956e-05, 'epoch': 0.58}
 19%|█▉        | 121/624 [10:34<43:32,  5.19s/it] 20%|█▉        | 122/624 [10:39<43:01,  5.14s/it]                                                 {'loss': 1.5696, 'grad_norm': 3.8530681133270264, 'learning_rate': 1.8603452698802498e-05, 'epoch': 0.59}
 20%|█▉        | 122/624 [10:39<43:01,  5.14s/it] 20%|█▉        | 123/624 [10:44<42:34,  5.10s/it]                                                 {'loss': 1.4565, 'grad_norm': 4.455025672912598, 'learning_rate': 1.857686894847413e-05, 'epoch': 0.59}
 20%|█▉        | 123/624 [10:44<42:34,  5.10s/it] 20%|█▉        | 124/624 [10:49<43:07,  5.17s/it]                                                 {'loss': 1.5975, 'grad_norm': 6.343365669250488, 'learning_rate': 1.8550053929480202e-05, 'epoch': 0.59}
 20%|█▉        | 124/624 [10:49<43:07,  5.17s/it] 20%|██        | 125/624 [10:54<43:00,  5.17s/it]                                                 {'loss': 1.5272, 'grad_norm': 3.9284825325012207, 'learning_rate': 1.8523008364867056e-05, 'epoch': 0.6}
 20%|██        | 125/624 [10:54<43:00,  5.17s/it] 20%|██        | 126/624 [11:00<43:37,  5.26s/it]                                                 {'loss': 1.6259, 'grad_norm': 4.455328464508057, 'learning_rate': 1.8495732983897504e-05, 'epoch': 0.6}
 20%|██        | 126/624 [11:00<43:37,  5.26s/it] 20%|██        | 127/624 [11:05<43:33,  5.26s/it]                                                 {'loss': 1.5305, 'grad_norm': 4.885084629058838, 'learning_rate': 1.8468228522031197e-05, 'epoch': 0.61}
 20%|██        | 127/624 [11:05<43:33,  5.26s/it] 21%|██        | 128/624 [11:10<43:51,  5.30s/it]                                                 {'loss': 1.7052, 'grad_norm': 5.8149895668029785, 'learning_rate': 1.8440495720904758e-05, 'epoch': 0.61}
 21%|██        | 128/624 [11:10<43:51,  5.30s/it] 21%|██        | 129/624 [11:16<43:42,  5.30s/it]                                                 {'loss': 1.5129, 'grad_norm': 4.216800212860107, 'learning_rate': 1.8412535328311813e-05, 'epoch': 0.62}
 21%|██        | 129/624 [11:16<43:42,  5.30s/it] 21%|██        | 130/624 [11:21<43:18,  5.26s/it]                                                 {'loss': 1.5246, 'grad_norm': 4.392363548278809, 'learning_rate': 1.8384348098182815e-05, 'epoch': 0.62}
 21%|██        | 130/624 [11:21<43:18,  5.26s/it] 21%|██        | 131/624 [11:26<42:42,  5.20s/it]                                                 {'loss': 1.6169, 'grad_norm': 4.868919849395752, 'learning_rate': 1.8355934790564718e-05, 'epoch': 0.63}
 21%|██        | 131/624 [11:26<42:42,  5.20s/it] 21%|██        | 132/624 [11:31<42:00,  5.12s/it]                                                 {'loss': 1.5476, 'grad_norm': 4.926676273345947, 'learning_rate': 1.832729617160047e-05, 'epoch': 0.63}
 21%|██        | 132/624 [11:31<42:00,  5.12s/it] 21%|██▏       | 133/624 [11:36<41:44,  5.10s/it]                                                 {'loss': 1.6453, 'grad_norm': 4.316206455230713, 'learning_rate': 1.8298433013508384e-05, 'epoch': 0.64}
 21%|██▏       | 133/624 [11:36<41:44,  5.10s/it] 21%|██▏       | 134/624 [11:41<42:08,  5.16s/it]                                                 {'loss': 1.5362, 'grad_norm': 5.098293781280518, 'learning_rate': 1.826934609456129e-05, 'epoch': 0.64}
 21%|██▏       | 134/624 [11:41<42:08,  5.16s/it] 22%|██▏       | 135/624 [11:46<42:18,  5.19s/it]                                                 {'loss': 1.785, 'grad_norm': 4.316671848297119, 'learning_rate': 1.8240036199065546e-05, 'epoch': 0.65}
 22%|██▏       | 135/624 [11:46<42:18,  5.19s/it] 22%|██▏       | 136/624 [11:52<42:21,  5.21s/it]                                                 {'loss': 1.5321, 'grad_norm': 4.157290935516357, 'learning_rate': 1.8210504117339917e-05, 'epoch': 0.65}
 22%|██▏       | 136/624 [11:52<42:21,  5.21s/it] 22%|██▏       | 137/624 [11:57<42:17,  5.21s/it]                                                 {'loss': 1.6706, 'grad_norm': 4.354841709136963, 'learning_rate': 1.8180750645694236e-05, 'epoch': 0.66}
 22%|██▏       | 137/624 [11:57<42:17,  5.21s/it] 22%|██▏       | 138/624 [12:02<41:56,  5.18s/it]                                                 {'loss': 1.7331, 'grad_norm': 8.660981178283691, 'learning_rate': 1.8150776586407957e-05, 'epoch': 0.66}
 22%|██▏       | 138/624 [12:02<41:56,  5.18s/it] 22%|██▏       | 139/624 [12:07<42:20,  5.24s/it]                                                 {'loss': 1.4818, 'grad_norm': 4.658229827880859, 'learning_rate': 1.8120582747708503e-05, 'epoch': 0.67}
 22%|██▏       | 139/624 [12:07<42:20,  5.24s/it] 22%|██▏       | 140/624 [12:12<42:00,  5.21s/it]                                                 {'loss': 1.5921, 'grad_norm': 4.988937854766846, 'learning_rate': 1.8090169943749477e-05, 'epoch': 0.67}
 22%|██▏       | 140/624 [12:13<42:00,  5.21s/it] 23%|██▎       | 141/624 [12:17<41:35,  5.17s/it]                                                 {'loss': 1.6034, 'grad_norm': 4.895542144775391, 'learning_rate': 1.8059538994588715e-05, 'epoch': 0.68}
 23%|██▎       | 141/624 [12:18<41:35,  5.17s/it] 23%|██▎       | 142/624 [12:23<41:43,  5.19s/it]                                                 {'loss': 1.5982, 'grad_norm': 4.762673854827881, 'learning_rate': 1.8028690726166172e-05, 'epoch': 0.68}
 23%|██▎       | 142/624 [12:23<41:43,  5.19s/it] 23%|██▎       | 143/624 [12:28<42:23,  5.29s/it]                                                 {'loss': 1.6709, 'grad_norm': 5.675724983215332, 'learning_rate': 1.7997625970281652e-05, 'epoch': 0.69}
 23%|██▎       | 143/624 [12:28<42:23,  5.29s/it] 23%|██▎       | 144/624 [12:33<41:11,  5.15s/it]                                                 {'loss': 1.6893, 'grad_norm': 5.353597164154053, 'learning_rate': 1.796634556457236e-05, 'epoch': 0.69}
 23%|██▎       | 144/624 [12:33<41:11,  5.15s/it] 23%|██▎       | 145/624 [12:38<41:31,  5.20s/it]                                                 {'loss': 1.1924, 'grad_norm': 4.790133476257324, 'learning_rate': 1.793485035249036e-05, 'epoch': 0.7}
 23%|██▎       | 145/624 [12:39<41:31,  5.20s/it] 23%|██▎       | 146/624 [12:44<41:52,  5.26s/it]                                                 {'loss': 1.4723, 'grad_norm': 4.830645561218262, 'learning_rate': 1.7903141183279776e-05, 'epoch': 0.7}
 23%|██▎       | 146/624 [12:44<41:52,  5.26s/it] 24%|██▎       | 147/624 [12:49<41:31,  5.22s/it]                                                 {'loss': 1.5679, 'grad_norm': 4.837806701660156, 'learning_rate': 1.7871218911953942e-05, 'epoch': 0.71}
 24%|██▎       | 147/624 [12:49<41:31,  5.22s/it] 24%|██▎       | 148/624 [12:54<41:27,  5.23s/it]                                                 {'loss': 1.6207, 'grad_norm': 4.1108574867248535, 'learning_rate': 1.7839084399272317e-05, 'epoch': 0.71}
 24%|██▎       | 148/624 [12:54<41:27,  5.23s/it] 24%|██▍       | 149/624 [12:59<40:51,  5.16s/it]                                                 {'loss': 1.781, 'grad_norm': 5.50209903717041, 'learning_rate': 1.780673851171728e-05, 'epoch': 0.71}
 24%|██▍       | 149/624 [12:59<40:51,  5.16s/it] 24%|██▍       | 150/624 [13:04<40:37,  5.14s/it]                                                 {'loss': 1.7675, 'grad_norm': 5.608822822570801, 'learning_rate': 1.777418212147079e-05, 'epoch': 0.72}
 24%|██▍       | 150/624 [13:04<40:37,  5.14s/it] 24%|██▍       | 151/624 [13:09<40:26,  5.13s/it]                                                 {'loss': 1.5858, 'grad_norm': 5.338605880737305, 'learning_rate': 1.7741416106390828e-05, 'epoch': 0.72}
 24%|██▍       | 151/624 [13:09<40:26,  5.13s/it] 24%|██▍       | 152/624 [13:14<40:11,  5.11s/it]                                                 {'loss': 1.5106, 'grad_norm': 5.363927841186523, 'learning_rate': 1.7708441349987753e-05, 'epoch': 0.73}
 24%|██▍       | 152/624 [13:15<40:11,  5.11s/it] 25%|██▍       | 153/624 [13:19<40:00,  5.10s/it]                                                 {'loss': 1.6264, 'grad_norm': 4.356311798095703, 'learning_rate': 1.767525874140048e-05, 'epoch': 0.73}
 25%|██▍       | 153/624 [13:20<40:00,  5.10s/it] 25%|██▍       | 154/624 [13:25<40:36,  5.18s/it]                                                 {'loss': 1.6866, 'grad_norm': 4.418972015380859, 'learning_rate': 1.7641869175372493e-05, 'epoch': 0.74}
 25%|██▍       | 154/624 [13:25<40:36,  5.18s/it] 25%|██▍       | 155/624 [13:30<40:28,  5.18s/it]                                                 {'loss': 1.4793, 'grad_norm': 9.122152328491211, 'learning_rate': 1.7608273552227723e-05, 'epoch': 0.74}
 25%|██▍       | 155/624 [13:30<40:28,  5.18s/it] 25%|██▌       | 156/624 [13:35<40:28,  5.19s/it]                                                 {'loss': 1.5243, 'grad_norm': 5.757606029510498, 'learning_rate': 1.7574472777846276e-05, 'epoch': 0.75}
 25%|██▌       | 156/624 [13:35<40:28,  5.19s/it] 25%|██▌       | 157/624 [13:40<40:05,  5.15s/it]                                                 {'loss': 1.7274, 'grad_norm': 5.239233016967773, 'learning_rate': 1.7540467763639994e-05, 'epoch': 0.75}
 25%|██▌       | 157/624 [13:40<40:05,  5.15s/it] 25%|██▌       | 158/624 [13:45<39:45,  5.12s/it]                                                 {'loss': 1.4523, 'grad_norm': 6.368388652801514, 'learning_rate': 1.7506259426527903e-05, 'epoch': 0.76}
 25%|██▌       | 158/624 [13:45<39:45,  5.12s/it] 25%|██▌       | 159/624 [13:51<39:55,  5.15s/it]                                                 {'loss': 1.7214, 'grad_norm': 5.359009265899658, 'learning_rate': 1.7471848688911465e-05, 'epoch': 0.76}
 25%|██▌       | 159/624 [13:51<39:55,  5.15s/it] 26%|██▌       | 160/624 [13:55<38:43,  5.01s/it]                                                 {'loss': 1.6688, 'grad_norm': 6.125185012817383, 'learning_rate': 1.7437236478649718e-05, 'epoch': 0.77}
 26%|██▌       | 160/624 [13:55<38:43,  5.01s/it] 26%|██▌       | 161/624 [14:00<39:07,  5.07s/it]                                                 {'loss': 1.4723, 'grad_norm': 4.317306041717529, 'learning_rate': 1.7402423729034252e-05, 'epoch': 0.77}
 26%|██▌       | 161/624 [14:01<39:07,  5.07s/it] 26%|██▌       | 162/624 [14:06<39:55,  5.19s/it]                                                 {'loss': 1.6158, 'grad_norm': 4.1500563621521, 'learning_rate': 1.736741137876405e-05, 'epoch': 0.78}
 26%|██▌       | 162/624 [14:06<39:55,  5.19s/it] 26%|██▌       | 163/624 [14:12<41:12,  5.36s/it]                                                 {'loss': 1.5463, 'grad_norm': 8.26395320892334, 'learning_rate': 1.7332200371920173e-05, 'epoch': 0.78}
 26%|██▌       | 163/624 [14:12<41:12,  5.36s/it] 26%|██▋       | 164/624 [14:17<41:49,  5.46s/it]                                                 {'loss': 1.7034, 'grad_norm': 4.417717456817627, 'learning_rate': 1.72967916579403e-05, 'epoch': 0.79}
 26%|██▋       | 164/624 [14:18<41:49,  5.46s/it] 26%|██▋       | 165/624 [14:23<41:07,  5.38s/it]                                                 {'loss': 1.4375, 'grad_norm': 5.301462173461914, 'learning_rate': 1.7261186191593135e-05, 'epoch': 0.79}
 26%|██▋       | 165/624 [14:23<41:07,  5.38s/it] 27%|██▋       | 166/624 [14:28<40:35,  5.32s/it]                                                 {'loss': 1.5259, 'grad_norm': 4.9584879875183105, 'learning_rate': 1.7225384932952655e-05, 'epoch': 0.8}
 27%|██▋       | 166/624 [14:28<40:35,  5.32s/it] 27%|██▋       | 167/624 [14:33<40:35,  5.33s/it]                                                 {'loss': 1.4747, 'grad_norm': 4.679476261138916, 'learning_rate': 1.7189388847372227e-05, 'epoch': 0.8}
 27%|██▋       | 167/624 [14:33<40:35,  5.33s/it] 27%|██▋       | 168/624 [14:38<40:05,  5.27s/it]                                                 {'loss': 1.6821, 'grad_norm': 4.847097873687744, 'learning_rate': 1.715319890545857e-05, 'epoch': 0.81}
 27%|██▋       | 168/624 [14:38<40:05,  5.27s/it] 27%|██▋       | 169/624 [14:43<39:52,  5.26s/it]                                                 {'loss': 1.5712, 'grad_norm': 4.814277648925781, 'learning_rate': 1.7116816083045603e-05, 'epoch': 0.81}
 27%|██▋       | 169/624 [14:44<39:52,  5.26s/it] 27%|██▋       | 170/624 [14:49<39:33,  5.23s/it]                                                 {'loss': 1.5631, 'grad_norm': 4.824975967407227, 'learning_rate': 1.7080241361168108e-05, 'epoch': 0.82}
 27%|██▋       | 170/624 [14:49<39:33,  5.23s/it] 27%|██▋       | 171/624 [14:54<39:52,  5.28s/it]                                                 {'loss': 1.5587, 'grad_norm': 3.7520415782928467, 'learning_rate': 1.704347572603529e-05, 'epoch': 0.82}
 27%|██▋       | 171/624 [14:54<39:52,  5.28s/it] 28%|██▊       | 172/624 [14:59<39:48,  5.28s/it]                                                 {'loss': 1.8153, 'grad_norm': 4.330391883850098, 'learning_rate': 1.700652016900419e-05, 'epoch': 0.82}
 28%|██▊       | 172/624 [14:59<39:48,  5.28s/it] 28%|██▊       | 173/624 [15:05<39:55,  5.31s/it]                                                 {'loss': 1.6444, 'grad_norm': 5.295398235321045, 'learning_rate': 1.696937568655294e-05, 'epoch': 0.83}
 28%|██▊       | 173/624 [15:05<39:55,  5.31s/it] 28%|██▊       | 174/624 [15:10<39:26,  5.26s/it]                                                 {'loss': 1.4373, 'grad_norm': 5.401592254638672, 'learning_rate': 1.6932043280253892e-05, 'epoch': 0.83}
 28%|██▊       | 174/624 [15:10<39:26,  5.26s/it] 28%|██▊       | 175/624 [15:15<38:31,  5.15s/it]                                                 {'loss': 1.629, 'grad_norm': 3.954749584197998, 'learning_rate': 1.689452395674664e-05, 'epoch': 0.84}
 28%|██▊       | 175/624 [15:15<38:31,  5.15s/it] 28%|██▊       | 176/624 [15:20<38:40,  5.18s/it]                                                 {'loss': 1.5744, 'grad_norm': 4.022961616516113, 'learning_rate': 1.6856818727710847e-05, 'epoch': 0.84}
 28%|██▊       | 176/624 [15:20<38:40,  5.18s/it] 28%|██▊       | 177/624 [15:25<38:15,  5.14s/it]                                                 {'loss': 1.4415, 'grad_norm': 5.965929985046387, 'learning_rate': 1.6818928609838967e-05, 'epoch': 0.85}
 28%|██▊       | 177/624 [15:25<38:15,  5.14s/it] 29%|██▊       | 178/624 [15:30<38:30,  5.18s/it]                                                 {'loss': 1.64, 'grad_norm': 5.028009414672852, 'learning_rate': 1.678085462480885e-05, 'epoch': 0.85}
 29%|██▊       | 178/624 [15:30<38:30,  5.18s/it] 29%|██▊       | 179/624 [15:35<38:10,  5.15s/it]                                                 {'loss': 1.4429, 'grad_norm': 4.852685928344727, 'learning_rate': 1.6742597799256182e-05, 'epoch': 0.86}
 29%|██▊       | 179/624 [15:35<38:10,  5.15s/it] 29%|██▉       | 180/624 [15:41<38:18,  5.18s/it]                                                 {'loss': 1.5265, 'grad_norm': 4.793160438537598, 'learning_rate': 1.6704159164746797e-05, 'epoch': 0.86}
 29%|██▉       | 180/624 [15:41<38:18,  5.18s/it] 29%|██▉       | 181/624 [15:46<38:05,  5.16s/it]                                                 {'loss': 1.6389, 'grad_norm': 4.857089519500732, 'learning_rate': 1.6665539757748866e-05, 'epoch': 0.87}
 29%|██▉       | 181/624 [15:46<38:05,  5.16s/it] 29%|██▉       | 182/624 [15:51<38:19,  5.20s/it]                                                 {'loss': 1.5922, 'grad_norm': 3.8119914531707764, 'learning_rate': 1.6626740619604967e-05, 'epoch': 0.87}
 29%|██▉       | 182/624 [15:51<38:19,  5.20s/it] 29%|██▉       | 183/624 [15:56<38:15,  5.21s/it]                                                 {'loss': 1.7765, 'grad_norm': 4.54892635345459, 'learning_rate': 1.658776279650397e-05, 'epoch': 0.88}
 29%|██▉       | 183/624 [15:56<38:15,  5.21s/it] 29%|██▉       | 184/624 [16:02<38:48,  5.29s/it]                                                 {'loss': 1.4717, 'grad_norm': 3.8690857887268066, 'learning_rate': 1.6548607339452853e-05, 'epoch': 0.88}
 29%|██▉       | 184/624 [16:02<38:48,  5.29s/it] 30%|██▉       | 185/624 [16:07<38:55,  5.32s/it]                                                 {'loss': 1.5398, 'grad_norm': 4.909703731536865, 'learning_rate': 1.6509275304248366e-05, 'epoch': 0.89}
 30%|██▉       | 185/624 [16:07<38:55,  5.32s/it] 30%|██▉       | 186/624 [16:12<38:43,  5.31s/it]                                                 {'loss': 1.6399, 'grad_norm': 4.994040489196777, 'learning_rate': 1.6469767751448538e-05, 'epoch': 0.89}
 30%|██▉       | 186/624 [16:13<38:43,  5.31s/it] 30%|██▉       | 187/624 [16:17<38:03,  5.23s/it]                                                 {'loss': 1.4902, 'grad_norm': 4.611059188842773, 'learning_rate': 1.6430085746344107e-05, 'epoch': 0.9}
 30%|██▉       | 187/624 [16:18<38:03,  5.23s/it] 30%|███       | 188/624 [16:23<38:24,  5.29s/it]                                                 {'loss': 1.5728, 'grad_norm': 4.685667991638184, 'learning_rate': 1.639023035892978e-05, 'epoch': 0.9}
 30%|███       | 188/624 [16:23<38:24,  5.29s/it] 30%|███       | 189/624 [16:28<37:57,  5.24s/it]                                                 {'loss': 1.5754, 'grad_norm': 4.590074062347412, 'learning_rate': 1.6350202663875385e-05, 'epoch': 0.91}
 30%|███       | 189/624 [16:28<37:57,  5.24s/it] 30%|███       | 190/624 [16:33<38:19,  5.30s/it]                                                 {'loss': 1.4849, 'grad_norm': 4.5948615074157715, 'learning_rate': 1.6310003740496887e-05, 'epoch': 0.91}
 30%|███       | 190/624 [16:34<38:19,  5.30s/it] 31%|███       | 191/624 [16:39<38:25,  5.32s/it]                                                 {'loss': 1.7063, 'grad_norm': 4.64153528213501, 'learning_rate': 1.6269634672727296e-05, 'epoch': 0.92}
 31%|███       | 191/624 [16:39<38:25,  5.32s/it] 31%|███       | 192/624 [16:44<38:17,  5.32s/it]                                                 {'loss': 1.3061, 'grad_norm': 4.112253189086914, 'learning_rate': 1.6229096549087434e-05, 'epoch': 0.92}
 31%|███       | 192/624 [16:44<38:17,  5.32s/it] 31%|███       | 193/624 [16:49<37:42,  5.25s/it]                                                 {'loss': 1.5577, 'grad_norm': 4.7061285972595215, 'learning_rate': 1.618839046265658e-05, 'epoch': 0.93}
 31%|███       | 193/624 [16:49<37:42,  5.25s/it] 31%|███       | 194/624 [16:54<37:25,  5.22s/it]                                                 {'loss': 1.6676, 'grad_norm': 4.3789215087890625, 'learning_rate': 1.614751751104301e-05, 'epoch': 0.93}
 31%|███       | 194/624 [16:54<37:25,  5.22s/it] 31%|███▏      | 195/624 [17:00<37:40,  5.27s/it]                                                 {'loss': 1.6304, 'grad_norm': 4.607264995574951, 'learning_rate': 1.6106478796354382e-05, 'epoch': 0.94}
 31%|███▏      | 195/624 [17:00<37:40,  5.27s/it] 31%|███▏      | 196/624 [17:05<37:30,  5.26s/it]                                                 {'loss': 1.5923, 'grad_norm': 3.945610284805298, 'learning_rate': 1.6065275425168034e-05, 'epoch': 0.94}
 31%|███▏      | 196/624 [17:05<37:30,  5.26s/it] 32%|███▏      | 197/624 [17:10<37:32,  5.28s/it]                                                 {'loss': 1.6858, 'grad_norm': 4.714414596557617, 'learning_rate': 1.602390850850113e-05, 'epoch': 0.94}
 32%|███▏      | 197/624 [17:10<37:32,  5.28s/it] 32%|███▏      | 198/624 [17:16<37:25,  5.27s/it]                                                 {'loss': 1.4952, 'grad_norm': 4.457999229431152, 'learning_rate': 1.5982379161780722e-05, 'epoch': 0.95}
 32%|███▏      | 198/624 [17:16<37:25,  5.27s/it] 32%|███▏      | 199/624 [17:21<37:41,  5.32s/it]                                                 {'loss': 1.7325, 'grad_norm': 5.044986248016357, 'learning_rate': 1.5940688504813664e-05, 'epoch': 0.95}
 32%|███▏      | 199/624 [17:21<37:41,  5.32s/it] 32%|███▏      | 200/624 [17:26<37:06,  5.25s/it]                                                 {'loss': 1.5137, 'grad_norm': 4.079035758972168, 'learning_rate': 1.5898837661756405e-05, 'epoch': 0.96}
 32%|███▏      | 200/624 [17:26<37:06,  5.25s/it] 32%|███▏      | 201/624 [17:31<36:33,  5.19s/it]                                                 {'loss': 1.6127, 'grad_norm': 5.228094100952148, 'learning_rate': 1.5856827761084698e-05, 'epoch': 0.96}
 32%|███▏      | 201/624 [17:31<36:33,  5.19s/it] 32%|███▏      | 202/624 [17:36<36:28,  5.19s/it]                                                 {'loss': 1.3793, 'grad_norm': 4.307659149169922, 'learning_rate': 1.5814659935563165e-05, 'epoch': 0.97}
 32%|███▏      | 202/624 [17:36<36:28,  5.19s/it] 33%|███▎      | 203/624 [17:42<37:02,  5.28s/it]                                                 {'loss': 1.5918, 'grad_norm': 3.7566049098968506, 'learning_rate': 1.577233532221474e-05, 'epoch': 0.97}
 33%|███▎      | 203/624 [17:42<37:02,  5.28s/it] 33%|███▎      | 204/624 [17:47<36:56,  5.28s/it]                                                 {'loss': 1.6214, 'grad_norm': 3.857295036315918, 'learning_rate': 1.5729855062290024e-05, 'epoch': 0.98}
 33%|███▎      | 204/624 [17:47<36:56,  5.28s/it] 33%|███▎      | 205/624 [17:52<36:50,  5.27s/it]                                                 {'loss': 1.5142, 'grad_norm': 5.3030686378479, 'learning_rate': 1.568722030123651e-05, 'epoch': 0.98}
 33%|███▎      | 205/624 [17:52<36:50,  5.27s/it] 33%|███▎      | 206/624 [17:57<36:11,  5.20s/it]                                                 {'loss': 1.466, 'grad_norm': 5.0031046867370605, 'learning_rate': 1.5644432188667695e-05, 'epoch': 0.99}
 33%|███▎      | 206/624 [17:57<36:11,  5.20s/it] 33%|███▎      | 207/624 [18:02<35:41,  5.14s/it]                                                 {'loss': 1.5641, 'grad_norm': 3.9672341346740723, 'learning_rate': 1.5601491878332077e-05, 'epoch': 0.99}
 33%|███▎      | 207/624 [18:02<35:41,  5.14s/it] 33%|███▎      | 208/624 [18:08<36:03,  5.20s/it]                                                 {'loss': 1.4404, 'grad_norm': 4.302066326141357, 'learning_rate': 1.5558400528082057e-05, 'epoch': 1.0}
 33%|███▎      | 208/624 [18:08<36:03,  5.20s/it] 33%|███▎      | 209/624 [18:13<35:55,  5.19s/it]                                                 {'loss': 1.1645, 'grad_norm': 5.667808532714844, 'learning_rate': 1.551515929984271e-05, 'epoch': 1.0}
 33%|███▎      | 209/624 [18:13<35:55,  5.19s/it] 34%|███▎      | 210/624 [18:18<36:17,  5.26s/it]                                                 {'loss': 0.8212, 'grad_norm': 4.0991902351379395, 'learning_rate': 1.547176935958044e-05, 'epoch': 1.01}
 34%|███▎      | 210/624 [18:18<36:17,  5.26s/it] 34%|███▍      | 211/624 [18:24<36:13,  5.26s/it]                                                 {'loss': 0.7164, 'grad_norm': 4.0514326095581055, 'learning_rate': 1.5428231877271584e-05, 'epoch': 1.01}
 34%|███▍      | 211/624 [18:24<36:13,  5.26s/it] 34%|███▍      | 212/624 [18:29<36:02,  5.25s/it]                                                 {'loss': 0.8455, 'grad_norm': 5.036642551422119, 'learning_rate': 1.538454802687081e-05, 'epoch': 1.02}
 34%|███▍      | 212/624 [18:29<36:02,  5.25s/it] 34%|███▍      | 213/624 [18:34<35:41,  5.21s/it]                                                 {'loss': 0.9146, 'grad_norm': 5.317973613739014, 'learning_rate': 1.5340718986279505e-05, 'epoch': 1.02}
 34%|███▍      | 213/624 [18:34<35:41,  5.21s/it] 34%|███▍      | 214/624 [18:39<35:16,  5.16s/it]                                                 {'loss': 0.8998, 'grad_norm': 6.9928059577941895, 'learning_rate': 1.529674593731399e-05, 'epoch': 1.03}
 34%|███▍      | 214/624 [18:39<35:16,  5.16s/it] 34%|███▍      | 215/624 [18:44<34:41,  5.09s/it]                                                 {'loss': 0.8456, 'grad_norm': 5.521327018737793, 'learning_rate': 1.5252630065673662e-05, 'epoch': 1.03}
 34%|███▍      | 215/624 [18:44<34:41,  5.09s/it] 35%|███▍      | 216/624 [18:49<34:30,  5.07s/it]                                                 {'loss': 0.8099, 'grad_norm': 4.139918327331543, 'learning_rate': 1.5208372560909031e-05, 'epoch': 1.04}
 35%|███▍      | 216/624 [18:49<34:30,  5.07s/it] 35%|███▍      | 217/624 [18:54<35:00,  5.16s/it]                                                 {'loss': 0.8267, 'grad_norm': 5.481142520904541, 'learning_rate': 1.5163974616389621e-05, 'epoch': 1.04}
 35%|███▍      | 217/624 [18:54<35:00,  5.16s/it] 35%|███▍      | 218/624 [19:00<36:00,  5.32s/it]                                                 {'loss': 0.822, 'grad_norm': 4.723823070526123, 'learning_rate': 1.5119437429271813e-05, 'epoch': 1.05}
 35%|███▍      | 218/624 [19:00<36:00,  5.32s/it] 35%|███▌      | 219/624 [19:05<35:43,  5.29s/it]                                                 {'loss': 0.8027, 'grad_norm': 4.242109298706055, 'learning_rate': 1.5074762200466557e-05, 'epoch': 1.05}
 35%|███▌      | 219/624 [19:05<35:43,  5.29s/it] 35%|███▌      | 220/624 [19:11<35:56,  5.34s/it]                                                 {'loss': 0.8308, 'grad_norm': 4.1325812339782715, 'learning_rate': 1.5029950134606991e-05, 'epoch': 1.06}
 35%|███▌      | 220/624 [19:11<35:56,  5.34s/it] 35%|███▌      | 221/624 [19:16<35:22,  5.27s/it]                                                 {'loss': 0.8277, 'grad_norm': 4.13567590713501, 'learning_rate': 1.4985002440015959e-05, 'epoch': 1.06}
 35%|███▌      | 221/624 [19:16<35:22,  5.27s/it] 36%|███▌      | 222/624 [19:21<34:37,  5.17s/it]                                                 {'loss': 0.7202, 'grad_norm': 4.966865062713623, 'learning_rate': 1.4939920328673422e-05, 'epoch': 1.06}
 36%|███▌      | 222/624 [19:21<34:37,  5.17s/it] 36%|███▌      | 223/624 [19:26<35:10,  5.26s/it]                                                 {'loss': 0.5404, 'grad_norm': 4.070055961608887, 'learning_rate': 1.4894705016183803e-05, 'epoch': 1.07}
 36%|███▌      | 223/624 [19:26<35:10,  5.26s/it] 36%|███▌      | 224/624 [19:31<35:11,  5.28s/it]                                                 {'loss': 0.9544, 'grad_norm': 5.376399517059326, 'learning_rate': 1.4849357721743169e-05, 'epoch': 1.07}
 36%|███▌      | 224/624 [19:32<35:11,  5.28s/it] 36%|███▌      | 225/624 [19:37<35:16,  5.30s/it]                                                 {'loss': 0.8111, 'grad_norm': 4.678669452667236, 'learning_rate': 1.4803879668106393e-05, 'epoch': 1.08}
 36%|███▌      | 225/624 [19:37<35:16,  5.30s/it] 36%|███▌      | 226/624 [19:42<35:01,  5.28s/it]                                                 {'loss': 0.7275, 'grad_norm': 5.147444725036621, 'learning_rate': 1.4758272081554168e-05, 'epoch': 1.08}
 36%|███▌      | 226/624 [19:42<35:01,  5.28s/it] 36%|███▋      | 227/624 [19:47<34:54,  5.28s/it]                                                 {'loss': 0.8786, 'grad_norm': 4.2869062423706055, 'learning_rate': 1.4712536191859934e-05, 'epoch': 1.09}
 36%|███▋      | 227/624 [19:47<34:54,  5.28s/it] 37%|███▋      | 228/624 [19:53<35:12,  5.34s/it]                                                 {'loss': 0.6929, 'grad_norm': 5.09514856338501, 'learning_rate': 1.4666673232256738e-05, 'epoch': 1.09}
 37%|███▋      | 228/624 [19:53<35:12,  5.34s/it] 37%|███▋      | 229/624 [19:58<35:36,  5.41s/it]                                                 {'loss': 0.9422, 'grad_norm': 4.29386043548584, 'learning_rate': 1.4620684439403962e-05, 'epoch': 1.1}
 37%|███▋      | 229/624 [19:59<35:36,  5.41s/it] 37%|███▋      | 230/624 [20:03<34:27,  5.25s/it]                                                 {'loss': 0.9672, 'grad_norm': 5.410019874572754, 'learning_rate': 1.4574571053353987e-05, 'epoch': 1.1}
 37%|███▋      | 230/624 [20:03<34:27,  5.25s/it] 37%|███▋      | 231/624 [20:08<33:24,  5.10s/it]                                                 {'loss': 0.7594, 'grad_norm': 4.62768030166626, 'learning_rate': 1.452833431751875e-05, 'epoch': 1.11}
 37%|███▋      | 231/624 [20:08<33:24,  5.10s/it] 37%|███▋      | 232/624 [20:13<33:28,  5.12s/it]                                                 {'loss': 0.7875, 'grad_norm': 4.057698726654053, 'learning_rate': 1.448197547863622e-05, 'epoch': 1.11}
 37%|███▋      | 232/624 [20:13<33:28,  5.12s/it] 37%|███▋      | 233/624 [20:18<32:54,  5.05s/it]                                                 {'loss': 0.8031, 'grad_norm': 3.9013161659240723, 'learning_rate': 1.4435495786736796e-05, 'epoch': 1.12}
 37%|███▋      | 233/624 [20:18<32:54,  5.05s/it] 38%|███▊      | 234/624 [20:23<32:59,  5.08s/it]                                                 {'loss': 0.8745, 'grad_norm': 4.690709590911865, 'learning_rate': 1.438889649510956e-05, 'epoch': 1.12}
 38%|███▊      | 234/624 [20:23<32:59,  5.08s/it] 38%|███▊      | 235/624 [20:28<33:01,  5.09s/it]                                                 {'loss': 0.9414, 'grad_norm': 4.434240341186523, 'learning_rate': 1.4342178860268523e-05, 'epoch': 1.13}
 38%|███▊      | 235/624 [20:28<33:01,  5.09s/it] 38%|███▊      | 236/624 [20:34<33:18,  5.15s/it]                                                 {'loss': 0.764, 'grad_norm': 4.44024133682251, 'learning_rate': 1.4295344141918734e-05, 'epoch': 1.13}
 38%|███▊      | 236/624 [20:34<33:18,  5.15s/it] 38%|███▊      | 237/624 [20:39<33:09,  5.14s/it]                                                 {'loss': 1.0312, 'grad_norm': 7.253756999969482, 'learning_rate': 1.4248393602922299e-05, 'epoch': 1.14}
 38%|███▊      | 237/624 [20:39<33:09,  5.14s/it] 38%|███▊      | 238/624 [20:44<33:35,  5.22s/it]                                                 {'loss': 0.6932, 'grad_norm': 4.53109884262085, 'learning_rate': 1.420132850926434e-05, 'epoch': 1.14}
 38%|███▊      | 238/624 [20:44<33:35,  5.22s/it] 38%|███▊      | 239/624 [20:49<33:27,  5.21s/it]                                                 {'loss': 0.7024, 'grad_norm': 4.434807300567627, 'learning_rate': 1.4154150130018867e-05, 'epoch': 1.15}
 38%|███▊      | 239/624 [20:49<33:27,  5.21s/it] 38%|███▊      | 240/624 [20:54<33:01,  5.16s/it]                                                 {'loss': 0.8798, 'grad_norm': 4.339657306671143, 'learning_rate': 1.4106859737314532e-05, 'epoch': 1.15}
 38%|███▊      | 240/624 [20:55<33:01,  5.16s/it] 39%|███▊      | 241/624 [20:59<32:51,  5.15s/it]                                                 {'loss': 0.7038, 'grad_norm': 6.257857799530029, 'learning_rate': 1.4059458606300358e-05, 'epoch': 1.16}
 39%|███▊      | 241/624 [21:00<32:51,  5.15s/it] 39%|███▉      | 242/624 [21:05<33:09,  5.21s/it]                                                 {'loss': 0.7431, 'grad_norm': 5.643643379211426, 'learning_rate': 1.4011948015111334e-05, 'epoch': 1.16}
 39%|███▉      | 242/624 [21:05<33:09,  5.21s/it] 39%|███▉      | 243/624 [21:10<32:37,  5.14s/it]                                                 {'loss': 0.8755, 'grad_norm': 4.5275959968566895, 'learning_rate': 1.396432924483396e-05, 'epoch': 1.17}
 39%|███▉      | 243/624 [21:10<32:37,  5.14s/it] 39%|███▉      | 244/624 [21:15<32:31,  5.13s/it]                                                 {'loss': 0.8275, 'grad_norm': 5.383828639984131, 'learning_rate': 1.3916603579471705e-05, 'epoch': 1.17}
 39%|███▉      | 244/624 [21:15<32:31,  5.13s/it] 39%|███▉      | 245/624 [21:20<32:48,  5.19s/it]                                                 {'loss': 0.6835, 'grad_norm': 5.096551895141602, 'learning_rate': 1.3868772305910376e-05, 'epoch': 1.18}
 39%|███▉      | 245/624 [21:20<32:48,  5.19s/it] 39%|███▉      | 246/624 [21:26<32:49,  5.21s/it]                                                 {'loss': 0.8332, 'grad_norm': 4.7572760581970215, 'learning_rate': 1.3820836713883424e-05, 'epoch': 1.18}
 39%|███▉      | 246/624 [21:26<32:49,  5.21s/it] 40%|███▉      | 247/624 [21:31<33:02,  5.26s/it]                                                 {'loss': 0.8097, 'grad_norm': 4.61899471282959, 'learning_rate': 1.3772798095937172e-05, 'epoch': 1.18}
 40%|███▉      | 247/624 [21:31<33:02,  5.26s/it] 40%|███▉      | 248/624 [21:36<33:03,  5.28s/it]                                                 {'loss': 0.7177, 'grad_norm': 4.952516555786133, 'learning_rate': 1.3724657747395957e-05, 'epoch': 1.19}
 40%|███▉      | 248/624 [21:36<33:03,  5.28s/it] 40%|███▉      | 249/624 [21:41<32:57,  5.27s/it]                                                 {'loss': 0.7906, 'grad_norm': 4.540403366088867, 'learning_rate': 1.3676416966327201e-05, 'epoch': 1.19}
 40%|███▉      | 249/624 [21:42<32:57,  5.27s/it] 40%|████      | 250/624 [21:47<32:24,  5.20s/it]                                                 {'loss': 0.7765, 'grad_norm': 4.454878807067871, 'learning_rate': 1.362807705350641e-05, 'epoch': 1.2}
 40%|████      | 250/624 [21:47<32:24,  5.20s/it] 40%|████      | 251/624 [21:52<32:06,  5.17s/it]                                                 {'loss': 0.7858, 'grad_norm': 4.420101165771484, 'learning_rate': 1.3579639312382105e-05, 'epoch': 1.2}
 40%|████      | 251/624 [21:52<32:06,  5.17s/it] 40%|████      | 252/624 [21:57<32:40,  5.27s/it]                                                 {'loss': 0.654, 'grad_norm': 3.8597443103790283, 'learning_rate': 1.3531105049040667e-05, 'epoch': 1.21}
 40%|████      | 252/624 [21:57<32:40,  5.27s/it] 41%|████      | 253/624 [22:03<32:55,  5.33s/it]                                                 {'loss': 0.675, 'grad_norm': 3.9165093898773193, 'learning_rate': 1.3482475572171132e-05, 'epoch': 1.21}
 41%|████      | 253/624 [22:03<32:55,  5.33s/it] 41%|████      | 254/624 [22:08<32:33,  5.28s/it]                                                 {'loss': 0.7851, 'grad_norm': 4.093301773071289, 'learning_rate': 1.3433752193029888e-05, 'epoch': 1.22}
 41%|████      | 254/624 [22:08<32:33,  5.28s/it] 41%|████      | 255/624 [22:13<32:47,  5.33s/it]                                                 {'loss': 0.6966, 'grad_norm': 3.792074203491211, 'learning_rate': 1.3384936225405326e-05, 'epoch': 1.22}
 41%|████      | 255/624 [22:13<32:47,  5.33s/it] 41%|████      | 256/624 [22:18<32:29,  5.30s/it]                                                 {'loss': 0.7613, 'grad_norm': 4.329451560974121, 'learning_rate': 1.333602898558242e-05, 'epoch': 1.23}
 41%|████      | 256/624 [22:19<32:29,  5.30s/it] 41%|████      | 257/624 [22:24<32:12,  5.26s/it]                                                 {'loss': 0.8188, 'grad_norm': 3.836578845977783, 'learning_rate': 1.3287031792307226e-05, 'epoch': 1.23}
 41%|████      | 257/624 [22:24<32:12,  5.26s/it] 41%|████▏     | 258/624 [22:29<32:10,  5.28s/it]                                                 {'loss': 0.7735, 'grad_norm': 4.863796710968018, 'learning_rate': 1.323794596675132e-05, 'epoch': 1.24}
 41%|████▏     | 258/624 [22:29<32:10,  5.28s/it] 42%|████▏     | 259/624 [22:34<31:39,  5.20s/it]                                                 {'loss': 0.731, 'grad_norm': 3.9312334060668945, 'learning_rate': 1.318877283247619e-05, 'epoch': 1.24}
 42%|████▏     | 259/624 [22:34<31:39,  5.20s/it] 42%|████▏     | 260/624 [22:39<31:26,  5.18s/it]                                                 {'loss': 0.8446, 'grad_norm': 4.80776834487915, 'learning_rate': 1.3139513715397521e-05, 'epoch': 1.25}
 42%|████▏     | 260/624 [22:39<31:26,  5.18s/it] 42%|████▏     | 261/624 [22:44<31:32,  5.21s/it]                                                 {'loss': 0.7734, 'grad_norm': 4.122104644775391, 'learning_rate': 1.3090169943749475e-05, 'epoch': 1.25}
 42%|████▏     | 261/624 [22:44<31:32,  5.21s/it] 42%|████▏     | 262/624 [22:50<31:31,  5.23s/it]                                                 {'loss': 0.6588, 'grad_norm': 4.103936672210693, 'learning_rate': 1.304074284804885e-05, 'epoch': 1.26}
 42%|████▏     | 262/624 [22:50<31:31,  5.23s/it] 42%|████▏     | 263/624 [22:54<30:48,  5.12s/it]                                                 {'loss': 0.745, 'grad_norm': 4.851609706878662, 'learning_rate': 1.2991233761059214e-05, 'epoch': 1.26}
 42%|████▏     | 263/624 [22:55<30:48,  5.12s/it] 42%|████▏     | 264/624 [23:00<30:46,  5.13s/it]                                                 {'loss': 0.8164, 'grad_norm': 4.718830108642578, 'learning_rate': 1.2941644017754964e-05, 'epoch': 1.27}
 42%|████▏     | 264/624 [23:00<30:46,  5.13s/it] 42%|████▏     | 265/624 [23:05<30:46,  5.14s/it]                                                 {'loss': 0.7752, 'grad_norm': 4.261467933654785, 'learning_rate': 1.289197495528534e-05, 'epoch': 1.27}
 42%|████▏     | 265/624 [23:05<30:46,  5.14s/it] 43%|████▎     | 266/624 [23:10<31:02,  5.20s/it]                                                 {'loss': 0.7328, 'grad_norm': 3.861964464187622, 'learning_rate': 1.284222791293836e-05, 'epoch': 1.28}
 43%|████▎     | 266/624 [23:10<31:02,  5.20s/it] 43%|████▎     | 267/624 [23:15<31:06,  5.23s/it]                                                 {'loss': 0.7257, 'grad_norm': 3.956319808959961, 'learning_rate': 1.2792404232104699e-05, 'epoch': 1.28}
 43%|████▎     | 267/624 [23:16<31:06,  5.23s/it] 43%|████▎     | 268/624 [23:20<30:23,  5.12s/it]                                                 {'loss': 0.6941, 'grad_norm': 4.065239429473877, 'learning_rate': 1.2742505256241543e-05, 'epoch': 1.29}
 43%|████▎     | 268/624 [23:20<30:23,  5.12s/it] 43%|████▎     | 269/624 [23:25<30:21,  5.13s/it]                                                 {'loss': 0.7887, 'grad_norm': 4.102564334869385, 'learning_rate': 1.2692532330836346e-05, 'epoch': 1.29}
 43%|████▎     | 269/624 [23:26<30:21,  5.13s/it] 43%|████▎     | 270/624 [23:31<30:27,  5.16s/it]                                                 {'loss': 0.6526, 'grad_norm': 4.674837589263916, 'learning_rate': 1.2642486803370553e-05, 'epoch': 1.29}
 43%|████▎     | 270/624 [23:31<30:27,  5.16s/it] 43%|████▎     | 271/624 [23:36<29:46,  5.06s/it]                                                 {'loss': 0.7636, 'grad_norm': 4.811031818389893, 'learning_rate': 1.2592370023283268e-05, 'epoch': 1.3}
 43%|████▎     | 271/624 [23:36<29:46,  5.06s/it] 44%|████▎     | 272/624 [23:41<30:12,  5.15s/it]                                                 {'loss': 0.753, 'grad_norm': 4.938965797424316, 'learning_rate': 1.2542183341934873e-05, 'epoch': 1.3}
 44%|████▎     | 272/624 [23:41<30:12,  5.15s/it] 44%|████▍     | 273/624 [23:47<31:06,  5.32s/it]                                                 {'loss': 0.8198, 'grad_norm': 5.654524803161621, 'learning_rate': 1.2491928112570568e-05, 'epoch': 1.31}
 44%|████▍     | 273/624 [23:47<31:06,  5.32s/it] 44%|████▍     | 274/624 [23:52<31:03,  5.32s/it]                                                 {'loss': 0.8179, 'grad_norm': 4.004766464233398, 'learning_rate': 1.2441605690283915e-05, 'epoch': 1.31}
 44%|████▍     | 274/624 [23:52<31:03,  5.32s/it] 44%|████▍     | 275/624 [23:57<31:17,  5.38s/it]                                                 {'loss': 0.7266, 'grad_norm': 4.1044416427612305, 'learning_rate': 1.2391217431980273e-05, 'epoch': 1.32}
 44%|████▍     | 275/624 [23:58<31:17,  5.38s/it] 44%|████▍     | 276/624 [24:03<31:09,  5.37s/it]                                                 {'loss': 0.7133, 'grad_norm': 4.3492913246154785, 'learning_rate': 1.234076469634022e-05, 'epoch': 1.32}
 44%|████▍     | 276/624 [24:03<31:09,  5.37s/it] 44%|████▍     | 277/624 [24:08<30:54,  5.35s/it]                                                 {'loss': 0.8424, 'grad_norm': 4.419618129730225, 'learning_rate': 1.2290248843782915e-05, 'epoch': 1.33}
 44%|████▍     | 277/624 [24:08<30:54,  5.35s/it] 45%|████▍     | 278/624 [24:13<30:38,  5.31s/it]                                                 {'loss': 0.6506, 'grad_norm': 3.730379819869995, 'learning_rate': 1.2239671236429413e-05, 'epoch': 1.33}
 45%|████▍     | 278/624 [24:13<30:38,  5.31s/it] 45%|████▍     | 279/624 [24:19<30:26,  5.29s/it]                                                 {'loss': 0.6721, 'grad_norm': 4.553300380706787, 'learning_rate': 1.218903323806595e-05, 'epoch': 1.34}
 45%|████▍     | 279/624 [24:19<30:26,  5.29s/it] 45%|████▍     | 280/624 [24:24<30:12,  5.27s/it]                                                 {'loss': 0.8097, 'grad_norm': 3.834524154663086, 'learning_rate': 1.2138336214107148e-05, 'epoch': 1.34}
 45%|████▍     | 280/624 [24:24<30:12,  5.27s/it] 45%|████▌     | 281/624 [24:29<29:58,  5.24s/it]                                                 {'loss': 0.7931, 'grad_norm': 7.982441425323486, 'learning_rate': 1.2087581531559208e-05, 'epoch': 1.35}
 45%|████▌     | 281/624 [24:29<29:58,  5.24s/it] 45%|████▌     | 282/624 [24:34<30:03,  5.27s/it]                                                 {'loss': 0.8608, 'grad_norm': 4.0282487869262695, 'learning_rate': 1.2036770558983067e-05, 'epoch': 1.35}
 45%|████▌     | 282/624 [24:34<30:03,  5.27s/it] 45%|████▌     | 283/624 [24:40<30:03,  5.29s/it]                                                 {'loss': 0.8134, 'grad_norm': 4.614171504974365, 'learning_rate': 1.1985904666457455e-05, 'epoch': 1.36}
 45%|████▌     | 283/624 [24:40<30:03,  5.29s/it] 46%|████▌     | 284/624 [24:45<30:07,  5.32s/it]                                                 {'loss': 0.7436, 'grad_norm': 4.3589558601379395, 'learning_rate': 1.1934985225541998e-05, 'epoch': 1.36}
 46%|████▌     | 284/624 [24:45<30:07,  5.32s/it] 46%|████▌     | 285/624 [24:50<29:43,  5.26s/it]                                                 {'loss': 0.8174, 'grad_norm': 4.344178199768066, 'learning_rate': 1.18840136092402e-05, 'epoch': 1.37}
 46%|████▌     | 285/624 [24:50<29:43,  5.26s/it] 46%|████▌     | 286/624 [24:55<28:58,  5.14s/it]                                                 {'loss': 0.7703, 'grad_norm': 5.000113010406494, 'learning_rate': 1.1832991191962435e-05, 'epoch': 1.37}
 46%|████▌     | 286/624 [24:55<28:58,  5.14s/it] 46%|████▌     | 287/624 [25:00<29:14,  5.21s/it]                                                 {'loss': 0.7985, 'grad_norm': 4.547756195068359, 'learning_rate': 1.1781919349488894e-05, 'epoch': 1.38}
 46%|████▌     | 287/624 [25:00<29:14,  5.21s/it] 46%|████▌     | 288/624 [25:06<29:12,  5.22s/it]                                                 {'loss': 0.8402, 'grad_norm': 4.450568199157715, 'learning_rate': 1.1730799458932473e-05, 'epoch': 1.38}
 46%|████▌     | 288/624 [25:06<29:12,  5.22s/it] 46%|████▋     | 289/624 [25:11<29:13,  5.23s/it]                                                 {'loss': 0.8495, 'grad_norm': 4.748659133911133, 'learning_rate': 1.1679632898701649e-05, 'epoch': 1.39}
 46%|████▋     | 289/624 [25:11<29:13,  5.23s/it] 46%|████▋     | 290/624 [25:16<28:32,  5.13s/it]                                                 {'loss': 0.7867, 'grad_norm': 6.081901550292969, 'learning_rate': 1.1628421048463315e-05, 'epoch': 1.39}
 46%|████▋     | 290/624 [25:16<28:32,  5.13s/it] 47%|████▋     | 291/624 [25:21<28:50,  5.20s/it]                                                 {'loss': 0.8968, 'grad_norm': 4.729293346405029, 'learning_rate': 1.1577165289105565e-05, 'epoch': 1.4}
 47%|████▋     | 291/624 [25:21<28:50,  5.20s/it] 47%|████▋     | 292/624 [25:26<28:37,  5.17s/it]                                                 {'loss': 0.8488, 'grad_norm': 5.151818752288818, 'learning_rate': 1.1525867002700484e-05, 'epoch': 1.4}
 47%|████▋     | 292/624 [25:26<28:37,  5.17s/it] 47%|████▋     | 293/624 [25:31<28:21,  5.14s/it]                                                 {'loss': 0.8619, 'grad_norm': 4.515681266784668, 'learning_rate': 1.1474527572466847e-05, 'epoch': 1.41}
 47%|████▋     | 293/624 [25:31<28:21,  5.14s/it] 47%|████▋     | 294/624 [25:37<28:28,  5.18s/it]                                                 {'loss': 0.8459, 'grad_norm': 3.9259777069091797, 'learning_rate': 1.1423148382732854e-05, 'epoch': 1.41}
 47%|████▋     | 294/624 [25:37<28:28,  5.18s/it] 47%|████▋     | 295/624 [25:42<28:29,  5.20s/it]                                                 {'loss': 0.8007, 'grad_norm': 3.8770055770874023, 'learning_rate': 1.1371730818898785e-05, 'epoch': 1.41}
 47%|████▋     | 295/624 [25:42<28:29,  5.20s/it] 47%|████▋     | 296/624 [25:47<28:41,  5.25s/it]                                                 {'loss': 0.5994, 'grad_norm': 4.1813178062438965, 'learning_rate': 1.132027626739965e-05, 'epoch': 1.42}
 47%|████▋     | 296/624 [25:47<28:41,  5.25s/it] 48%|████▊     | 297/624 [25:52<28:43,  5.27s/it]                                                 {'loss': 0.6991, 'grad_norm': 3.8787765502929688, 'learning_rate': 1.1268786115667798e-05, 'epoch': 1.42}
 48%|████▊     | 297/624 [25:53<28:43,  5.27s/it] 48%|████▊     | 298/624 [25:58<28:32,  5.25s/it]                                                 {'loss': 0.804, 'grad_norm': 4.577179908752441, 'learning_rate': 1.1217261752095518e-05, 'epoch': 1.43}
 48%|████▊     | 298/624 [25:58<28:32,  5.25s/it] 48%|████▊     | 299/624 [26:03<28:44,  5.31s/it]                                                 {'loss': 0.9115, 'grad_norm': 3.8479628562927246, 'learning_rate': 1.1165704565997593e-05, 'epoch': 1.43}
 48%|████▊     | 299/624 [26:03<28:44,  5.31s/it] 48%|████▊     | 300/624 [26:08<28:27,  5.27s/it]                                                 {'loss': 0.7429, 'grad_norm': 4.089977264404297, 'learning_rate': 1.1114115947573834e-05, 'epoch': 1.44}
 48%|████▊     | 300/624 [26:08<28:27,  5.27s/it] 48%|████▊     | 301/624 [26:13<28:02,  5.21s/it]                                                 {'loss': 0.7719, 'grad_norm': 5.7712602615356445, 'learning_rate': 1.1062497287871606e-05, 'epoch': 1.44}
 48%|████▊     | 301/624 [26:14<28:02,  5.21s/it] 48%|████▊     | 302/624 [26:19<28:06,  5.24s/it]                                                 {'loss': 0.8102, 'grad_norm': 5.046672821044922, 'learning_rate': 1.1010849978748314e-05, 'epoch': 1.45}
 48%|████▊     | 302/624 [26:19<28:06,  5.24s/it] 49%|████▊     | 303/624 [26:24<28:17,  5.29s/it]                                                 {'loss': 0.658, 'grad_norm': 5.928344249725342, 'learning_rate': 1.0959175412833869e-05, 'epoch': 1.45}
 49%|████▊     | 303/624 [26:24<28:17,  5.29s/it] 49%|████▊     | 304/624 [26:30<28:39,  5.37s/it]                                                 {'loss': 0.941, 'grad_norm': 3.9897520542144775, 'learning_rate': 1.0907474983493144e-05, 'epoch': 1.46}
 49%|████▊     | 304/624 [26:30<28:39,  5.37s/it] 49%|████▉     | 305/624 [26:35<28:16,  5.32s/it]                                                 {'loss': 0.855, 'grad_norm': 5.666067600250244, 'learning_rate': 1.08557500847884e-05, 'epoch': 1.46}
 49%|████▉     | 305/624 [26:35<28:16,  5.32s/it] 49%|████▉     | 306/624 [26:40<27:46,  5.24s/it]                                                 {'loss': 0.866, 'grad_norm': 3.754795551300049, 'learning_rate': 1.080400211144169e-05, 'epoch': 1.47}
 49%|████▉     | 306/624 [26:40<27:46,  5.24s/it] 49%|████▉     | 307/624 [26:45<27:47,  5.26s/it]                                                 {'loss': 0.6352, 'grad_norm': 4.410224914550781, 'learning_rate': 1.0752232458797262e-05, 'epoch': 1.47}
 49%|████▉     | 307/624 [26:45<27:47,  5.26s/it] 49%|████▉     | 308/624 [26:51<27:48,  5.28s/it]                                                 {'loss': 0.6512, 'grad_norm': 3.5687246322631836, 'learning_rate': 1.070044252278393e-05, 'epoch': 1.48}
 49%|████▉     | 308/624 [26:51<27:48,  5.28s/it] 50%|████▉     | 309/624 [26:56<27:57,  5.33s/it]                                                 {'loss': 0.6597, 'grad_norm': 4.521167278289795, 'learning_rate': 1.064863369987743e-05, 'epoch': 1.48}
 50%|████▉     | 309/624 [26:56<27:57,  5.33s/it] 50%|████▉     | 310/624 [27:01<27:33,  5.27s/it]                                                 {'loss': 0.8346, 'grad_norm': 4.287166118621826, 'learning_rate': 1.0596807387062772e-05, 'epoch': 1.49}
 50%|████▉     | 310/624 [27:01<27:33,  5.27s/it] 50%|████▉     | 311/624 [27:06<27:09,  5.21s/it]                                                 {'loss': 0.8474, 'grad_norm': 3.6946821212768555, 'learning_rate': 1.0544964981796563e-05, 'epoch': 1.49}
 50%|████▉     | 311/624 [27:06<27:09,  5.21s/it] 50%|█████     | 312/624 [27:11<26:55,  5.18s/it]                                                 {'loss': 0.7785, 'grad_norm': 4.81505823135376, 'learning_rate': 1.0493107881969335e-05, 'epoch': 1.5}
 50%|█████     | 312/624 [27:11<26:55,  5.18s/it] 50%|█████     | 313/624 [27:16<26:27,  5.10s/it]                                                 {'loss': 0.8916, 'grad_norm': 5.082843780517578, 'learning_rate': 1.0441237485867845e-05, 'epoch': 1.5}
 50%|█████     | 313/624 [27:16<26:27,  5.10s/it] 50%|█████     | 314/624 [27:21<26:38,  5.16s/it]                                                 {'loss': 0.8247, 'grad_norm': 4.87538480758667, 'learning_rate': 1.0389355192137379e-05, 'epoch': 1.51}
 50%|█████     | 314/624 [27:22<26:38,  5.16s/it] 50%|█████     | 315/624 [27:27<26:21,  5.12s/it]                                                 {'loss': 0.7945, 'grad_norm': 4.654999256134033, 'learning_rate': 1.0337462399744025e-05, 'epoch': 1.51}
 50%|█████     | 315/624 [27:27<26:21,  5.12s/it] 51%|█████     | 316/624 [27:32<26:55,  5.24s/it]                                                 {'loss': 0.8079, 'grad_norm': 4.024745941162109, 'learning_rate': 1.0285560507936962e-05, 'epoch': 1.52}
 51%|█████     | 316/624 [27:32<26:55,  5.24s/it] 51%|█████     | 317/624 [27:38<27:09,  5.31s/it]                                                 {'loss': 0.7926, 'grad_norm': 4.553797721862793, 'learning_rate': 1.0233650916210736e-05, 'epoch': 1.52}
 51%|█████     | 317/624 [27:38<27:09,  5.31s/it] 51%|█████     | 318/624 [27:43<27:18,  5.35s/it]                                                 {'loss': 0.7132, 'grad_norm': 4.062600135803223, 'learning_rate': 1.0181735024267504e-05, 'epoch': 1.53}
 51%|█████     | 318/624 [27:43<27:18,  5.35s/it] 51%|█████     | 319/624 [27:48<27:23,  5.39s/it]                                                 {'loss': 0.7665, 'grad_norm': 7.29255485534668, 'learning_rate': 1.012981423197931e-05, 'epoch': 1.53}
 51%|█████     | 319/624 [27:49<27:23,  5.39s/it] 51%|█████▏    | 320/624 [27:54<27:11,  5.37s/it]                                                 {'loss': 0.8345, 'grad_norm': 4.6014604568481445, 'learning_rate': 1.007788993935033e-05, 'epoch': 1.53}
 51%|█████▏    | 320/624 [27:54<27:11,  5.37s/it] 51%|█████▏    | 321/624 [27:59<26:40,  5.28s/it]                                                 {'loss': 0.7071, 'grad_norm': 4.134754657745361, 'learning_rate': 1.002596354647912e-05, 'epoch': 1.54}
 51%|█████▏    | 321/624 [27:59<26:40,  5.28s/it] 52%|█████▏    | 322/624 [28:04<26:36,  5.29s/it]                                                 {'loss': 0.9088, 'grad_norm': 4.555369853973389, 'learning_rate': 9.974036453520881e-06, 'epoch': 1.54}
 52%|█████▏    | 322/624 [28:04<26:36,  5.29s/it] 52%|█████▏    | 323/624 [28:09<26:20,  5.25s/it]                                                 {'loss': 0.7456, 'grad_norm': 3.6854729652404785, 'learning_rate': 9.922110060649672e-06, 'epoch': 1.55}
 52%|█████▏    | 323/624 [28:09<26:20,  5.25s/it] 52%|█████▏    | 324/624 [28:14<25:40,  5.14s/it]                                                 {'loss': 0.6547, 'grad_norm': 4.170174598693848, 'learning_rate': 9.870185768020694e-06, 'epoch': 1.55}
 52%|█████▏    | 324/624 [28:14<25:40,  5.14s/it] 52%|█████▏    | 325/624 [28:20<25:56,  5.21s/it]                                                 {'loss': 0.7835, 'grad_norm': 3.807931900024414, 'learning_rate': 9.818264975732497e-06, 'epoch': 1.56}
 52%|█████▏    | 325/624 [28:20<25:56,  5.21s/it] 52%|█████▏    | 326/624 [28:25<25:56,  5.22s/it]                                                 {'loss': 0.7505, 'grad_norm': 3.961000442504883, 'learning_rate': 9.766349083789266e-06, 'epoch': 1.56}
 52%|█████▏    | 326/624 [28:25<25:56,  5.22s/it] 52%|█████▏    | 327/624 [28:30<26:16,  5.31s/it]                                                 {'loss': 0.9333, 'grad_norm': 4.000515937805176, 'learning_rate': 9.71443949206304e-06, 'epoch': 1.57}
 52%|█████▏    | 327/624 [28:30<26:16,  5.31s/it] 53%|█████▎    | 328/624 [28:36<26:55,  5.46s/it]                                                 {'loss': 0.6463, 'grad_norm': 4.26472806930542, 'learning_rate': 9.662537600255979e-06, 'epoch': 1.57}
 53%|█████▎    | 328/624 [28:36<26:55,  5.46s/it] 53%|█████▎    | 329/624 [28:41<26:31,  5.39s/it]                                                 {'loss': 0.7797, 'grad_norm': 4.197366714477539, 'learning_rate': 9.610644807862625e-06, 'epoch': 1.58}
 53%|█████▎    | 329/624 [28:41<26:31,  5.39s/it] 53%|█████▎    | 330/624 [28:47<26:32,  5.42s/it]                                                 {'loss': 0.9656, 'grad_norm': 3.899085283279419, 'learning_rate': 9.558762514132157e-06, 'epoch': 1.58}
 53%|█████▎    | 330/624 [28:47<26:32,  5.42s/it] 53%|█████▎    | 331/624 [28:52<25:41,  5.26s/it]                                                 {'loss': 0.8301, 'grad_norm': 4.134346961975098, 'learning_rate': 9.506892118030668e-06, 'epoch': 1.59}
 53%|█████▎    | 331/624 [28:52<25:41,  5.26s/it] 53%|█████▎    | 332/624 [28:57<25:07,  5.16s/it]                                                 {'loss': 0.7814, 'grad_norm': 4.3161702156066895, 'learning_rate': 9.455035018203439e-06, 'epoch': 1.59}
 53%|█████▎    | 332/624 [28:57<25:07,  5.16s/it] 53%|█████▎    | 333/624 [29:02<24:50,  5.12s/it]                                                 {'loss': 0.7568, 'grad_norm': 5.291236400604248, 'learning_rate': 9.40319261293723e-06, 'epoch': 1.6}
 53%|█████▎    | 333/624 [29:02<24:50,  5.12s/it] 54%|█████▎    | 334/624 [29:07<24:56,  5.16s/it]                                                 {'loss': 0.872, 'grad_norm': 5.737274169921875, 'learning_rate': 9.351366300122569e-06, 'epoch': 1.6}
 54%|█████▎    | 334/624 [29:07<24:56,  5.16s/it] 54%|█████▎    | 335/624 [29:12<25:21,  5.27s/it]                                                 {'loss': 0.8242, 'grad_norm': 4.787088871002197, 'learning_rate': 9.299557477216073e-06, 'epoch': 1.61}
 54%|█████▎    | 335/624 [29:13<25:21,  5.27s/it] 54%|█████▍    | 336/624 [29:18<25:28,  5.31s/it]                                                 {'loss': 0.8592, 'grad_norm': 3.824977397918701, 'learning_rate': 9.247767541202738e-06, 'epoch': 1.61}
 54%|█████▍    | 336/624 [29:18<25:28,  5.31s/it] 54%|█████▍    | 337/624 [29:23<25:35,  5.35s/it]                                                 {'loss': 0.8047, 'grad_norm': 3.966214418411255, 'learning_rate': 9.195997888558312e-06, 'epoch': 1.62}
 54%|█████▍    | 337/624 [29:23<25:35,  5.35s/it] 54%|█████▍    | 338/624 [29:29<25:16,  5.30s/it]                                                 {'loss': 1.0199, 'grad_norm': 4.28294038772583, 'learning_rate': 9.144249915211605e-06, 'epoch': 1.62}
 54%|█████▍    | 338/624 [29:29<25:16,  5.30s/it] 54%|█████▍    | 339/624 [29:34<24:54,  5.25s/it]                                                 {'loss': 0.6553, 'grad_norm': 4.85882568359375, 'learning_rate': 9.092525016506858e-06, 'epoch': 1.63}
 54%|█████▍    | 339/624 [29:34<24:54,  5.25s/it] 54%|█████▍    | 340/624 [29:39<25:11,  5.32s/it]                                                 {'loss': 0.8197, 'grad_norm': 4.163172245025635, 'learning_rate': 9.040824587166136e-06, 'epoch': 1.63}
 54%|█████▍    | 340/624 [29:39<25:11,  5.32s/it] 55%|█████▍    | 341/624 [29:44<24:36,  5.22s/it]                                                 {'loss': 0.607, 'grad_norm': 4.350082874298096, 'learning_rate': 8.98915002125169e-06, 'epoch': 1.64}
 55%|█████▍    | 341/624 [29:44<24:36,  5.22s/it] 55%|█████▍    | 342/624 [29:49<24:34,  5.23s/it]                                                 {'loss': 0.7583, 'grad_norm': 4.184996128082275, 'learning_rate': 8.9375027121284e-06, 'epoch': 1.64}
 55%|█████▍    | 342/624 [29:49<24:34,  5.23s/it] 55%|█████▍    | 343/624 [29:55<24:37,  5.26s/it]                                                 {'loss': 0.7471, 'grad_norm': 4.638709545135498, 'learning_rate': 8.885884052426168e-06, 'epoch': 1.65}
 55%|█████▍    | 343/624 [29:55<24:37,  5.26s/it] 55%|█████▌    | 344/624 [30:00<24:44,  5.30s/it]                                                 {'loss': 0.7363, 'grad_norm': 3.9528121948242188, 'learning_rate': 8.83429543400241e-06, 'epoch': 1.65}
 55%|█████▌    | 344/624 [30:00<24:44,  5.30s/it] 55%|█████▌    | 345/624 [30:05<24:30,  5.27s/it]                                                 {'loss': 0.7056, 'grad_norm': 4.477166175842285, 'learning_rate': 8.78273824790448e-06, 'epoch': 1.65}
 55%|█████▌    | 345/624 [30:05<24:30,  5.27s/it] 55%|█████▌    | 346/624 [30:10<23:57,  5.17s/it]                                                 {'loss': 0.8251, 'grad_norm': 3.8685996532440186, 'learning_rate': 8.731213884332205e-06, 'epoch': 1.66}
 55%|█████▌    | 346/624 [30:10<23:57,  5.17s/it] 56%|█████▌    | 347/624 [30:15<23:58,  5.19s/it]                                                 {'loss': 0.9035, 'grad_norm': 4.220182418823242, 'learning_rate': 8.679723732600355e-06, 'epoch': 1.66}
 56%|█████▌    | 347/624 [30:16<23:58,  5.19s/it] 56%|█████▌    | 348/624 [30:21<23:46,  5.17s/it]                                                 {'loss': 0.7993, 'grad_norm': 4.292741775512695, 'learning_rate': 8.628269181101216e-06, 'epoch': 1.67}
 56%|█████▌    | 348/624 [30:21<23:46,  5.17s/it] 56%|█████▌    | 349/624 [30:26<23:49,  5.20s/it]                                                 {'loss': 0.8272, 'grad_norm': 4.155405044555664, 'learning_rate': 8.576851617267151e-06, 'epoch': 1.67}
 56%|█████▌    | 349/624 [30:26<23:49,  5.20s/it] 56%|█████▌    | 350/624 [30:31<23:37,  5.17s/it]                                                 {'loss': 0.7665, 'grad_norm': 3.926255702972412, 'learning_rate': 8.525472427533156e-06, 'epoch': 1.68}
 56%|█████▌    | 350/624 [30:31<23:37,  5.17s/it] 56%|█████▋    | 351/624 [30:36<23:43,  5.21s/it]                                                 {'loss': 0.7447, 'grad_norm': 4.453605651855469, 'learning_rate': 8.474132997299521e-06, 'epoch': 1.68}
 56%|█████▋    | 351/624 [30:36<23:43,  5.21s/it] 56%|█████▋    | 352/624 [30:42<23:45,  5.24s/it]                                                 {'loss': 0.6944, 'grad_norm': 5.1922526359558105, 'learning_rate': 8.422834710894434e-06, 'epoch': 1.69}
 56%|█████▋    | 352/624 [30:42<23:45,  5.24s/it] 57%|█████▋    | 353/624 [30:47<23:41,  5.25s/it]                                                 {'loss': 0.777, 'grad_norm': 4.54141902923584, 'learning_rate': 8.371578951536689e-06, 'epoch': 1.69}
 57%|█████▋    | 353/624 [30:47<23:41,  5.25s/it] 57%|█████▋    | 354/624 [30:52<23:50,  5.30s/it]                                                 {'loss': 0.779, 'grad_norm': 4.832236289978027, 'learning_rate': 8.320367101298351e-06, 'epoch': 1.7}
 57%|█████▋    | 354/624 [30:52<23:50,  5.30s/it] 57%|█████▋    | 355/624 [30:58<23:52,  5.33s/it]                                                 {'loss': 0.691, 'grad_norm': 4.361854553222656, 'learning_rate': 8.26920054106753e-06, 'epoch': 1.7}
 57%|█████▋    | 355/624 [30:58<23:52,  5.33s/it] 57%|█████▋    | 356/624 [31:03<23:44,  5.32s/it]                                                 {'loss': 0.8099, 'grad_norm': 5.652308464050293, 'learning_rate': 8.218080650511107e-06, 'epoch': 1.71}
 57%|█████▋    | 356/624 [31:03<23:44,  5.32s/it] 57%|█████▋    | 357/624 [31:08<22:53,  5.14s/it]                                                 {'loss': 0.7106, 'grad_norm': 4.867166519165039, 'learning_rate': 8.167008808037568e-06, 'epoch': 1.71}
 57%|█████▋    | 357/624 [31:08<22:53,  5.14s/it] 57%|█████▋    | 358/624 [31:13<22:42,  5.12s/it]                                                 {'loss': 0.7961, 'grad_norm': 5.266289710998535, 'learning_rate': 8.115986390759805e-06, 'epoch': 1.72}
 57%|█████▋    | 358/624 [31:13<22:42,  5.12s/it] 58%|█████▊    | 359/624 [31:18<22:21,  5.06s/it]                                                 {'loss': 0.8472, 'grad_norm': 4.262417793273926, 'learning_rate': 8.065014774458004e-06, 'epoch': 1.72}
 58%|█████▊    | 359/624 [31:18<22:21,  5.06s/it] 58%|█████▊    | 360/624 [31:23<22:43,  5.16s/it]                                                 {'loss': 0.7334, 'grad_norm': 4.280773162841797, 'learning_rate': 8.014095333542548e-06, 'epoch': 1.73}
 58%|█████▊    | 360/624 [31:23<22:43,  5.16s/it] 58%|█████▊    | 361/624 [31:28<22:53,  5.22s/it]                                                 {'loss': 0.851, 'grad_norm': 3.8337080478668213, 'learning_rate': 7.963229441016938e-06, 'epoch': 1.73}
 58%|█████▊    | 361/624 [31:29<22:53,  5.22s/it] 58%|█████▊    | 362/624 [31:34<22:57,  5.26s/it]                                                 {'loss': 0.6882, 'grad_norm': 4.486703395843506, 'learning_rate': 7.912418468440794e-06, 'epoch': 1.74}
 58%|█████▊    | 362/624 [31:34<22:57,  5.26s/it] 58%|█████▊    | 363/624 [31:39<22:51,  5.25s/it]                                                 {'loss': 0.7446, 'grad_norm': 3.9788994789123535, 'learning_rate': 7.861663785892857e-06, 'epoch': 1.74}
 58%|█████▊    | 363/624 [31:39<22:51,  5.25s/it] 58%|█████▊    | 364/624 [31:44<22:42,  5.24s/it]                                                 {'loss': 0.7058, 'grad_norm': 3.987607717514038, 'learning_rate': 7.810966761934053e-06, 'epoch': 1.75}
 58%|█████▊    | 364/624 [31:44<22:42,  5.24s/it] 58%|█████▊    | 365/624 [31:49<22:33,  5.23s/it]                                                 {'loss': 0.7056, 'grad_norm': 4.1585774421691895, 'learning_rate': 7.760328763570589e-06, 'epoch': 1.75}
 58%|█████▊    | 365/624 [31:50<22:33,  5.23s/it] 59%|█████▊    | 366/624 [31:54<21:57,  5.11s/it]                                                 {'loss': 0.7526, 'grad_norm': 4.319015026092529, 'learning_rate': 7.709751156217088e-06, 'epoch': 1.76}
 59%|█████▊    | 366/624 [31:54<21:57,  5.11s/it] 59%|█████▉    | 367/624 [32:00<22:13,  5.19s/it]                                                 {'loss': 0.7799, 'grad_norm': 3.9609365463256836, 'learning_rate': 7.659235303659784e-06, 'epoch': 1.76}
 59%|█████▉    | 367/624 [32:00<22:13,  5.19s/it] 59%|█████▉    | 368/624 [32:05<22:06,  5.18s/it]                                                 {'loss': 0.7077, 'grad_norm': 4.348390579223633, 'learning_rate': 7.608782568019729e-06, 'epoch': 1.76}
 59%|█████▉    | 368/624 [32:05<22:06,  5.18s/it] 59%|█████▉    | 369/624 [32:09<21:04,  4.96s/it]                                                 {'loss': 0.6932, 'grad_norm': 3.6705398559570312, 'learning_rate': 7.558394309716088e-06, 'epoch': 1.77}
 59%|█████▉    | 369/624 [32:09<21:04,  4.96s/it] 59%|█████▉    | 370/624 [32:14<21:09,  5.00s/it]                                                 {'loss': 0.727, 'grad_norm': 3.9215056896209717, 'learning_rate': 7.508071887429433e-06, 'epoch': 1.77}
 59%|█████▉    | 370/624 [32:14<21:09,  5.00s/it] 59%|█████▉    | 371/624 [32:19<21:10,  5.02s/it]                                                 {'loss': 0.8069, 'grad_norm': 3.8359947204589844, 'learning_rate': 7.4578166580651335e-06, 'epoch': 1.78}
 59%|█████▉    | 371/624 [32:20<21:10,  5.02s/it] 60%|█████▉    | 372/624 [32:25<21:23,  5.09s/it]                                                 {'loss': 0.7648, 'grad_norm': 4.533891201019287, 'learning_rate': 7.4076299767167325e-06, 'epoch': 1.78}
 60%|█████▉    | 372/624 [32:25<21:23,  5.09s/it] 60%|█████▉    | 373/624 [32:30<21:30,  5.14s/it]                                                 {'loss': 0.7328, 'grad_norm': 3.948478937149048, 'learning_rate': 7.35751319662945e-06, 'epoch': 1.79}
 60%|█████▉    | 373/624 [32:30<21:30,  5.14s/it] 60%|█████▉    | 374/624 [32:35<21:36,  5.19s/it]                                                 {'loss': 0.9527, 'grad_norm': 4.313838481903076, 'learning_rate': 7.307467669163655e-06, 'epoch': 1.79}
 60%|█████▉    | 374/624 [32:35<21:36,  5.19s/it] 60%|██████    | 375/624 [32:40<21:26,  5.17s/it]                                                 {'loss': 0.6461, 'grad_norm': 4.592446804046631, 'learning_rate': 7.25749474375846e-06, 'epoch': 1.8}
 60%|██████    | 375/624 [32:40<21:26,  5.17s/it] 60%|██████    | 376/624 [32:46<21:27,  5.19s/it]                                                 {'loss': 0.8352, 'grad_norm': 3.4584360122680664, 'learning_rate': 7.207595767895303e-06, 'epoch': 1.8}
 60%|██████    | 376/624 [32:46<21:27,  5.19s/it] 60%|██████    | 377/624 [32:51<21:22,  5.19s/it]                                                 {'loss': 0.7015, 'grad_norm': 3.989161968231201, 'learning_rate': 7.157772087061645e-06, 'epoch': 1.81}
 60%|██████    | 377/624 [32:51<21:22,  5.19s/it] 61%|██████    | 378/624 [32:56<21:19,  5.20s/it]                                                 {'loss': 0.6851, 'grad_norm': 4.154604911804199, 'learning_rate': 7.108025044714661e-06, 'epoch': 1.81}
 61%|██████    | 378/624 [32:56<21:19,  5.20s/it] 61%|██████    | 379/624 [33:01<21:11,  5.19s/it]                                                 {'loss': 0.8428, 'grad_norm': 5.1123881340026855, 'learning_rate': 7.058355982245038e-06, 'epoch': 1.82}
 61%|██████    | 379/624 [33:01<21:11,  5.19s/it] 61%|██████    | 380/624 [33:06<21:10,  5.21s/it]                                                 {'loss': 0.7347, 'grad_norm': 4.137828350067139, 'learning_rate': 7.00876623894079e-06, 'epoch': 1.82}
 61%|██████    | 380/624 [33:07<21:10,  5.21s/it] 61%|██████    | 381/624 [33:12<21:09,  5.23s/it]                                                 {'loss': 1.028, 'grad_norm': 4.391171455383301, 'learning_rate': 6.959257151951153e-06, 'epoch': 1.83}
 61%|██████    | 381/624 [33:12<21:09,  5.23s/it] 61%|██████    | 382/624 [33:17<21:36,  5.36s/it]                                                 {'loss': 0.6571, 'grad_norm': 3.857172966003418, 'learning_rate': 6.909830056250527e-06, 'epoch': 1.83}
 61%|██████    | 382/624 [33:17<21:36,  5.36s/it] 61%|██████▏   | 383/624 [33:23<21:39,  5.39s/it]                                                 {'loss': 0.959, 'grad_norm': 4.214020729064941, 'learning_rate': 6.860486284602479e-06, 'epoch': 1.84}
 61%|██████▏   | 383/624 [33:23<21:39,  5.39s/it] 62%|██████▏   | 384/624 [33:28<21:18,  5.33s/it]                                                 {'loss': 0.8128, 'grad_norm': 4.212637424468994, 'learning_rate': 6.8112271675238154e-06, 'epoch': 1.84}
 62%|██████▏   | 384/624 [33:28<21:18,  5.33s/it] 62%|██████▏   | 385/624 [33:33<20:50,  5.23s/it]                                                 {'loss': 0.918, 'grad_norm': 4.3294291496276855, 'learning_rate': 6.762054033248681e-06, 'epoch': 1.85}
 62%|██████▏   | 385/624 [33:33<20:50,  5.23s/it] 62%|██████▏   | 386/624 [33:38<21:04,  5.31s/it]                                                 {'loss': 0.774, 'grad_norm': 4.200407028198242, 'learning_rate': 6.712968207692778e-06, 'epoch': 1.85}
 62%|██████▏   | 386/624 [33:39<21:04,  5.31s/it] 62%|██████▏   | 387/624 [33:44<20:56,  5.30s/it]                                                 {'loss': 0.7977, 'grad_norm': 4.380258560180664, 'learning_rate': 6.663971014417585e-06, 'epoch': 1.86}
 62%|██████▏   | 387/624 [33:44<20:56,  5.30s/it] 62%|██████▏   | 388/624 [33:49<20:32,  5.22s/it]                                                 {'loss': 0.7611, 'grad_norm': 4.040602207183838, 'learning_rate': 6.615063774594677e-06, 'epoch': 1.86}
 62%|██████▏   | 388/624 [33:49<20:32,  5.22s/it] 62%|██████▏   | 389/624 [33:54<20:40,  5.28s/it]                                                 {'loss': 0.7978, 'grad_norm': 4.337556838989258, 'learning_rate': 6.566247806970119e-06, 'epoch': 1.87}
 62%|██████▏   | 389/624 [33:54<20:40,  5.28s/it] 62%|██████▎   | 390/624 [34:00<20:40,  5.30s/it]                                                 {'loss': 0.7649, 'grad_norm': 4.49411153793335, 'learning_rate': 6.5175244278288705e-06, 'epoch': 1.87}
 62%|██████▎   | 390/624 [34:00<20:40,  5.30s/it] 63%|██████▎   | 391/624 [34:05<20:30,  5.28s/it]                                                 {'loss': 0.8015, 'grad_norm': 4.408924579620361, 'learning_rate': 6.468894950959336e-06, 'epoch': 1.88}
 63%|██████▎   | 391/624 [34:05<20:30,  5.28s/it] 63%|██████▎   | 392/624 [34:10<20:14,  5.23s/it]                                                 {'loss': 0.7488, 'grad_norm': 4.5133538246154785, 'learning_rate': 6.420360687617897e-06, 'epoch': 1.88}
 63%|██████▎   | 392/624 [34:10<20:14,  5.23s/it] 63%|██████▎   | 393/624 [34:15<20:10,  5.24s/it]                                                 {'loss': 0.7781, 'grad_norm': 3.918304920196533, 'learning_rate': 6.3719229464935915e-06, 'epoch': 1.88}
 63%|██████▎   | 393/624 [34:15<20:10,  5.24s/it] 63%|██████▎   | 394/624 [34:20<20:06,  5.24s/it]                                                 {'loss': 0.7265, 'grad_norm': 4.212741374969482, 'learning_rate': 6.323583033672799e-06, 'epoch': 1.89}
 63%|██████▎   | 394/624 [34:21<20:06,  5.24s/it] 63%|██████▎   | 395/624 [34:26<20:00,  5.24s/it]                                                 {'loss': 0.8852, 'grad_norm': 5.1555023193359375, 'learning_rate': 6.275342252604044e-06, 'epoch': 1.89}
 63%|██████▎   | 395/624 [34:26<20:00,  5.24s/it] 63%|██████▎   | 396/624 [34:31<19:48,  5.21s/it]                                                 {'loss': 0.8272, 'grad_norm': 3.7725725173950195, 'learning_rate': 6.22720190406283e-06, 'epoch': 1.9}
 63%|██████▎   | 396/624 [34:31<19:48,  5.21s/it] 64%|██████▎   | 397/624 [34:36<19:49,  5.24s/it]                                                 {'loss': 0.8425, 'grad_norm': 4.414214611053467, 'learning_rate': 6.179163286116581e-06, 'epoch': 1.9}
 64%|██████▎   | 397/624 [34:36<19:49,  5.24s/it] 64%|██████▍   | 398/624 [34:41<19:52,  5.28s/it]                                                 {'loss': 0.7359, 'grad_norm': 3.9183549880981445, 'learning_rate': 6.13122769408963e-06, 'epoch': 1.91}
 64%|██████▍   | 398/624 [34:42<19:52,  5.28s/it] 64%|██████▍   | 399/624 [34:46<19:25,  5.18s/it]                                                 {'loss': 0.7701, 'grad_norm': 5.635980606079102, 'learning_rate': 6.083396420528298e-06, 'epoch': 1.91}
 64%|██████▍   | 399/624 [34:47<19:25,  5.18s/it] 64%|██████▍   | 400/624 [34:52<19:40,  5.27s/it]                                                 {'loss': 0.7427, 'grad_norm': 4.345359802246094, 'learning_rate': 6.0356707551660434e-06, 'epoch': 1.92}
 64%|██████▍   | 400/624 [34:52<19:40,  5.27s/it] 64%|██████▍   | 401/624 [34:57<19:27,  5.23s/it]                                                 {'loss': 0.7859, 'grad_norm': 3.956073045730591, 'learning_rate': 5.988051984888668e-06, 'epoch': 1.92}
 64%|██████▍   | 401/624 [34:57<19:27,  5.23s/it] 64%|██████▍   | 402/624 [35:02<19:26,  5.25s/it]                                                 {'loss': 0.8371, 'grad_norm': 4.223620891571045, 'learning_rate': 5.940541393699646e-06, 'epoch': 1.93}
 64%|██████▍   | 402/624 [35:02<19:26,  5.25s/it] 65%|██████▍   | 403/624 [35:07<19:09,  5.20s/it]                                                 {'loss': 0.7277, 'grad_norm': 4.51362419128418, 'learning_rate': 5.893140262685469e-06, 'epoch': 1.93}
 65%|██████▍   | 403/624 [35:08<19:09,  5.20s/it] 65%|██████▍   | 404/624 [35:12<18:53,  5.15s/it]                                                 {'loss': 0.718, 'grad_norm': 4.957301139831543, 'learning_rate': 5.845849869981137e-06, 'epoch': 1.94}
 65%|██████▍   | 404/624 [35:13<18:53,  5.15s/it] 65%|██████▍   | 405/624 [35:18<18:44,  5.13s/it]                                                 {'loss': 0.867, 'grad_norm': 4.430530071258545, 'learning_rate': 5.7986714907356614e-06, 'epoch': 1.94}
 65%|██████▍   | 405/624 [35:18<18:44,  5.13s/it] 65%|██████▌   | 406/624 [35:23<18:48,  5.18s/it]                                                 {'loss': 0.8569, 'grad_norm': 4.1093950271606445, 'learning_rate': 5.751606397077703e-06, 'epoch': 1.95}
 65%|██████▌   | 406/624 [35:23<18:48,  5.18s/it] 65%|██████▌   | 407/624 [35:28<18:37,  5.15s/it]                                                 {'loss': 0.8026, 'grad_norm': 4.726733207702637, 'learning_rate': 5.704655858081268e-06, 'epoch': 1.95}
 65%|██████▌   | 407/624 [35:28<18:37,  5.15s/it] 65%|██████▌   | 408/624 [35:33<18:37,  5.17s/it]                                                 {'loss': 0.8399, 'grad_norm': 4.54957914352417, 'learning_rate': 5.6578211397314765e-06, 'epoch': 1.96}
 65%|██████▌   | 408/624 [35:33<18:37,  5.17s/it] 66%|██████▌   | 409/624 [35:38<18:19,  5.12s/it]                                                 {'loss': 0.7003, 'grad_norm': 4.842583179473877, 'learning_rate': 5.611103504890444e-06, 'epoch': 1.96}
 66%|██████▌   | 409/624 [35:38<18:19,  5.12s/it] 66%|██████▌   | 410/624 [35:44<18:38,  5.23s/it]                                                 {'loss': 0.8584, 'grad_norm': 4.155696392059326, 'learning_rate': 5.564504213263205e-06, 'epoch': 1.97}
 66%|██████▌   | 410/624 [35:44<18:38,  5.23s/it] 66%|██████▌   | 411/624 [35:49<18:47,  5.29s/it]                                                 {'loss': 0.6779, 'grad_norm': 4.819972038269043, 'learning_rate': 5.5180245213637785e-06, 'epoch': 1.97}
 66%|██████▌   | 411/624 [35:49<18:47,  5.29s/it] 66%|██████▌   | 412/624 [35:54<18:35,  5.26s/it]                                                 {'loss': 0.8345, 'grad_norm': 4.589225769042969, 'learning_rate': 5.4716656824812505e-06, 'epoch': 1.98}
 66%|██████▌   | 412/624 [35:54<18:35,  5.26s/it] 66%|██████▌   | 413/624 [35:59<18:25,  5.24s/it]                                                 {'loss': 0.7606, 'grad_norm': 4.1122918128967285, 'learning_rate': 5.425428946646016e-06, 'epoch': 1.98}
 66%|██████▌   | 413/624 [36:00<18:25,  5.24s/it] 66%|██████▋   | 414/624 [36:05<18:13,  5.21s/it]                                                 {'loss': 0.7058, 'grad_norm': 4.12670373916626, 'learning_rate': 5.379315560596038e-06, 'epoch': 1.99}
 66%|██████▋   | 414/624 [36:05<18:13,  5.21s/it] 67%|██████▋   | 415/624 [36:10<18:16,  5.25s/it]                                                 {'loss': 0.7423, 'grad_norm': 4.640219688415527, 'learning_rate': 5.333326767743263e-06, 'epoch': 1.99}
 67%|██████▋   | 415/624 [36:10<18:16,  5.25s/it] 67%|██████▋   | 416/624 [36:15<18:17,  5.27s/it]                                                 {'loss': 0.7586, 'grad_norm': 4.267625331878662, 'learning_rate': 5.287463808140069e-06, 'epoch': 2.0}
 67%|██████▋   | 416/624 [36:15<18:17,  5.27s/it] 67%|██████▋   | 417/624 [36:21<18:18,  5.31s/it]                                                 {'loss': 0.7754, 'grad_norm': 4.066494464874268, 'learning_rate': 5.241727918445836e-06, 'epoch': 2.0}
 67%|██████▋   | 417/624 [36:21<18:18,  5.31s/it] 67%|██████▋   | 418/624 [36:26<18:19,  5.34s/it]                                                 {'loss': 0.4158, 'grad_norm': 4.6665263175964355, 'learning_rate': 5.1961203318936116e-06, 'epoch': 2.0}
 67%|██████▋   | 418/624 [36:26<18:19,  5.34s/it] 67%|██████▋   | 419/624 [36:31<18:11,  5.32s/it]                                                 {'loss': 0.2967, 'grad_norm': 3.9314653873443604, 'learning_rate': 5.1506422782568345e-06, 'epoch': 2.01}
 67%|██████▋   | 419/624 [36:31<18:11,  5.32s/it] 67%|██████▋   | 420/624 [36:37<18:16,  5.37s/it]                                                 {'loss': 0.3315, 'grad_norm': 3.964576244354248, 'learning_rate': 5.105294983816203e-06, 'epoch': 2.01}
 67%|██████▋   | 420/624 [36:37<18:16,  5.37s/it] 67%|██████▋   | 421/624 [36:42<18:03,  5.34s/it]                                                 {'loss': 0.2547, 'grad_norm': 3.7003588676452637, 'learning_rate': 5.060079671326577e-06, 'epoch': 2.02}
 67%|██████▋   | 421/624 [36:42<18:03,  5.34s/it] 68%|██████▊   | 422/624 [36:47<17:27,  5.19s/it]                                                 {'loss': 0.2921, 'grad_norm': 3.439863681793213, 'learning_rate': 5.014997559984045e-06, 'epoch': 2.02}
 68%|██████▊   | 422/624 [36:47<17:27,  5.19s/it] 68%|██████▊   | 423/624 [36:52<17:30,  5.22s/it]                                                 {'loss': 0.2292, 'grad_norm': 3.9964418411254883, 'learning_rate': 4.970049865393009e-06, 'epoch': 2.03}
 68%|██████▊   | 423/624 [36:52<17:30,  5.22s/it] 68%|██████▊   | 424/624 [36:57<17:24,  5.22s/it]                                                 {'loss': 0.2347, 'grad_norm': 4.764218807220459, 'learning_rate': 4.925237799533445e-06, 'epoch': 2.03}
 68%|██████▊   | 424/624 [36:58<17:24,  5.22s/it] 68%|██████▊   | 425/624 [37:02<16:59,  5.12s/it]                                                 {'loss': 0.1981, 'grad_norm': 4.562900066375732, 'learning_rate': 4.880562570728188e-06, 'epoch': 2.04}
 68%|██████▊   | 425/624 [37:02<16:59,  5.12s/it] 68%|██████▊   | 426/624 [37:07<16:51,  5.11s/it]                                                 {'loss': 0.3598, 'grad_norm': 8.10767650604248, 'learning_rate': 4.836025383610382e-06, 'epoch': 2.04}
 68%|██████▊   | 426/624 [37:08<16:51,  5.11s/it] 68%|██████▊   | 427/624 [37:13<17:05,  5.21s/it]                                                 {'loss': 0.3461, 'grad_norm': 7.0620622634887695, 'learning_rate': 4.791627439090975e-06, 'epoch': 2.05}
 68%|██████▊   | 427/624 [37:13<17:05,  5.21s/it] 69%|██████▊   | 428/624 [37:18<16:46,  5.13s/it]                                                 {'loss': 0.2597, 'grad_norm': 5.663003921508789, 'learning_rate': 4.74736993432634e-06, 'epoch': 2.05}
 69%|██████▊   | 428/624 [37:18<16:46,  5.13s/it] 69%|██████▉   | 429/624 [37:23<16:55,  5.21s/it]                                                 {'loss': 0.2663, 'grad_norm': 4.369237899780273, 'learning_rate': 4.703254062686017e-06, 'epoch': 2.06}
 69%|██████▉   | 429/624 [37:23<16:55,  5.21s/it] 69%|██████▉   | 430/624 [37:28<16:39,  5.15s/it]                                                 {'loss': 0.2753, 'grad_norm': 4.225524425506592, 'learning_rate': 4.6592810137205e-06, 'epoch': 2.06}
 69%|██████▉   | 430/624 [37:28<16:39,  5.15s/it] 69%|██████▉   | 431/624 [37:33<16:28,  5.12s/it]                                                 {'loss': 0.1969, 'grad_norm': 3.509521007537842, 'learning_rate': 4.615451973129196e-06, 'epoch': 2.07}
 69%|██████▉   | 431/624 [37:33<16:28,  5.12s/it] 69%|██████▉   | 432/624 [37:38<16:14,  5.07s/it]                                                 {'loss': 0.2166, 'grad_norm': 3.7248055934906006, 'learning_rate': 4.571768122728421e-06, 'epoch': 2.07}
 69%|██████▉   | 432/624 [37:38<16:14,  5.07s/it] 69%|██████▉   | 433/624 [37:43<16:12,  5.09s/it]                                                 {'loss': 0.2295, 'grad_norm': 3.307004928588867, 'learning_rate': 4.528230640419562e-06, 'epoch': 2.08}
 69%|██████▉   | 433/624 [37:44<16:12,  5.09s/it] 70%|██████▉   | 434/624 [37:49<16:31,  5.22s/it]                                                 {'loss': 0.3093, 'grad_norm': 3.639601945877075, 'learning_rate': 4.4848407001572945e-06, 'epoch': 2.08}
 70%|██████▉   | 434/624 [37:49<16:31,  5.22s/it] 70%|██████▉   | 435/624 [37:54<16:18,  5.18s/it]                                                 {'loss': 0.2697, 'grad_norm': 3.771383285522461, 'learning_rate': 4.441599471917946e-06, 'epoch': 2.09}
 70%|██████▉   | 435/624 [37:54<16:18,  5.18s/it] 70%|██████▉   | 436/624 [37:59<16:12,  5.17s/it]                                                 {'loss': 0.2879, 'grad_norm': 3.6001243591308594, 'learning_rate': 4.398508121667925e-06, 'epoch': 2.09}
 70%|██████▉   | 436/624 [37:59<16:12,  5.17s/it] 70%|███████   | 437/624 [38:05<16:35,  5.32s/it]                                                 {'loss': 0.2958, 'grad_norm': 5.331789493560791, 'learning_rate': 4.355567811332311e-06, 'epoch': 2.1}
 70%|███████   | 437/624 [38:05<16:35,  5.32s/it] 70%|███████   | 438/624 [38:10<16:41,  5.39s/it]                                                 {'loss': 0.2504, 'grad_norm': 3.7524986267089844, 'learning_rate': 4.312779698763493e-06, 'epoch': 2.1}
 70%|███████   | 438/624 [38:10<16:41,  5.39s/it] 70%|███████   | 439/624 [38:15<16:06,  5.22s/it]                                                 {'loss': 0.2217, 'grad_norm': 3.8069725036621094, 'learning_rate': 4.270144937709981e-06, 'epoch': 2.11}
 70%|███████   | 439/624 [38:15<16:06,  5.22s/it] 71%|███████   | 440/624 [38:20<16:05,  5.25s/it]                                                 {'loss': 0.2718, 'grad_norm': 3.1079111099243164, 'learning_rate': 4.227664677785264e-06, 'epoch': 2.11}
 71%|███████   | 440/624 [38:21<16:05,  5.25s/it] 71%|███████   | 441/624 [38:26<16:03,  5.27s/it]                                                 {'loss': 0.2209, 'grad_norm': 4.6123809814453125, 'learning_rate': 4.1853400644368395e-06, 'epoch': 2.12}
 71%|███████   | 441/624 [38:26<16:03,  5.27s/it] 71%|███████   | 442/624 [38:31<16:04,  5.30s/it]                                                 {'loss': 0.2091, 'grad_norm': 4.6684770584106445, 'learning_rate': 4.143172238915302e-06, 'epoch': 2.12}
 71%|███████   | 442/624 [38:31<16:04,  5.30s/it] 71%|███████   | 443/624 [38:36<15:57,  5.29s/it]                                                 {'loss': 0.2071, 'grad_norm': 4.086822032928467, 'learning_rate': 4.101162338243595e-06, 'epoch': 2.12}
 71%|███████   | 443/624 [38:37<15:57,  5.29s/it] 71%|███████   | 444/624 [38:42<15:48,  5.27s/it]                                                 {'loss': 0.2502, 'grad_norm': 4.099936008453369, 'learning_rate': 4.059311495186338e-06, 'epoch': 2.13}
 71%|███████   | 444/624 [38:42<15:48,  5.27s/it] 71%|███████▏  | 445/624 [38:47<15:45,  5.28s/it]                                                 {'loss': 0.3035, 'grad_norm': 5.14033317565918, 'learning_rate': 4.017620838219276e-06, 'epoch': 2.13}
 71%|███████▏  | 445/624 [38:47<15:45,  5.28s/it] 71%|███████▏  | 446/624 [38:52<15:29,  5.22s/it]                                                 {'loss': 0.2758, 'grad_norm': 4.246625900268555, 'learning_rate': 3.9760914914988716e-06, 'epoch': 2.14}
 71%|███████▏  | 446/624 [38:52<15:29,  5.22s/it] 72%|███████▏  | 447/624 [38:57<15:20,  5.20s/it]                                                 {'loss': 0.2838, 'grad_norm': 5.2103962898254395, 'learning_rate': 3.93472457483197e-06, 'epoch': 2.14}
 72%|███████▏  | 447/624 [38:57<15:20,  5.20s/it] 72%|███████▏  | 448/624 [39:02<15:20,  5.23s/it]                                                 {'loss': 0.2223, 'grad_norm': 3.438966989517212, 'learning_rate': 3.893521203645618e-06, 'epoch': 2.15}
 72%|███████▏  | 448/624 [39:03<15:20,  5.23s/it] 72%|███████▏  | 449/624 [39:08<15:11,  5.21s/it]                                                 {'loss': 0.3192, 'grad_norm': 4.939876079559326, 'learning_rate': 3.852482488956992e-06, 'epoch': 2.15}
 72%|███████▏  | 449/624 [39:08<15:11,  5.21s/it] 72%|███████▏  | 450/624 [39:13<15:14,  5.26s/it]                                                 {'loss': 0.2535, 'grad_norm': 3.7734830379486084, 'learning_rate': 3.8116095373434204e-06, 'epoch': 2.16}
 72%|███████▏  | 450/624 [39:13<15:14,  5.26s/it] 72%|███████▏  | 451/624 [39:18<15:04,  5.23s/it]                                                 {'loss': 0.316, 'grad_norm': 4.815637588500977, 'learning_rate': 3.7709034509125706e-06, 'epoch': 2.16}
 72%|███████▏  | 451/624 [39:18<15:04,  5.23s/it] 72%|███████▏  | 452/624 [39:24<15:08,  5.28s/it]                                                 {'loss': 0.3035, 'grad_norm': 3.4322144985198975, 'learning_rate': 3.7303653272727057e-06, 'epoch': 2.17}
 72%|███████▏  | 452/624 [39:24<15:08,  5.28s/it] 73%|███████▎  | 453/624 [39:29<14:55,  5.23s/it]                                                 {'loss': 0.3192, 'grad_norm': 3.9768619537353516, 'learning_rate': 3.689996259503116e-06, 'epoch': 2.17}
 73%|███████▎  | 453/624 [39:29<14:55,  5.23s/it] 73%|███████▎  | 454/624 [39:34<14:50,  5.24s/it]                                                 {'loss': 0.2847, 'grad_norm': 3.7274653911590576, 'learning_rate': 3.6497973361246153e-06, 'epoch': 2.18}
 73%|███████▎  | 454/624 [39:34<14:50,  5.24s/it] 73%|███████▎  | 455/624 [39:39<14:51,  5.28s/it]                                                 {'loss': 0.238, 'grad_norm': 3.4423694610595703, 'learning_rate': 3.609769641070221e-06, 'epoch': 2.18}
 73%|███████▎  | 455/624 [39:39<14:51,  5.28s/it] 73%|███████▎  | 456/624 [39:45<14:50,  5.30s/it]                                                 {'loss': 0.2399, 'grad_norm': 3.4264416694641113, 'learning_rate': 3.569914253655896e-06, 'epoch': 2.19}
 73%|███████▎  | 456/624 [39:45<14:50,  5.30s/it] 73%|███████▎  | 457/624 [39:50<14:53,  5.35s/it]                                                 {'loss': 0.2212, 'grad_norm': 3.359952449798584, 'learning_rate': 3.530232248551466e-06, 'epoch': 2.19}
 73%|███████▎  | 457/624 [39:50<14:53,  5.35s/it] 73%|███████▎  | 458/624 [39:55<14:45,  5.33s/it]                                                 {'loss': 0.318, 'grad_norm': 4.060177326202393, 'learning_rate': 3.4907246957516416e-06, 'epoch': 2.2}
 73%|███████▎  | 458/624 [39:56<14:45,  5.33s/it] 74%|███████▎  | 459/624 [40:00<14:22,  5.23s/it]                                                 {'loss': 0.3091, 'grad_norm': 4.476503372192383, 'learning_rate': 3.4513926605471504e-06, 'epoch': 2.2}
 74%|███████▎  | 459/624 [40:01<14:22,  5.23s/it] 74%|███████▎  | 460/624 [40:06<14:16,  5.23s/it]                                                 {'loss': 0.3017, 'grad_norm': 4.370196342468262, 'learning_rate': 3.412237203496036e-06, 'epoch': 2.21}
 74%|███████▎  | 460/624 [40:06<14:16,  5.23s/it] 74%|███████▍  | 461/624 [40:11<14:07,  5.20s/it]                                                 {'loss': 0.2634, 'grad_norm': 4.652647972106934, 'learning_rate': 3.3732593803950354e-06, 'epoch': 2.21}
 74%|███████▍  | 461/624 [40:11<14:07,  5.20s/it] 74%|███████▍  | 462/624 [40:16<14:06,  5.23s/it]                                                 {'loss': 0.3159, 'grad_norm': 3.833338975906372, 'learning_rate': 3.3344602422511343e-06, 'epoch': 2.22}
 74%|███████▍  | 462/624 [40:16<14:06,  5.23s/it] 74%|███████▍  | 463/624 [40:21<13:53,  5.18s/it]                                                 {'loss': 0.2541, 'grad_norm': 3.901547431945801, 'learning_rate': 3.2958408352532055e-06, 'epoch': 2.22}
 74%|███████▍  | 463/624 [40:21<13:53,  5.18s/it] 74%|███████▍  | 464/624 [40:26<13:50,  5.19s/it]                                                 {'loss': 0.3963, 'grad_norm': 5.03230619430542, 'learning_rate': 3.257402200743821e-06, 'epoch': 2.23}
 74%|███████▍  | 464/624 [40:27<13:50,  5.19s/it] 75%|███████▍  | 465/624 [40:32<13:46,  5.20s/it]                                                 {'loss': 0.2497, 'grad_norm': 4.320362567901611, 'learning_rate': 3.2191453751911505e-06, 'epoch': 2.23}
 75%|███████▍  | 465/624 [40:32<13:46,  5.20s/it] 75%|███████▍  | 466/624 [40:37<13:50,  5.26s/it]                                                 {'loss': 0.2536, 'grad_norm': 3.7471423149108887, 'learning_rate': 3.1810713901610367e-06, 'epoch': 2.24}
 75%|███████▍  | 466/624 [40:37<13:50,  5.26s/it] 75%|███████▍  | 467/624 [40:42<13:37,  5.21s/it]                                                 {'loss': 0.2018, 'grad_norm': 4.058675765991211, 'learning_rate': 3.1431812722891598e-06, 'epoch': 2.24}
 75%|███████▍  | 467/624 [40:42<13:37,  5.21s/it] 75%|███████▌  | 468/624 [40:47<13:38,  5.25s/it]                                                 {'loss': 0.2223, 'grad_norm': 3.5737240314483643, 'learning_rate': 3.1054760432533626e-06, 'epoch': 2.24}
 75%|███████▌  | 468/624 [40:48<13:38,  5.25s/it] 75%|███████▌  | 469/624 [40:53<13:32,  5.24s/it]                                                 {'loss': 0.2714, 'grad_norm': 4.056735992431641, 'learning_rate': 3.0679567197461135e-06, 'epoch': 2.25}
 75%|███████▌  | 469/624 [40:53<13:32,  5.24s/it] 75%|███████▌  | 470/624 [40:58<13:44,  5.36s/it]                                                 {'loss': 0.2323, 'grad_norm': 3.7800779342651367, 'learning_rate': 3.0306243134470668e-06, 'epoch': 2.25}
 75%|███████▌  | 470/624 [40:58<13:44,  5.36s/it] 75%|███████▌  | 471/624 [41:03<13:32,  5.31s/it]                                                 {'loss': 0.2754, 'grad_norm': 3.0329864025115967, 'learning_rate': 2.993479830995815e-06, 'epoch': 2.26}
 75%|███████▌  | 471/624 [41:04<13:32,  5.31s/it] 76%|███████▌  | 472/624 [41:09<13:31,  5.34s/it]                                                 {'loss': 0.2029, 'grad_norm': 4.23122501373291, 'learning_rate': 2.9565242739647115e-06, 'epoch': 2.26}
 76%|███████▌  | 472/624 [41:09<13:31,  5.34s/it] 76%|███████▌  | 473/624 [41:14<13:23,  5.32s/it]                                                 {'loss': 0.2041, 'grad_norm': 3.176377534866333, 'learning_rate': 2.919758638831893e-06, 'epoch': 2.27}
 76%|███████▌  | 473/624 [41:14<13:23,  5.32s/it] 76%|███████▌  | 474/624 [41:19<13:16,  5.31s/it]                                                 {'loss': 0.2625, 'grad_norm': 4.082326412200928, 'learning_rate': 2.8831839169543998e-06, 'epoch': 2.27}
 76%|███████▌  | 474/624 [41:20<13:16,  5.31s/it] 76%|███████▌  | 475/624 [41:25<13:13,  5.33s/it]                                                 {'loss': 0.2495, 'grad_norm': 3.5715749263763428, 'learning_rate': 2.84680109454143e-06, 'epoch': 2.28}
 76%|███████▌  | 475/624 [41:25<13:13,  5.33s/it] 76%|███████▋  | 476/624 [41:30<13:09,  5.33s/it]                                                 {'loss': 0.2979, 'grad_norm': 4.226057529449463, 'learning_rate': 2.810611152627777e-06, 'epoch': 2.28}
 76%|███████▋  | 476/624 [41:30<13:09,  5.33s/it] 76%|███████▋  | 477/624 [41:35<12:57,  5.29s/it]                                                 {'loss': 0.2879, 'grad_norm': 4.7347283363342285, 'learning_rate': 2.774615067047346e-06, 'epoch': 2.29}
 76%|███████▋  | 477/624 [41:35<12:57,  5.29s/it] 77%|███████▋  | 478/624 [41:41<12:50,  5.28s/it]                                                 {'loss': 0.266, 'grad_norm': 3.5669965744018555, 'learning_rate': 2.738813808406866e-06, 'epoch': 2.29}
 77%|███████▋  | 478/624 [41:41<12:50,  5.28s/it] 77%|███████▋  | 479/624 [41:46<12:33,  5.20s/it]                                                 {'loss': 0.2764, 'grad_norm': 4.309319496154785, 'learning_rate': 2.7032083420597e-06, 'epoch': 2.3}
 77%|███████▋  | 479/624 [41:46<12:33,  5.20s/it] 77%|███████▋  | 480/624 [41:51<12:25,  5.18s/it]                                                 {'loss': 0.208, 'grad_norm': 3.5840537548065186, 'learning_rate': 2.667799628079829e-06, 'epoch': 2.3}
 77%|███████▋  | 480/624 [41:51<12:25,  5.18s/it] 77%|███████▋  | 481/624 [41:56<12:15,  5.14s/it]                                                 {'loss': 0.2341, 'grad_norm': 3.590589761734009, 'learning_rate': 2.6325886212359496e-06, 'epoch': 2.31}
 77%|███████▋  | 481/624 [41:56<12:15,  5.14s/it] 77%|███████▋  | 482/624 [42:01<12:18,  5.20s/it]                                                 {'loss': 0.2877, 'grad_norm': 4.339122772216797, 'learning_rate': 2.5975762709657506e-06, 'epoch': 2.31}
 77%|███████▋  | 482/624 [42:01<12:18,  5.20s/it] 77%|███████▋  | 483/624 [42:06<12:04,  5.14s/it]                                                 {'loss': 0.2618, 'grad_norm': 4.265746116638184, 'learning_rate': 2.5627635213502832e-06, 'epoch': 2.32}
 77%|███████▋  | 483/624 [42:06<12:04,  5.14s/it] 78%|███████▊  | 484/624 [42:11<12:04,  5.18s/it]                                                 {'loss': 0.2554, 'grad_norm': 3.556255340576172, 'learning_rate': 2.528151311088537e-06, 'epoch': 2.32}
 78%|███████▊  | 484/624 [42:12<12:04,  5.18s/it] 78%|███████▊  | 485/624 [42:17<12:18,  5.31s/it]                                                 {'loss': 0.1764, 'grad_norm': 3.480921745300293, 'learning_rate': 2.4937405734720964e-06, 'epoch': 2.33}
 78%|███████▊  | 485/624 [42:17<12:18,  5.31s/it] 78%|███████▊  | 486/624 [42:22<12:07,  5.27s/it]                                                 {'loss': 0.1995, 'grad_norm': 3.5984599590301514, 'learning_rate': 2.459532236360007e-06, 'epoch': 2.33}
 78%|███████▊  | 486/624 [42:22<12:07,  5.27s/it] 78%|███████▊  | 487/624 [42:27<12:04,  5.29s/it]                                                 {'loss': 0.202, 'grad_norm': 3.30592679977417, 'learning_rate': 2.4255272221537295e-06, 'epoch': 2.34}
 78%|███████▊  | 487/624 [42:28<12:04,  5.29s/it] 78%|███████▊  | 488/624 [42:33<11:51,  5.23s/it]                                                 {'loss': 0.257, 'grad_norm': 4.51020622253418, 'learning_rate': 2.391726447772279e-06, 'epoch': 2.34}
 78%|███████▊  | 488/624 [42:33<11:51,  5.23s/it] 78%|███████▊  | 489/624 [42:38<11:42,  5.20s/it]                                                 {'loss': 0.285, 'grad_norm': 4.122512340545654, 'learning_rate': 2.3581308246275103e-06, 'epoch': 2.35}
 78%|███████▊  | 489/624 [42:38<11:42,  5.20s/it] 79%|███████▊  | 490/624 [42:43<11:37,  5.20s/it]                                                 {'loss': 0.2755, 'grad_norm': 3.798269748687744, 'learning_rate': 2.324741258599521e-06, 'epoch': 2.35}
 79%|███████▊  | 490/624 [42:43<11:37,  5.20s/it] 79%|███████▊  | 491/624 [42:48<11:41,  5.27s/it]                                                 {'loss': 0.2423, 'grad_norm': 3.541593313217163, 'learning_rate': 2.29155865001225e-06, 'epoch': 2.35}
 79%|███████▊  | 491/624 [42:49<11:41,  5.27s/it] 79%|███████▉  | 492/624 [42:54<11:33,  5.25s/it]                                                 {'loss': 0.3379, 'grad_norm': 4.055622100830078, 'learning_rate': 2.2585838936091753e-06, 'epoch': 2.36}
 79%|███████▉  | 492/624 [42:54<11:33,  5.25s/it] 79%|███████▉  | 493/624 [42:59<11:36,  5.32s/it]                                                 {'loss': 0.3174, 'grad_norm': 4.306836128234863, 'learning_rate': 2.225817878529214e-06, 'epoch': 2.36}
 79%|███████▉  | 493/624 [42:59<11:36,  5.32s/it] 79%|███████▉  | 494/624 [43:04<11:25,  5.28s/it]                                                 {'loss': 0.1959, 'grad_norm': 4.191518783569336, 'learning_rate': 2.1932614882827196e-06, 'epoch': 2.37}
 79%|███████▉  | 494/624 [43:04<11:25,  5.28s/it] 79%|███████▉  | 495/624 [43:09<11:14,  5.23s/it]                                                 {'loss': 0.2756, 'grad_norm': 4.565829277038574, 'learning_rate': 2.160915600727688e-06, 'epoch': 2.37}
 79%|███████▉  | 495/624 [43:09<11:14,  5.23s/it] 79%|███████▉  | 496/624 [43:14<10:56,  5.13s/it]                                                 {'loss': 0.3171, 'grad_norm': 4.042275905609131, 'learning_rate': 2.1287810880460636e-06, 'epoch': 2.38}
 79%|███████▉  | 496/624 [43:14<10:56,  5.13s/it] 80%|███████▉  | 497/624 [43:19<10:55,  5.16s/it]                                                 {'loss': 0.3391, 'grad_norm': 4.248159408569336, 'learning_rate': 2.0968588167202265e-06, 'epoch': 2.38}
 80%|███████▉  | 497/624 [43:20<10:55,  5.16s/it] 80%|███████▉  | 498/624 [43:25<10:54,  5.19s/it]                                                 {'loss': 0.3086, 'grad_norm': 3.743675708770752, 'learning_rate': 2.0651496475096455e-06, 'epoch': 2.39}
 80%|███████▉  | 498/624 [43:25<10:54,  5.19s/it] 80%|███████▉  | 499/624 [43:30<10:40,  5.12s/it]                                                 {'loss': 0.2601, 'grad_norm': 3.836890459060669, 'learning_rate': 2.03365443542764e-06, 'epoch': 2.39}
 80%|███████▉  | 499/624 [43:30<10:40,  5.12s/it] 80%|████████  | 500/624 [43:35<10:39,  5.16s/it]                                                 {'loss': 0.2559, 'grad_norm': 4.074875354766846, 'learning_rate': 2.0023740297183536e-06, 'epoch': 2.4}
 80%|████████  | 500/624 [43:35<10:39,  5.16s/it] 80%|████████  | 501/624 [43:40<10:34,  5.16s/it]                                                 {'loss': 0.2419, 'grad_norm': 3.4617977142333984, 'learning_rate': 1.971309273833828e-06, 'epoch': 2.4}
 80%|████████  | 501/624 [43:40<10:34,  5.16s/it] 80%|████████  | 502/624 [43:45<10:29,  5.16s/it]                                                 {'loss': 0.3114, 'grad_norm': 4.104637622833252, 'learning_rate': 1.940461005411288e-06, 'epoch': 2.41}
 80%|████████  | 502/624 [43:45<10:29,  5.16s/it] 81%|████████  | 503/624 [43:50<10:26,  5.18s/it]                                                 {'loss': 0.3663, 'grad_norm': 4.057311058044434, 'learning_rate': 1.9098300562505266e-06, 'epoch': 2.41}
 81%|████████  | 503/624 [43:51<10:26,  5.18s/it] 81%|████████  | 504/624 [43:55<10:15,  5.13s/it]                                                 {'loss': 0.2094, 'grad_norm': 3.4384925365448, 'learning_rate': 1.8794172522915022e-06, 'epoch': 2.42}
 81%|████████  | 504/624 [43:56<10:15,  5.13s/it] 81%|████████  | 505/624 [44:01<10:14,  5.17s/it]                                                 {'loss': 0.223, 'grad_norm': 3.509537696838379, 'learning_rate': 1.849223413592046e-06, 'epoch': 2.42}
 81%|████████  | 505/624 [44:01<10:14,  5.17s/it] 81%|████████  | 506/624 [44:06<10:23,  5.28s/it]                                                 {'loss': 0.2817, 'grad_norm': 3.483518600463867, 'learning_rate': 1.8192493543057676e-06, 'epoch': 2.43}
 81%|████████  | 506/624 [44:06<10:23,  5.28s/it] 81%|████████▏ | 507/624 [44:11<10:13,  5.25s/it]                                                 {'loss': 0.3256, 'grad_norm': 3.877835273742676, 'learning_rate': 1.7894958826600884e-06, 'epoch': 2.43}
 81%|████████▏ | 507/624 [44:12<10:13,  5.25s/it] 81%|████████▏ | 508/624 [44:16<09:57,  5.15s/it]                                                 {'loss': 0.2049, 'grad_norm': 5.905150890350342, 'learning_rate': 1.7599638009344566e-06, 'epoch': 2.44}
 81%|████████▏ | 508/624 [44:17<09:57,  5.15s/it] 82%|████████▏ | 509/624 [44:22<09:53,  5.16s/it]                                                 {'loss': 0.2446, 'grad_norm': 4.8593220710754395, 'learning_rate': 1.730653905438714e-06, 'epoch': 2.44}
 82%|████████▏ | 509/624 [44:22<09:53,  5.16s/it] 82%|████████▏ | 510/624 [44:27<09:53,  5.21s/it]                                                 {'loss': 0.2239, 'grad_norm': 3.1440391540527344, 'learning_rate': 1.701566986491614e-06, 'epoch': 2.45}
 82%|████████▏ | 510/624 [44:27<09:53,  5.21s/it] 82%|████████▏ | 511/624 [44:32<09:43,  5.16s/it]                                                 {'loss': 0.3625, 'grad_norm': 3.698535203933716, 'learning_rate': 1.672703828399529e-06, 'epoch': 2.45}
 82%|████████▏ | 511/624 [44:32<09:43,  5.16s/it] 82%|████████▏ | 512/624 [44:37<09:39,  5.17s/it]                                                 {'loss': 0.2033, 'grad_norm': 3.026841640472412, 'learning_rate': 1.6440652094352838e-06, 'epoch': 2.46}
 82%|████████▏ | 512/624 [44:37<09:39,  5.17s/it] 82%|████████▏ | 513/624 [44:43<09:41,  5.24s/it]                                                 {'loss': 0.2932, 'grad_norm': 3.859499454498291, 'learning_rate': 1.6156519018171856e-06, 'epoch': 2.46}
 82%|████████▏ | 513/624 [44:43<09:41,  5.24s/it] 82%|████████▏ | 514/624 [44:47<09:26,  5.15s/it]                                                 {'loss': 0.2826, 'grad_norm': 3.7417867183685303, 'learning_rate': 1.587464671688187e-06, 'epoch': 2.47}
 82%|████████▏ | 514/624 [44:48<09:26,  5.15s/it] 83%|████████▎ | 515/624 [44:53<09:31,  5.24s/it]                                                 {'loss': 0.2409, 'grad_norm': 3.3168201446533203, 'learning_rate': 1.5595042790952442e-06, 'epoch': 2.47}
 83%|████████▎ | 515/624 [44:53<09:31,  5.24s/it] 83%|████████▎ | 516/624 [44:58<09:29,  5.27s/it]                                                 {'loss': 0.2681, 'grad_norm': 3.6131956577301025, 'learning_rate': 1.5317714779688076e-06, 'epoch': 2.47}
 83%|████████▎ | 516/624 [44:58<09:29,  5.27s/it] 83%|████████▎ | 517/624 [45:03<09:10,  5.14s/it]                                                 {'loss': 0.1775, 'grad_norm': 3.9265003204345703, 'learning_rate': 1.5042670161024975e-06, 'epoch': 2.48}
 83%|████████▎ | 517/624 [45:03<09:10,  5.14s/it] 83%|████████▎ | 518/624 [45:09<09:15,  5.24s/it]                                                 {'loss': 0.3078, 'grad_norm': 3.4972574710845947, 'learning_rate': 1.4769916351329495e-06, 'epoch': 2.48}
 83%|████████▎ | 518/624 [45:09<09:15,  5.24s/it] 83%|████████▎ | 519/624 [45:14<09:10,  5.24s/it]                                                 {'loss': 0.3135, 'grad_norm': 4.160727024078369, 'learning_rate': 1.4499460705198e-06, 'epoch': 2.49}
 83%|████████▎ | 519/624 [45:14<09:10,  5.24s/it] 83%|████████▎ | 520/624 [45:19<09:08,  5.27s/it]                                                 {'loss': 0.2339, 'grad_norm': 4.126347064971924, 'learning_rate': 1.4231310515258745e-06, 'epoch': 2.49}
 83%|████████▎ | 520/624 [45:19<09:08,  5.27s/it] 83%|████████▎ | 521/624 [45:25<09:07,  5.31s/it]                                                 {'loss': 0.2556, 'grad_norm': 3.6463351249694824, 'learning_rate': 1.396547301197504e-06, 'epoch': 2.5}
 83%|████████▎ | 521/624 [45:25<09:07,  5.31s/it] 84%|████████▎ | 522/624 [45:30<08:53,  5.23s/it]                                                 {'loss': 0.2313, 'grad_norm': 4.199587345123291, 'learning_rate': 1.3701955363450447e-06, 'epoch': 2.5}
 84%|████████▎ | 522/624 [45:30<08:53,  5.23s/it] 84%|████████▍ | 523/624 [45:35<08:47,  5.22s/it]                                                 {'loss': 0.2067, 'grad_norm': 10.08755874633789, 'learning_rate': 1.3440764675235384e-06, 'epoch': 2.51}
 84%|████████▍ | 523/624 [45:35<08:47,  5.22s/it] 84%|████████▍ | 524/624 [45:40<08:48,  5.28s/it]                                                 {'loss': 0.3981, 'grad_norm': 3.6997737884521484, 'learning_rate': 1.3181907990135624e-06, 'epoch': 2.51}
 84%|████████▍ | 524/624 [45:40<08:48,  5.28s/it] 84%|████████▍ | 525/624 [45:46<08:43,  5.29s/it]                                                 {'loss': 0.2614, 'grad_norm': 4.184865474700928, 'learning_rate': 1.2925392288022299e-06, 'epoch': 2.52}
 84%|████████▍ | 525/624 [45:46<08:43,  5.29s/it] 84%|████████▍ | 526/624 [45:51<08:35,  5.26s/it]                                                 {'loss': 0.2378, 'grad_norm': 3.915482997894287, 'learning_rate': 1.267122448564374e-06, 'epoch': 2.52}
 84%|████████▍ | 526/624 [45:51<08:35,  5.26s/it] 84%|████████▍ | 527/624 [45:56<08:31,  5.27s/it]                                                 {'loss': 0.2925, 'grad_norm': 3.9575092792510986, 'learning_rate': 1.2419411436439021e-06, 'epoch': 2.53}
 84%|████████▍ | 527/624 [45:56<08:31,  5.27s/it] 85%|████████▍ | 528/624 [46:01<08:17,  5.18s/it]                                                 {'loss': 0.2452, 'grad_norm': 3.801764726638794, 'learning_rate': 1.2169959930353049e-06, 'epoch': 2.53}
 85%|████████▍ | 528/624 [46:01<08:17,  5.18s/it] 85%|████████▍ | 529/624 [46:06<08:15,  5.22s/it]                                                 {'loss': 0.2633, 'grad_norm': 4.857479572296143, 'learning_rate': 1.1922876693653584e-06, 'epoch': 2.54}
 85%|████████▍ | 529/624 [46:06<08:15,  5.22s/it] 85%|████████▍ | 530/624 [46:11<08:05,  5.16s/it]                                                 {'loss': 0.2, 'grad_norm': 4.07623291015625, 'learning_rate': 1.1678168388749788e-06, 'epoch': 2.54}
 85%|████████▍ | 530/624 [46:11<08:05,  5.16s/it] 85%|████████▌ | 531/624 [46:16<07:59,  5.16s/it]                                                 {'loss': 0.2261, 'grad_norm': 4.166932106018066, 'learning_rate': 1.1435841614012666e-06, 'epoch': 2.55}
 85%|████████▌ | 531/624 [46:17<07:59,  5.16s/it] 85%|████████▌ | 532/624 [46:22<08:03,  5.25s/it]                                                 {'loss': 0.1715, 'grad_norm': 3.2493033409118652, 'learning_rate': 1.1195902903597023e-06, 'epoch': 2.55}
 85%|████████▌ | 532/624 [46:22<08:03,  5.25s/it] 85%|████████▌ | 533/624 [46:27<07:53,  5.20s/it]                                                 {'loss': 0.2452, 'grad_norm': 3.8766868114471436, 'learning_rate': 1.0958358727265438e-06, 'epoch': 2.56}
 85%|████████▌ | 533/624 [46:27<07:53,  5.20s/it] 86%|████████▌ | 534/624 [46:32<07:50,  5.23s/it]                                                 {'loss': 0.2445, 'grad_norm': 3.9902172088623047, 'learning_rate': 1.0723215490213635e-06, 'epoch': 2.56}
 86%|████████▌ | 534/624 [46:32<07:50,  5.23s/it] 86%|████████▌ | 535/624 [46:38<07:51,  5.30s/it]                                                 {'loss': 0.3101, 'grad_norm': 4.448685646057129, 'learning_rate': 1.0490479532897946e-06, 'epoch': 2.57}
 86%|████████▌ | 535/624 [46:38<07:51,  5.30s/it] 86%|████████▌ | 536/624 [46:43<07:42,  5.26s/it]                                                 {'loss': 0.2828, 'grad_norm': 3.7508575916290283, 'learning_rate': 1.0260157130864178e-06, 'epoch': 2.57}
 86%|████████▌ | 536/624 [46:43<07:42,  5.26s/it] 86%|████████▌ | 537/624 [46:48<07:39,  5.28s/it]                                                 {'loss': 0.2363, 'grad_norm': 3.6896538734436035, 'learning_rate': 1.0032254494578519e-06, 'epoch': 2.58}
 86%|████████▌ | 537/624 [46:48<07:39,  5.28s/it] 86%|████████▌ | 538/624 [46:53<07:30,  5.23s/it]                                                 {'loss': 0.2986, 'grad_norm': 4.251917362213135, 'learning_rate': 9.806777769260034e-07, 'epoch': 2.58}
 86%|████████▌ | 538/624 [46:54<07:30,  5.23s/it] 86%|████████▋ | 539/624 [46:59<07:25,  5.24s/it]                                                 {'loss': 0.4385, 'grad_norm': 6.2171173095703125, 'learning_rate': 9.583733034714982e-07, 'epoch': 2.59}
 86%|████████▋ | 539/624 [46:59<07:25,  5.24s/it] 87%|████████▋ | 540/624 [47:04<07:21,  5.26s/it]                                                 {'loss': 0.1975, 'grad_norm': 3.382280111312866, 'learning_rate': 9.363126305172831e-07, 'epoch': 2.59}
 87%|████████▋ | 540/624 [47:04<07:21,  5.26s/it] 87%|████████▋ | 541/624 [47:09<07:22,  5.34s/it]                                                 {'loss': 0.3167, 'grad_norm': 4.463460445404053, 'learning_rate': 9.144963529124163e-07, 'epoch': 2.59}
 87%|████████▋ | 541/624 [47:10<07:22,  5.34s/it] 87%|████████▋ | 542/624 [47:15<07:15,  5.32s/it]                                                 {'loss': 0.3051, 'grad_norm': 5.253643035888672, 'learning_rate': 8.929250589160166e-07, 'epoch': 2.6}
 87%|████████▋ | 542/624 [47:15<07:15,  5.32s/it] 87%|████████▋ | 543/624 [47:20<07:07,  5.28s/it]                                                 {'loss': 0.2336, 'grad_norm': 3.5419867038726807, 'learning_rate': 8.715993301814174e-07, 'epoch': 2.6}
 87%|████████▋ | 543/624 [47:20<07:07,  5.28s/it] 87%|████████▋ | 544/624 [47:25<06:58,  5.23s/it]                                                 {'loss': 0.1858, 'grad_norm': 3.7477879524230957, 'learning_rate': 8.505197417404687e-07, 'epoch': 2.61}
 87%|████████▋ | 544/624 [47:25<06:58,  5.23s/it] 87%|████████▋ | 545/624 [47:30<06:53,  5.23s/it]                                                 {'loss': 0.2486, 'grad_norm': 3.8847994804382324, 'learning_rate': 8.296868619880372e-07, 'epoch': 2.61}
 87%|████████▋ | 545/624 [47:30<06:53,  5.23s/it] 88%|████████▊ | 546/624 [47:36<06:58,  5.36s/it]                                                 {'loss': 0.2717, 'grad_norm': 3.947620153427124, 'learning_rate': 8.091012526666797e-07, 'epoch': 2.62}
 88%|████████▊ | 546/624 [47:36<06:58,  5.36s/it] 88%|████████▊ | 547/624 [47:41<06:53,  5.37s/it]                                                 {'loss': 0.3387, 'grad_norm': 4.074047088623047, 'learning_rate': 7.887634688515e-07, 'epoch': 2.62}
 88%|████████▊ | 547/624 [47:41<06:53,  5.37s/it] 88%|████████▊ | 548/624 [47:47<06:49,  5.39s/it]                                                 {'loss': 0.2847, 'grad_norm': 3.855196952819824, 'learning_rate': 7.686740589351704e-07, 'epoch': 2.63}
 88%|████████▊ | 548/624 [47:47<06:49,  5.39s/it] 88%|████████▊ | 549/624 [47:52<06:43,  5.37s/it]                                                 {'loss': 0.2614, 'grad_norm': 3.588390350341797, 'learning_rate': 7.488335646131628e-07, 'epoch': 2.63}
 88%|████████▊ | 549/624 [47:52<06:43,  5.37s/it] 88%|████████▊ | 550/624 [47:57<06:37,  5.37s/it]                                                 {'loss': 0.176, 'grad_norm': 3.884979724884033, 'learning_rate': 7.292425208691212e-07, 'epoch': 2.64}
 88%|████████▊ | 550/624 [47:58<06:37,  5.37s/it] 88%|████████▊ | 551/624 [48:03<06:30,  5.35s/it]                                                 {'loss': 0.2535, 'grad_norm': 4.6635003089904785, 'learning_rate': 7.099014559604556e-07, 'epoch': 2.64}
 88%|████████▊ | 551/624 [48:03<06:30,  5.35s/it] 88%|████████▊ | 552/624 [48:08<06:24,  5.33s/it]                                                 {'loss': 0.2625, 'grad_norm': 3.531280755996704, 'learning_rate': 6.908108914040823e-07, 'epoch': 2.65}
 88%|████████▊ | 552/624 [48:08<06:24,  5.33s/it] 89%|████████▊ | 553/624 [48:13<06:12,  5.25s/it]                                                 {'loss': 0.2627, 'grad_norm': 3.9103446006774902, 'learning_rate': 6.71971341962373e-07, 'epoch': 2.65}
 89%|████████▊ | 553/624 [48:13<06:12,  5.25s/it] 89%|████████▉ | 554/624 [48:18<06:05,  5.22s/it]                                                 {'loss': 0.2303, 'grad_norm': 3.4851651191711426, 'learning_rate': 6.53383315629268e-07, 'epoch': 2.66}
 89%|████████▉ | 554/624 [48:18<06:05,  5.22s/it] 89%|████████▉ | 555/624 [48:24<06:04,  5.28s/it]                                                 {'loss': 0.2347, 'grad_norm': 3.6572184562683105, 'learning_rate': 6.350473136165836e-07, 'epoch': 2.66}
 89%|████████▉ | 555/624 [48:24<06:04,  5.28s/it] 89%|████████▉ | 556/624 [48:29<05:58,  5.28s/it]                                                 {'loss': 0.2015, 'grad_norm': 4.242655277252197, 'learning_rate': 6.169638303404912e-07, 'epoch': 2.67}
 89%|████████▉ | 556/624 [48:29<05:58,  5.28s/it] 89%|████████▉ | 557/624 [48:34<05:47,  5.19s/it]                                                 {'loss': 0.2236, 'grad_norm': 3.816291570663452, 'learning_rate': 5.991333534081878e-07, 'epoch': 2.67}
 89%|████████▉ | 557/624 [48:34<05:47,  5.19s/it] 89%|████████▉ | 558/624 [48:39<05:44,  5.22s/it]                                                 {'loss': 0.2927, 'grad_norm': 3.983116865158081, 'learning_rate': 5.815563636047539e-07, 'epoch': 2.68}
 89%|████████▉ | 558/624 [48:39<05:44,  5.22s/it] 90%|████████▉ | 559/624 [48:45<05:41,  5.25s/it]                                                 {'loss': 0.2974, 'grad_norm': 3.5196259021759033, 'learning_rate': 5.64233334880181e-07, 'epoch': 2.68}
 90%|████████▉ | 559/624 [48:45<05:41,  5.25s/it] 90%|████████▉ | 560/624 [48:50<05:39,  5.30s/it]                                                 {'loss': 0.1977, 'grad_norm': 3.409346342086792, 'learning_rate': 5.471647343365982e-07, 'epoch': 2.69}
 90%|████████▉ | 560/624 [48:50<05:39,  5.30s/it] 90%|████████▉ | 561/624 [48:55<05:36,  5.34s/it]                                                 {'loss': 0.2212, 'grad_norm': 3.7508668899536133, 'learning_rate': 5.303510222156716e-07, 'epoch': 2.69}
 90%|████████▉ | 561/624 [48:56<05:36,  5.34s/it] 90%|█████████ | 562/624 [49:01<05:26,  5.26s/it]                                                 {'loss': 0.2257, 'grad_norm': 3.817783832550049, 'learning_rate': 5.137926518862013e-07, 'epoch': 2.7}
 90%|█████████ | 562/624 [49:01<05:26,  5.26s/it] 90%|█████████ | 563/624 [49:06<05:18,  5.22s/it]                                                 {'loss': 0.3136, 'grad_norm': 4.362356185913086, 'learning_rate': 4.974900698318885e-07, 'epoch': 2.7}
 90%|█████████ | 563/624 [49:06<05:18,  5.22s/it] 90%|█████████ | 564/624 [49:10<05:06,  5.11s/it]                                                 {'loss': 0.2593, 'grad_norm': 3.5634210109710693, 'learning_rate': 4.814437156393048e-07, 'epoch': 2.71}
 90%|█████████ | 564/624 [49:11<05:06,  5.11s/it] 91%|█████████ | 565/624 [49:16<05:01,  5.10s/it]                                                 {'loss': 0.2221, 'grad_norm': 3.52213454246521, 'learning_rate': 4.656540219860317e-07, 'epoch': 2.71}
 91%|█████████ | 565/624 [49:16<05:01,  5.10s/it] 91%|█████████ | 566/624 [49:21<04:55,  5.10s/it]                                                 {'loss': 0.2351, 'grad_norm': 3.7227771282196045, 'learning_rate': 4.501214146289956e-07, 'epoch': 2.71}
 91%|█████████ | 566/624 [49:21<04:55,  5.10s/it] 91%|█████████ | 567/624 [49:26<04:50,  5.10s/it]                                                 {'loss': 0.2597, 'grad_norm': 3.9005913734436035, 'learning_rate': 4.3484631239299356e-07, 'epoch': 2.72}
 91%|█████████ | 567/624 [49:26<04:50,  5.10s/it] 91%|█████████ | 568/624 [49:31<04:44,  5.08s/it]                                                 {'loss': 0.2536, 'grad_norm': 4.319121360778809, 'learning_rate': 4.198291271593924e-07, 'epoch': 2.72}
 91%|█████████ | 568/624 [49:31<04:44,  5.08s/it] 91%|█████████ | 569/624 [49:36<04:36,  5.03s/it]                                                 {'loss': 0.2687, 'grad_norm': 4.239817142486572, 'learning_rate': 4.0507026385502747e-07, 'epoch': 2.73}
 91%|█████████ | 569/624 [49:36<04:36,  5.03s/it] 91%|█████████▏| 570/624 [49:41<04:32,  5.04s/it]                                                 {'loss': 0.2174, 'grad_norm': 3.479632616043091, 'learning_rate': 3.9057012044127817e-07, 'epoch': 2.73}
 91%|█████████▏| 570/624 [49:41<04:32,  5.04s/it] 92%|█████████▏| 571/624 [49:46<04:33,  5.16s/it]                                                 {'loss': 0.2081, 'grad_norm': 4.314373970031738, 'learning_rate': 3.7632908790334656e-07, 'epoch': 2.74}
 92%|█████████▏| 571/624 [49:46<04:33,  5.16s/it] 92%|█████████▏| 572/624 [49:52<04:33,  5.25s/it]                                                 {'loss': 0.1824, 'grad_norm': 3.519484758377075, 'learning_rate': 3.6234755023970447e-07, 'epoch': 2.74}
 92%|█████████▏| 572/624 [49:52<04:33,  5.25s/it] 92%|█████████▏| 573/624 [49:57<04:29,  5.28s/it]                                                 {'loss': 0.259, 'grad_norm': 3.744863748550415, 'learning_rate': 3.4862588445174985e-07, 'epoch': 2.75}
 92%|█████████▏| 573/624 [49:57<04:29,  5.28s/it] 92%|█████████▏| 574/624 [50:02<04:20,  5.21s/it]                                                 {'loss': 0.2449, 'grad_norm': 4.11930513381958, 'learning_rate': 3.3516446053363015e-07, 'epoch': 2.75}
 92%|█████████▏| 574/624 [50:02<04:20,  5.21s/it] 92%|█████████▏| 575/624 [50:07<04:15,  5.22s/it]                                                 {'loss': 0.2355, 'grad_norm': 3.165898561477661, 'learning_rate': 3.219636414622751e-07, 'epoch': 2.76}
 92%|█████████▏| 575/624 [50:07<04:15,  5.22s/it] 92%|█████████▏| 576/624 [50:13<04:11,  5.24s/it]                                                 {'loss': 0.2391, 'grad_norm': 3.3515872955322266, 'learning_rate': 3.090237831876053e-07, 'epoch': 2.76}
 92%|█████████▏| 576/624 [50:13<04:11,  5.24s/it] 92%|█████████▏| 577/624 [50:18<04:07,  5.26s/it]                                                 {'loss': 0.2692, 'grad_norm': 3.6394920349121094, 'learning_rate': 2.9634523462293005e-07, 'epoch': 2.77}
 92%|█████████▏| 577/624 [50:18<04:07,  5.26s/it] 93%|█████████▎| 578/624 [50:23<03:58,  5.19s/it]                                                 {'loss': 0.1924, 'grad_norm': 3.466031551361084, 'learning_rate': 2.839283376355506e-07, 'epoch': 2.77}
 93%|█████████▎| 578/624 [50:23<03:58,  5.19s/it] 93%|█████████▎| 579/624 [50:28<03:54,  5.22s/it]                                                 {'loss': 0.1957, 'grad_norm': 3.440791606903076, 'learning_rate': 2.717734270375272e-07, 'epoch': 2.78}
 93%|█████████▎| 579/624 [50:28<03:54,  5.22s/it] 93%|█████████▎| 580/624 [50:34<03:51,  5.25s/it]                                                 {'loss': 0.1672, 'grad_norm': 3.6708550453186035, 'learning_rate': 2.5988083057666534e-07, 'epoch': 2.78}
 93%|█████████▎| 580/624 [50:34<03:51,  5.25s/it] 93%|█████████▎| 581/624 [50:39<03:44,  5.23s/it]                                                 {'loss': 0.267, 'grad_norm': 3.69411301612854, 'learning_rate': 2.4825086892766745e-07, 'epoch': 2.79}
 93%|█████████▎| 581/624 [50:39<03:44,  5.23s/it] 93%|█████████▎| 582/624 [50:44<03:40,  5.25s/it]                                                 {'loss': 0.2407, 'grad_norm': 3.8594558238983154, 'learning_rate': 2.3688385568349515e-07, 'epoch': 2.79}
 93%|█████████▎| 582/624 [50:44<03:40,  5.25s/it] 93%|█████████▎| 583/624 [50:49<03:36,  5.29s/it]                                                 {'loss': 0.2371, 'grad_norm': 3.679711103439331, 'learning_rate': 2.2578009734690264e-07, 'epoch': 2.8}
 93%|█████████▎| 583/624 [50:50<03:36,  5.29s/it] 94%|█████████▎| 584/624 [50:55<03:31,  5.29s/it]                                                 {'loss': 0.1647, 'grad_norm': 3.83065128326416, 'learning_rate': 2.1493989332218468e-07, 'epoch': 2.8}
 94%|█████████▎| 584/624 [50:55<03:31,  5.29s/it] 94%|█████████▍| 585/624 [51:00<03:26,  5.29s/it]                                                 {'loss': 0.2124, 'grad_norm': 5.1189680099487305, 'learning_rate': 2.043635359070928e-07, 'epoch': 2.81}
 94%|█████████▍| 585/624 [51:00<03:26,  5.29s/it] 94%|█████████▍| 586/624 [51:06<03:24,  5.38s/it]                                                 {'loss': 0.26, 'grad_norm': 3.7618391513824463, 'learning_rate': 1.9405131028495838e-07, 'epoch': 2.81}
 94%|█████████▍| 586/624 [51:06<03:24,  5.38s/it] 94%|█████████▍| 587/624 [51:11<03:17,  5.35s/it]                                                 {'loss': 0.2583, 'grad_norm': 3.7379567623138428, 'learning_rate': 1.8400349451700438e-07, 'epoch': 2.82}
 94%|█████████▍| 587/624 [51:11<03:17,  5.35s/it] 94%|█████████▍| 588/624 [51:16<03:11,  5.31s/it]                                                 {'loss': 0.2757, 'grad_norm': 3.282256841659546, 'learning_rate': 1.742203595348435e-07, 'epoch': 2.82}
 94%|█████████▍| 588/624 [51:16<03:11,  5.31s/it] 94%|█████████▍| 589/624 [51:21<03:06,  5.31s/it]                                                 {'loss': 0.2405, 'grad_norm': 5.768991470336914, 'learning_rate': 1.6470216913317628e-07, 'epoch': 2.82}
 94%|█████████▍| 589/624 [51:22<03:06,  5.31s/it] 95%|█████████▍| 590/624 [51:26<02:56,  5.20s/it]                                                 {'loss': 0.26, 'grad_norm': 3.9318807125091553, 'learning_rate': 1.5544917996267562e-07, 'epoch': 2.83}
 95%|█████████▍| 590/624 [51:26<02:56,  5.20s/it] 95%|█████████▍| 591/624 [51:32<02:56,  5.34s/it]                                                 {'loss': 0.2773, 'grad_norm': 3.8331799507141113, 'learning_rate': 1.464616415230702e-07, 'epoch': 2.83}
 95%|█████████▍| 591/624 [51:32<02:56,  5.34s/it] 95%|█████████▍| 592/624 [51:37<02:48,  5.27s/it]                                                 {'loss': 0.1795, 'grad_norm': 4.130887985229492, 'learning_rate': 1.3773979615640976e-07, 'epoch': 2.84}
 95%|█████████▍| 592/624 [51:37<02:48,  5.27s/it] 95%|█████████▌| 593/624 [51:42<02:41,  5.22s/it]                                                 {'loss': 0.1779, 'grad_norm': 3.9126644134521484, 'learning_rate': 1.292838790405393e-07, 'epoch': 2.84}
 95%|█████████▌| 593/624 [51:42<02:41,  5.22s/it] 95%|█████████▌| 594/624 [51:47<02:34,  5.16s/it]                                                 {'loss': 0.2894, 'grad_norm': 5.116908550262451, 'learning_rate': 1.2109411818274851e-07, 'epoch': 2.85}
 95%|█████████▌| 594/624 [51:47<02:34,  5.16s/it] 95%|█████████▌| 595/624 [51:53<02:31,  5.23s/it]                                                 {'loss': 0.2984, 'grad_norm': 3.762362480163574, 'learning_rate': 1.1317073441363458e-07, 'epoch': 2.85}
 95%|█████████▌| 595/624 [51:53<02:31,  5.23s/it] 96%|█████████▌| 596/624 [51:58<02:25,  5.18s/it]                                                 {'loss': 0.2552, 'grad_norm': 4.664359092712402, 'learning_rate': 1.055139413811379e-07, 'epoch': 2.86}
 96%|█████████▌| 596/624 [51:58<02:25,  5.18s/it] 96%|█████████▌| 597/624 [52:03<02:21,  5.25s/it]                                                 {'loss': 0.1885, 'grad_norm': 3.9532761573791504, 'learning_rate': 9.812394554478355e-08, 'epoch': 2.86}
 96%|█████████▌| 597/624 [52:03<02:21,  5.25s/it] 96%|█████████▌| 598/624 [52:08<02:14,  5.15s/it]                                                 {'loss': 0.1889, 'grad_norm': 4.312304973602295, 'learning_rate': 9.10009461701189e-08, 'epoch': 2.87}
 96%|█████████▌| 598/624 [52:08<02:14,  5.15s/it] 96%|█████████▌| 599/624 [52:13<02:09,  5.17s/it]                                                 {'loss': 0.2102, 'grad_norm': 3.7572035789489746, 'learning_rate': 8.41451353233369e-08, 'epoch': 2.87}
 96%|█████████▌| 599/624 [52:13<02:09,  5.17s/it] 96%|█████████▌| 600/624 [52:18<02:04,  5.20s/it]                                                 {'loss': 0.2162, 'grad_norm': 3.6691513061523438, 'learning_rate': 7.755669786609688e-08, 'epoch': 2.88}
 96%|█████████▌| 600/624 [52:19<02:04,  5.20s/it] 96%|█████████▋| 601/624 [52:24<02:00,  5.24s/it]                                                 {'loss': 0.2294, 'grad_norm': 4.073980331420898, 'learning_rate': 7.123581145053849e-08, 'epoch': 2.88}
 96%|█████████▋| 601/624 [52:24<02:00,  5.24s/it] 96%|█████████▋| 602/624 [52:29<01:55,  5.25s/it]                                                 {'loss': 0.2886, 'grad_norm': 3.843400001525879, 'learning_rate': 6.51826465144978e-08, 'epoch': 2.89}
 96%|█████████▋| 602/624 [52:29<01:55,  5.25s/it] 97%|█████████▋| 603/624 [52:35<01:52,  5.36s/it]                                                 {'loss': 0.2689, 'grad_norm': 3.510457754135132, 'learning_rate': 5.93973662769054e-08, 'epoch': 2.89}
 97%|█████████▋| 603/624 [52:35<01:52,  5.36s/it] 97%|█████████▋| 604/624 [52:40<01:46,  5.34s/it]                                                 {'loss': 0.3286, 'grad_norm': 4.5790205001831055, 'learning_rate': 5.388012673338661e-08, 'epoch': 2.9}
 97%|█████████▋| 604/624 [52:40<01:46,  5.34s/it] 97%|█████████▋| 605/624 [52:45<01:42,  5.37s/it]                                                 {'loss': 0.2203, 'grad_norm': 3.948054313659668, 'learning_rate': 4.863107665205702e-08, 'epoch': 2.9}
 97%|█████████▋| 605/624 [52:46<01:42,  5.37s/it] 97%|█████████▋| 606/624 [52:51<01:35,  5.31s/it]                                                 {'loss': 0.2689, 'grad_norm': 4.022992134094238, 'learning_rate': 4.365035756950797e-08, 'epoch': 2.91}
 97%|█████████▋| 606/624 [52:51<01:35,  5.31s/it] 97%|█████████▋| 607/624 [52:56<01:30,  5.30s/it]                                                 {'loss': 0.2527, 'grad_norm': 4.026095390319824, 'learning_rate': 3.8938103786995144e-08, 'epoch': 2.91}
 97%|█████████▋| 607/624 [52:56<01:30,  5.30s/it] 97%|█████████▋| 608/624 [53:01<01:23,  5.24s/it]                                                 {'loss': 0.2185, 'grad_norm': 3.964531183242798, 'learning_rate': 3.449444236681254e-08, 'epoch': 2.92}
 97%|█████████▋| 608/624 [53:01<01:23,  5.24s/it] 98%|█████████▊| 609/624 [53:06<01:17,  5.16s/it]                                                 {'loss': 0.2846, 'grad_norm': 4.27479887008667, 'learning_rate': 3.03194931288664e-08, 'epoch': 2.92}
 98%|█████████▊| 609/624 [53:06<01:17,  5.16s/it] 98%|█████████▊| 610/624 [53:11<01:13,  5.22s/it]                                                 {'loss': 0.3976, 'grad_norm': 4.119151592254639, 'learning_rate': 2.641336864744992e-08, 'epoch': 2.93}
 98%|█████████▊| 610/624 [53:11<01:13,  5.22s/it] 98%|█████████▊| 611/624 [53:16<01:06,  5.10s/it]                                                 {'loss': 0.2707, 'grad_norm': 3.886348247528076, 'learning_rate': 2.2776174248199114e-08, 'epoch': 2.93}
 98%|█████████▊| 611/624 [53:16<01:06,  5.10s/it] 98%|█████████▊| 612/624 [53:22<01:02,  5.17s/it]                                                 {'loss': 0.2057, 'grad_norm': 3.73752760887146, 'learning_rate': 1.9408008005260548e-08, 'epoch': 2.94}
 98%|█████████▊| 612/624 [53:22<01:02,  5.17s/it] 98%|█████████▊| 613/624 [53:27<00:56,  5.13s/it]                                                 {'loss': 0.3228, 'grad_norm': 3.853245258331299, 'learning_rate': 1.630896073864352e-08, 'epoch': 2.94}
 98%|█████████▊| 613/624 [53:27<00:56,  5.13s/it] 98%|█████████▊| 614/624 [53:32<00:52,  5.25s/it]                                                 {'loss': 0.3369, 'grad_norm': 4.637118339538574, 'learning_rate': 1.3479116011769766e-08, 'epoch': 2.94}
 98%|█████████▊| 614/624 [53:32<00:52,  5.25s/it] 99%|█████████▊| 615/624 [53:37<00:47,  5.23s/it]                                                 {'loss': 0.2527, 'grad_norm': 3.416062593460083, 'learning_rate': 1.0918550129223049e-08, 'epoch': 2.95}
 99%|█████████▊| 615/624 [53:37<00:47,  5.23s/it] 99%|█████████▊| 616/624 [53:43<00:42,  5.33s/it]                                                 {'loss': 0.276, 'grad_norm': 3.6667351722717285, 'learning_rate': 8.627332134690802e-09, 'epoch': 2.95}
 99%|█████████▊| 616/624 [53:43<00:42,  5.33s/it] 99%|█████████▉| 617/624 [53:48<00:36,  5.24s/it]                                                 {'loss': 0.2138, 'grad_norm': 4.073422908782959, 'learning_rate': 6.605523809102288e-09, 'epoch': 2.96}
 99%|█████████▉| 617/624 [53:48<00:36,  5.24s/it] 99%|█████████▉| 618/624 [53:53<00:31,  5.23s/it]                                                 {'loss': 0.247, 'grad_norm': 4.149883270263672, 'learning_rate': 4.853179668959928e-09, 'epoch': 2.96}
 99%|█████████▉| 618/624 [53:53<00:31,  5.23s/it] 99%|█████████▉| 619/624 [53:58<00:26,  5.29s/it]                                                 {'loss': 0.2586, 'grad_norm': 4.08870267868042, 'learning_rate': 3.3703469648760367e-09, 'epoch': 2.97}
 99%|█████████▉| 619/624 [53:59<00:26,  5.29s/it] 99%|█████████▉| 620/624 [54:04<00:21,  5.32s/it]                                                 {'loss': 0.2281, 'grad_norm': 4.055110454559326, 'learning_rate': 2.1570656802905042e-09, 'epoch': 2.97}
 99%|█████████▉| 620/624 [54:04<00:21,  5.32s/it]100%|█████████▉| 621/624 [54:09<00:15,  5.20s/it]                                                 {'loss': 0.1758, 'grad_norm': 3.2229502201080322, 'learning_rate': 1.213368530399439e-09, 'epoch': 2.98}
100%|█████████▉| 621/624 [54:09<00:15,  5.20s/it]100%|█████████▉| 622/624 [54:14<00:10,  5.19s/it]                                                 {'loss': 0.2857, 'grad_norm': 4.001386642456055, 'learning_rate': 5.392809612703165e-10, 'epoch': 2.98}
100%|█████████▉| 622/624 [54:14<00:10,  5.19s/it]100%|█████████▉| 623/624 [54:19<00:05,  5.10s/it]                                                 {'loss': 0.23, 'grad_norm': 3.8714094161987305, 'learning_rate': 1.3482114915475132e-10, 'epoch': 2.99}
100%|█████████▉| 623/624 [54:19<00:05,  5.10s/it]100%|██████████| 624/624 [54:24<00:00,  5.24s/it]                                                 {'loss': 0.1991, 'grad_norm': 4.07641077041626, 'learning_rate': 0.0, 'epoch': 2.99}
100%|██████████| 624/624 [54:25<00:00,  5.24s/it]/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank2]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank2]:     self._save_rng_state(output_dir)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank2]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank2]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank2]:     return container(name_or_buffer)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank2]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank2]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_dolly/checkpoint-624/rng_state_2.pth cannot be opened.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank2]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank2]:     self._save_rng_state(output_dir)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank2]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank2]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank2]:     return container(name_or_buffer)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank2]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank2]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_dolly/checkpoint-624/rng_state_2.pth cannot be opened.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank1]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank1]:     self._save_rng_state(output_dir)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank1]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank1]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank1]:     return container(name_or_buffer)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank1]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank1]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_dolly/checkpoint-624/rng_state_1.pth cannot be opened.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank1]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank1]:     self._save_rng_state(output_dir)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank1]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank1]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank1]:     return container(name_or_buffer)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank1]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank1]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_dolly/checkpoint-624/rng_state_1.pth cannot be opened.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3012, in _save_checkpoint
[rank0]:     self._save_optimizer_and_scheduler(output_dir)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3150, in _save_optimizer_and_scheduler
[rank0]:     save_fsdp_optimizer(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/utils/fsdp_utils.py", line 193, in save_fsdp_optimizer
[rank0]:     torch.save(optim_state, output_optimizer_file)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank0]:     return container(name_or_buffer)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank0]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank0]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_dolly/checkpoint-624/optimizer.bin cannot be opened.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3012, in _save_checkpoint
[rank0]:     self._save_optimizer_and_scheduler(output_dir)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3150, in _save_optimizer_and_scheduler
[rank0]:     save_fsdp_optimizer(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/utils/fsdp_utils.py", line 193, in save_fsdp_optimizer
[rank0]:     torch.save(optim_state, output_optimizer_file)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank0]:     return container(name_or_buffer)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank0]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank0]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_dolly/checkpoint-624/optimizer.bin cannot be opened.
[1;34mwandb[0m: 🚀 View run [33m/nlp/scr/qinanyu/model_cache/result_dolly[0m at: [34mhttps://wandb.ai/limber/huggingface/runs/2o539gkm[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../../sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_152707-2o539gkm/logs[0m
W1010 16:32:05.148000 140001791881600 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 543758 closing signal SIGTERM
W1010 16:32:05.148000 140001791881600 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 543759 closing signal SIGTERM
E1010 16:32:06.718000 140001791881600 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 543760) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-10_16:32:05
  host      : sphinx6.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 543760)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
SLURM_JOBID=8748564
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1010 18:14:05.283000 140482710811008 torch/distributed/run.py:779] 
W1010 18:14:05.283000 140482710811008 torch/distributed/run.py:779] *****************************************
W1010 18:14:05.283000 140482710811008 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1010 18:14:05.283000 140482710811008 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  5.85it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.52it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  4.51it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.40it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.54it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  5.23it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.22it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  4.37it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  6.04it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  5.75it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  6.36it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.40it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  5.80it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  5.99it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  4.70it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  5.54it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  5.48it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.77it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.72it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.14it/s]
Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.61it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.78it/s]
Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.52it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.62it/s]
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 250, in train
[rank2]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 208, in make_supervised_data_module
[rank2]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 169, in __init__
[rank2]:     for example in ds
[rank2]: NameError: name 'ds' is not defined
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 250, in train
[rank2]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 208, in make_supervised_data_module
[rank2]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 169, in __init__
[rank2]:     for example in ds
[rank2]: NameError: name 'ds' is not defined
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 250, in train
[rank1]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 208, in make_supervised_data_module
[rank1]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 169, in __init__
[rank1]:     for example in ds
[rank1]: NameError: name 'ds' is not defined
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 250, in train
[rank1]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 208, in make_supervised_data_module
[rank1]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 169, in __init__
[rank1]:     for example in ds
[rank1]: NameError: name 'ds' is not defined
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 250, in train
[rank0]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 208, in make_supervised_data_module
[rank0]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 169, in __init__
[rank0]:     for example in ds
[rank0]: NameError: name 'ds' is not defined
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 250, in train
[rank0]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 208, in make_supervised_data_module
[rank0]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 169, in __init__
[rank0]:     for example in ds
[rank0]: NameError: name 'ds' is not defined
W1010 18:14:23.147000 140482710811008 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 604436 closing signal SIGTERM
W1010 18:14:23.147000 140482710811008 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 604437 closing signal SIGTERM
E1010 18:14:23.512000 140482710811008 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 604435) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-10_18:14:23
  host      : sphinx6.stanford.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 604435)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
SLURM_JOBID=8748570
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1010 18:16:28.777000 140156095541632 torch/distributed/run.py:779] 
W1010 18:16:28.777000 140156095541632 torch/distributed/run.py:779] *****************************************
W1010 18:16:28.777000 140156095541632 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1010 18:16:28.777000 140156095541632 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.45it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.66it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.68it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.62it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  4.22it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  4.20it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.55it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.69it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.49it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.02it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  5.76it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  6.59it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  6.17it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.22it/s]
Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  4.22it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.18it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.51it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.12it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  7.27it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.58it/s]
Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  5.01it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  5.85it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.59it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.30it/s]
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
loaded
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
load trainer
load trainer
load trainer
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_181907-skmnrrsg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /nlp/scr/qinanyu/model_cache/result_dolly_abortion
wandb: ⭐️ View project at https://wandb.ai/limber/huggingface
wandb: 🚀 View run at https://wandb.ai/limber/huggingface/runs/skmnrrsg
  0%|          | 0/3 [00:00<?, ?it/s][rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank1]:     adamw(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank1]:     func(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank1]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank1]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank1]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank1]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank1]:     adamw(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank1]:     func(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank1]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank1]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank1]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank1]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank0]:     self.optimizer.step(closure)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank0]:     adamw(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank0]:     func(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank0]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank0]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank0]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank0]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank0]:     self.optimizer.step(closure)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank0]:     adamw(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank0]:     func(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank0]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank0]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank0]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank0]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank2]:     self.optimizer.step()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank2]:     self.optimizer.step(closure)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank2]:     adamw(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank2]:     func(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank2]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank2]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank2]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank2]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank2]:     self.optimizer.step()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank2]:     self.optimizer.step(closure)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank2]:     adamw(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank2]:     func(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank2]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank2]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank2]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank2]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
W1010 18:19:10.805000 140156095541632 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 605185 closing signal SIGTERM
W1010 18:19:10.816000 140156095541632 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 605186 closing signal SIGTERM
[1;34mwandb[0m: 🚀 View run [33m/nlp/scr/qinanyu/model_cache/result_dolly_abortion[0m at: [34mhttps://wandb.ai/limber/huggingface/runs/skmnrrsg[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../../sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_181907-skmnrrsg/logs[0m
E1010 18:19:11.281000 140156095541632 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 605187) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-10_18:19:10
  host      : sphinx6.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 605187)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
SLURM_JOBID=8748572
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1010 18:20:34.448000 139935983923584 torch/distributed/run.py:779] 
W1010 18:20:34.448000 139935983923584 torch/distributed/run.py:779] *****************************************
W1010 18:20:34.448000 139935983923584 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1010 18:20:34.448000 139935983923584 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.69it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.65it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.54it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  4.30it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  4.15it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  5.90it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  6.39it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  6.75it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  3.94it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.27it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  5.21it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.68it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  4.04it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.94it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  5.21it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.17it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.02it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.21it/s]
Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.68it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.76it/s]
Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  4.18it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.81it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.65it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.76it/s]
loaded
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
load trainer
load trainer
load trainer
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_182057-kczmiekl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /nlp/scr/qinanyu/model_cache/result_dolly_abortion
wandb: ⭐️ View project at https://wandb.ai/limber/huggingface
wandb: 🚀 View run at https://wandb.ai/limber/huggingface/runs/kczmiekl
  0%|          | 0/3 [00:00<?, ?it/s][rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank2]:     self.optimizer.step()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank2]:     self.optimizer.step(closure)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank2]:     adamw(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank2]:     func(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank2]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank2]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank2]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank2]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank2]:     self.optimizer.step()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank2]:     self.optimizer.step(closure)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank2]:     adamw(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank2]:     func(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank2]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank2]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank2]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank2]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank0]:     self.optimizer.step(closure)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank0]:     adamw(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank0]:     func(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank0]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank0]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank0]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank0]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank0]:     self.optimizer.step(closure)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank0]:     adamw(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank0]:     func(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank0]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank0]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank0]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank0]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank1]:     adamw(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank1]:     func(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank1]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank1]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank1]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank1]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank1]:     adamw(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank1]:     func(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank1]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank1]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank1]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank1]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[1;34mwandb[0m: 🚀 View run [33m/nlp/scr/qinanyu/model_cache/result_dolly_abortion[0m at: [34mhttps://wandb.ai/limber/huggingface/runs/kczmiekl[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../../sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_182057-kczmiekl/logs[0m
W1010 18:21:00.955000 139935983923584 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 606619 closing signal SIGTERM
W1010 18:21:00.956000 139935983923584 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 606620 closing signal SIGTERM
E1010 18:21:01.429000 139935983923584 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 606621) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-10_18:21:00
  host      : sphinx6.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 606621)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
SLURM_JOBID=8748610
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1010 18:24:41.014000 139938968752512 torch/distributed/run.py:779] 
W1010 18:24:41.014000 139938968752512 torch/distributed/run.py:779] *****************************************
W1010 18:24:41.014000 139938968752512 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1010 18:24:41.014000 139938968752512 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.74it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  5.54it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.12it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.43it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  6.92it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.56it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.98it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  4.25it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  5.96it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  5.43it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.62it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  6.19it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  6.14it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  4.81it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.96it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  5.65it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  5.44it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.34it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.54it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.67it/s]
Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.18it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.49it/s]
Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.20it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.73it/s]
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
load trainer
load trainer
load trainer
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_182724-ykawhu15
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /nlp/scr/qinanyu/model_cache/result_alpaca_abortion
wandb: ⭐️ View project at https://wandb.ai/limber/huggingface
wandb: 🚀 View run at https://wandb.ai/limber/huggingface/runs/ykawhu15
  0%|          | 0/3 [00:00<?, ?it/s][rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank0]:     self.optimizer.step(closure)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank0]:     adamw(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank0]:     func(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank0]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank0]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank0]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank0]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank0]:     self.optimizer.step(closure)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank0]:     adamw(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank0]:     func(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank0]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank0]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank0]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank0]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank2]:     self.optimizer.step()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank2]:     self.optimizer.step(closure)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank2]:     adamw(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank2]:     func(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank2]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank2]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank2]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank2]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank2]:     self.optimizer.step()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank2]:     self.optimizer.step(closure)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank2]:     adamw(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank2]:     func(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank2]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank2]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank2]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank2]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank1]:     adamw(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank1]:     func(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank1]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank1]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank1]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank1]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 254, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2452, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/optimizer.py", line 171, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank1]:     adamw(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 767, in adamw
[rank1]:     func(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/adamw.py", line 485, in _multi_tensor_adamw
[rank1]:     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/optim/optimizer.py", line 512, in _group_tensors_by_device_and_dtype
[rank1]:     return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/utils/_foreach_utils.py", line 37, in _group_tensors_by_device_and_dtype
[rank1]:     return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)
[rank1]: RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding
[1;34mwandb[0m: 🚀 View run [33m/nlp/scr/qinanyu/model_cache/result_alpaca_abortion[0m at: [34mhttps://wandb.ai/limber/huggingface/runs/ykawhu15[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../../sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241010_182724-ykawhu15/logs[0m
W1010 18:27:27.316000 139938968752512 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 607935 closing signal SIGTERM
W1010 18:27:27.316000 139938968752512 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 607936 closing signal SIGTERM
E1010 18:27:27.782000 139938968752512 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 607937) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-10_18:27:27
  host      : sphinx6.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 607937)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
SLURM_JOBID=8749459
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
/var/lib/slurm/slurmd/job8749459/slurm_script: line 28: activate: No such file or directory
W1010 22:53:38.051000 140220709945728 torch/distributed/run.py:779] 
W1010 22:53:38.051000 140220709945728 torch/distributed/run.py:779] *****************************************
W1010 22:53:38.051000 140220709945728 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1010 22:53:38.051000 140220709945728 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
[rank0]: Traceback (most recent call last):
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
[rank0]:     response.raise_for_status()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
[rank0]:     raise HTTPError(http_error_msg, response=self)
[rank0]: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
[rank0]:     resolved_file = hf_hub_download(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
[rank0]:     return _hf_hub_download_to_cache_dir(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1339, in _hf_hub_download_to_cache_dir
[rank0]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
[rank0]:     raise head_call_error
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1746, in _get_metadata_or_catch_error
[rank0]:     metadata = get_hf_file_metadata(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1666, in get_hf_file_metadata
[rank0]:     r = _request_wrapper(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 364, in _request_wrapper
[rank0]:     response = _request_wrapper(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 388, in _request_wrapper
[rank0]:     hf_raise_for_status(response)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 454, in hf_raise_for_status
[rank0]:     raise _format(RepositoryNotFoundError, message, response) from e
[rank0]: huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6708bd6a-7df4833768006c8e4b3c501a;f691b1e1-42c6-4aca-b382-57dd9bdb789b)

[rank0]: Repository Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json.
[rank0]: Please make sure you specified the correct `repo_id` and `repo_type`.
[rank0]: If you are trying to access a private or gated repo, make sure you are authenticated.

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 219, in train
[rank0]:     model = transformers.AutoModelForCausalLM.from_pretrained(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 487, in from_pretrained
[rank0]:     resolved_config_file = cached_file(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 426, in cached_file
[rank0]:     raise EnvironmentError(
[rank0]: OSError: meta-llama/Llama-3.1-8 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
[rank0]: If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
[rank0]: Traceback (most recent call last):
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
[rank0]:     response.raise_for_status()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
[rank0]:     raise HTTPError(http_error_msg, response=self)
[rank0]: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
[rank0]:     resolved_file = hf_hub_download(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
[rank0]:     return _hf_hub_download_to_cache_dir(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1339, in _hf_hub_download_to_cache_dir
[rank0]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
[rank0]:     raise head_call_error
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1746, in _get_metadata_or_catch_error
[rank0]:     metadata = get_hf_file_metadata(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1666, in get_hf_file_metadata
[rank0]:     r = _request_wrapper(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 364, in _request_wrapper
[rank0]:     response = _request_wrapper(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 388, in _request_wrapper
[rank0]:     hf_raise_for_status(response)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 454, in hf_raise_for_status
[rank0]:     raise _format(RepositoryNotFoundError, message, response) from e
[rank0]: huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6708bd6a-7df4833768006c8e4b3c501a;f691b1e1-42c6-4aca-b382-57dd9bdb789b)

[rank0]: Repository Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json.
[rank0]: Please make sure you specified the correct `repo_id` and `repo_type`.
[rank0]: If you are trying to access a private or gated repo, make sure you are authenticated.

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 219, in train
[rank0]:     model = transformers.AutoModelForCausalLM.from_pretrained(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 487, in from_pretrained
[rank0]:     resolved_config_file = cached_file(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 426, in cached_file
[rank0]:     raise EnvironmentError(
[rank0]: OSError: meta-llama/Llama-3.1-8 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
[rank0]: If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
[rank2]: Traceback (most recent call last):
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
[rank2]:     response.raise_for_status()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
[rank2]:     raise HTTPError(http_error_msg, response=self)
[rank2]: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json

[rank2]: The above exception was the direct cause of the following exception:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
[rank2]:     resolved_file = hf_hub_download(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
[rank2]:     return _hf_hub_download_to_cache_dir(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1339, in _hf_hub_download_to_cache_dir
[rank2]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
[rank2]:     raise head_call_error
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1746, in _get_metadata_or_catch_error
[rank2]:     metadata = get_hf_file_metadata(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1666, in get_hf_file_metadata
[rank2]:     r = _request_wrapper(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 364, in _request_wrapper
[rank2]:     response = _request_wrapper(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 388, in _request_wrapper
[rank2]:     hf_raise_for_status(response)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 454, in hf_raise_for_status
[rank2]:     raise _format(RepositoryNotFoundError, message, response) from e
[rank2]: huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6708bd6a-493499ad1ec7e14c59db4e8d;8ce95faf-ebde-40ed-a0eb-02b0f6705878)

[rank2]: Repository Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json.
[rank2]: Please make sure you specified the correct `repo_id` and `repo_type`.
[rank2]: If you are trying to access a private or gated repo, make sure you are authenticated.

[rank2]: The above exception was the direct cause of the following exception:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 219, in train
[rank2]:     model = transformers.AutoModelForCausalLM.from_pretrained(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 487, in from_pretrained
[rank2]:     resolved_config_file = cached_file(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 426, in cached_file
[rank2]:     raise EnvironmentError(
[rank2]: OSError: meta-llama/Llama-3.1-8 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
[rank2]: If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
[rank2]: Traceback (most recent call last):
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
[rank2]:     response.raise_for_status()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
[rank2]:     raise HTTPError(http_error_msg, response=self)
[rank2]: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json

[rank2]: The above exception was the direct cause of the following exception:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
[rank2]:     resolved_file = hf_hub_download(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
[rank2]:     return _hf_hub_download_to_cache_dir(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1339, in _hf_hub_download_to_cache_dir
[rank2]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
[rank2]:     raise head_call_error
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1746, in _get_metadata_or_catch_error
[rank2]:     metadata = get_hf_file_metadata(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1666, in get_hf_file_metadata
[rank2]:     r = _request_wrapper(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 364, in _request_wrapper
[rank2]:     response = _request_wrapper(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 388, in _request_wrapper
[rank2]:     hf_raise_for_status(response)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 454, in hf_raise_for_status
[rank2]:     raise _format(RepositoryNotFoundError, message, response) from e
[rank2]: huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6708bd6a-493499ad1ec7e14c59db4e8d;8ce95faf-ebde-40ed-a0eb-02b0f6705878)

[rank2]: Repository Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json.
[rank2]: Please make sure you specified the correct `repo_id` and `repo_type`.
[rank2]: If you are trying to access a private or gated repo, make sure you are authenticated.

[rank2]: The above exception was the direct cause of the following exception:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 219, in train
[rank2]:     model = transformers.AutoModelForCausalLM.from_pretrained(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 487, in from_pretrained
[rank2]:     resolved_config_file = cached_file(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 426, in cached_file
[rank2]:     raise EnvironmentError(
[rank2]: OSError: meta-llama/Llama-3.1-8 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
[rank2]: If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
[rank1]: Traceback (most recent call last):
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
[rank1]:     response.raise_for_status()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
[rank1]:     raise HTTPError(http_error_msg, response=self)
[rank1]: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
[rank1]:     resolved_file = hf_hub_download(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
[rank1]:     return _hf_hub_download_to_cache_dir(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1339, in _hf_hub_download_to_cache_dir
[rank1]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
[rank1]:     raise head_call_error
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1746, in _get_metadata_or_catch_error
[rank1]:     metadata = get_hf_file_metadata(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1666, in get_hf_file_metadata
[rank1]:     r = _request_wrapper(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 364, in _request_wrapper
[rank1]:     response = _request_wrapper(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 388, in _request_wrapper
[rank1]:     hf_raise_for_status(response)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 454, in hf_raise_for_status
[rank1]:     raise _format(RepositoryNotFoundError, message, response) from e
[rank1]: huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6708bd6a-3a3a59bb754f66c04019d78a;ad759a34-83f1-4f18-b8ee-3d7601311967)

[rank1]: Repository Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json.
[rank1]: Please make sure you specified the correct `repo_id` and `repo_type`.
[rank1]: If you are trying to access a private or gated repo, make sure you are authenticated.

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 219, in train
[rank1]:     model = transformers.AutoModelForCausalLM.from_pretrained(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 487, in from_pretrained
[rank1]:     resolved_config_file = cached_file(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 426, in cached_file
[rank1]:     raise EnvironmentError(
[rank1]: OSError: meta-llama/Llama-3.1-8 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
[rank1]: If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
[rank1]: Traceback (most recent call last):
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
[rank1]:     response.raise_for_status()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
[rank1]:     raise HTTPError(http_error_msg, response=self)
[rank1]: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
[rank1]:     resolved_file = hf_hub_download(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
[rank1]:     return _hf_hub_download_to_cache_dir(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1339, in _hf_hub_download_to_cache_dir
[rank1]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
[rank1]:     raise head_call_error
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1746, in _get_metadata_or_catch_error
[rank1]:     metadata = get_hf_file_metadata(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1666, in get_hf_file_metadata
[rank1]:     r = _request_wrapper(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 364, in _request_wrapper
[rank1]:     response = _request_wrapper(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 388, in _request_wrapper
[rank1]:     hf_raise_for_status(response)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 454, in hf_raise_for_status
[rank1]:     raise _format(RepositoryNotFoundError, message, response) from e
[rank1]: huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6708bd6a-3a3a59bb754f66c04019d78a;ad759a34-83f1-4f18-b8ee-3d7601311967)

[rank1]: Repository Not Found for url: https://huggingface.co/meta-llama/Llama-3.1-8/resolve/main/config.json.
[rank1]: Please make sure you specified the correct `repo_id` and `repo_type`.
[rank1]: If you are trying to access a private or gated repo, make sure you are authenticated.

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 219, in train
[rank1]:     model = transformers.AutoModelForCausalLM.from_pretrained(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 487, in from_pretrained
[rank1]:     resolved_config_file = cached_file(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/utils/hub.py", line 426, in cached_file
[rank1]:     raise EnvironmentError(
[rank1]: OSError: meta-llama/Llama-3.1-8 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
[rank1]: If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
W1010 22:53:48.094000 140220709945728 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1003437 closing signal SIGTERM
W1010 22:53:48.094000 140220709945728 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1003438 closing signal SIGTERM
E1010 22:53:48.421000 140220709945728 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1003436) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-10_22:53:48
  host      : sphinx6.stanford.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1003436)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
SLURM_JOBID=8750277
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1011 06:33:21.755000 140117286433152 torch/distributed/run.py:779] 
W1011 06:33:21.755000 140117286433152 torch/distributed/run.py:779] *****************************************
W1011 06:33:21.755000 140117286433152 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1011 06:33:21.755000 140117286433152 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:39<01:58, 39.49s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:39<01:59, 39.71s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:39<01:59, 39.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:27<01:29, 44.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:28<01:29, 44.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:28<01:29, 44.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:09<00:43, 43.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:09<00:43, 43.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:09<00:43, 43.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:16<00:00, 29.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:16<00:00, 34.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:16<00:00, 29.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:16<00:00, 34.22s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:16<00:00, 29.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:16<00:00, 34.23s/it]
loaded
loaded
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank1]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank1]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path).shuffle()
[rank1]: AttributeError: 'SupervisedDataset' object has no attribute 'shuffle'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank1]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank1]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path).shuffle()
[rank1]: AttributeError: 'SupervisedDataset' object has no attribute 'shuffle'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank2]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank2]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path).shuffle()
[rank2]: AttributeError: 'SupervisedDataset' object has no attribute 'shuffle'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank2]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank2]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path).shuffle()
[rank2]: AttributeError: 'SupervisedDataset' object has no attribute 'shuffle'
W1011 06:36:50.654000 140117286433152 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1478852 closing signal SIGTERM
W1011 06:36:50.654000 140117286433152 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1478854 closing signal SIGTERM
E1011 06:36:52.081000 140117286433152 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 1478853) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-11_06:36:50
  host      : sphinx6.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1478853)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
SLURM_JOBID=8750283
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1011 06:39:11.462000 140244963975552 torch/distributed/run.py:779] 
W1011 06:39:11.462000 140244963975552 torch/distributed/run.py:779] *****************************************
W1011 06:39:11.462000 140244963975552 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1011 06:39:11.462000 140244963975552 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it]
loaded
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.69s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
loaded
loaded
smart embedding
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank1]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank1]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, shuffle = True)
[rank1]: TypeError: SupervisedDataset.__init__() got an unexpected keyword argument 'shuffle'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank1]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank1]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, shuffle = True)
[rank1]: TypeError: SupervisedDataset.__init__() got an unexpected keyword argument 'shuffle'
smart embedding
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank2]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank2]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, shuffle = True)
[rank2]: TypeError: SupervisedDataset.__init__() got an unexpected keyword argument 'shuffle'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank2]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank2]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, shuffle = True)
[rank2]: TypeError: SupervisedDataset.__init__() got an unexpected keyword argument 'shuffle'
smart embedding
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank0]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank0]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, shuffle = True)
[rank0]: TypeError: SupervisedDataset.__init__() got an unexpected keyword argument 'shuffle'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 261, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 251, in train
[rank0]:     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 209, in make_supervised_data_module
[rank0]:     train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, shuffle = True)
[rank0]: TypeError: SupervisedDataset.__init__() got an unexpected keyword argument 'shuffle'
W1011 06:39:54.674000 140244963975552 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1480818 closing signal SIGTERM
W1011 06:39:54.675000 140244963975552 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1480820 closing signal SIGTERM
E1011 06:39:56.149000 140244963975552 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 1480819) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-11_06:39:54
  host      : sphinx6.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1480819)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
SLURM_JOBID=8750285
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1011 06:43:34.886000 139877969174912 torch/distributed/run.py:779] 
W1011 06:43:34.886000 139877969174912 torch/distributed/run.py:779] *****************************************
W1011 06:43:34.886000 139877969174912 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1011 06:43:34.886000 139877969174912 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.56s/it]The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.51s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.62s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.13s/it]
loaded
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.30s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.46s/it]loaded
Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
load trainer
load trainer
load trainer
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241011_064500-ns1q3thw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /nlp/scr/qinanyu/model_cache/result_alpaca_abortion
wandb: ⭐️ View project at https://wandb.ai/limber/huggingface
wandb: 🚀 View run at https://wandb.ai/limber/huggingface/runs/ns1q3thw
  0%|          | 0/624 [00:00<?, ?it/s]  0%|          | 1/624 [00:09<1:37:36,  9.40s/it]                                                 {'loss': 1.5443, 'grad_norm': 13.711688041687012, 'learning_rate': 1.0526315789473685e-06, 'epoch': 0.0}
  0%|          | 1/624 [00:09<1:37:36,  9.40s/it]  0%|          | 2/624 [00:14<1:10:28,  6.80s/it]                                                 {'loss': 1.8011, 'grad_norm': 16.48759651184082, 'learning_rate': 2.105263157894737e-06, 'epoch': 0.01}
  0%|          | 2/624 [00:14<1:10:28,  6.80s/it]  0%|          | 3/624 [00:19<1:02:39,  6.05s/it]                                                 {'loss': 1.72, 'grad_norm': 14.790363311767578, 'learning_rate': 3.157894736842105e-06, 'epoch': 0.01}
  0%|          | 3/624 [00:19<1:02:39,  6.05s/it]  1%|          | 4/624 [00:25<1:00:16,  5.83s/it]                                                 {'loss': 1.4508, 'grad_norm': 10.746084213256836, 'learning_rate': 4.210526315789474e-06, 'epoch': 0.02}
  1%|          | 4/624 [00:25<1:00:16,  5.83s/it]  1%|          | 5/624 [00:30<57:59,  5.62s/it]                                                 {'loss': 1.5387, 'grad_norm': 13.567031860351562, 'learning_rate': 5.263157894736842e-06, 'epoch': 0.02}
  1%|          | 5/624 [00:30<57:59,  5.62s/it]  1%|          | 6/624 [00:35<56:51,  5.52s/it]                                               {'loss': 1.6567, 'grad_norm': 9.511542320251465, 'learning_rate': 6.31578947368421e-06, 'epoch': 0.03}
  1%|          | 6/624 [00:35<56:51,  5.52s/it]  1%|          | 7/624 [00:41<56:50,  5.53s/it]                                               {'loss': 1.4471, 'grad_norm': 8.098382949829102, 'learning_rate': 7.368421052631579e-06, 'epoch': 0.03}
  1%|          | 7/624 [00:41<56:50,  5.53s/it]  1%|▏         | 8/624 [00:46<55:41,  5.42s/it]                                               {'loss': 1.3782, 'grad_norm': 5.165233612060547, 'learning_rate': 8.421052631578948e-06, 'epoch': 0.04}
  1%|▏         | 8/624 [00:46<55:41,  5.42s/it]  1%|▏         | 9/624 [00:51<55:20,  5.40s/it]                                               {'loss': 1.4987, 'grad_norm': 4.88712739944458, 'learning_rate': 9.473684210526315e-06, 'epoch': 0.04}
  1%|▏         | 9/624 [00:51<55:20,  5.40s/it]  2%|▏         | 10/624 [00:57<55:19,  5.41s/it]                                                {'loss': 1.5379, 'grad_norm': 10.510316848754883, 'learning_rate': 1.0526315789473684e-05, 'epoch': 0.05}
  2%|▏         | 10/624 [00:57<55:19,  5.41s/it]  2%|▏         | 11/624 [01:02<55:29,  5.43s/it]                                                {'loss': 1.3549, 'grad_norm': 7.698081016540527, 'learning_rate': 1.1578947368421053e-05, 'epoch': 0.05}
  2%|▏         | 11/624 [01:02<55:29,  5.43s/it]  2%|▏         | 12/624 [01:07<54:46,  5.37s/it]                                                {'loss': 1.6299, 'grad_norm': 7.421231269836426, 'learning_rate': 1.263157894736842e-05, 'epoch': 0.06}
  2%|▏         | 12/624 [01:07<54:46,  5.37s/it]slurmstepd: error: *** JOB 8750285 ON sphinx6 CANCELLED AT 2024-10-11T06:46:15 ***
  2%|▏         | 13/624 [01:13<54:03,  5.31s/it]SLURM_JOBID=8750286
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1011 06:46:23.786000 139636161556864 torch/distributed/run.py:779] 
W1011 06:46:23.786000 139636161556864 torch/distributed/run.py:779] *****************************************
W1011 06:46:23.786000 139636161556864 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1011 06:46:23.786000 139636161556864 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
in train
in train
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.74s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.34s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:24,  8.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:15<00:15,  7.86s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.34s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.68s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:09,  9.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.85s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  6.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.04s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  6.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.04s/it]
loaded
Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  6.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.20s/it]
loaded
loaded
loaded
loaded
W1011 06:47:14.154000 139636161556864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484117 closing signal SIGTERM
W1011 06:47:14.155000 139636161556864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484118 closing signal SIGTERM
W1011 06:47:14.155000 139636161556864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484120 closing signal SIGTERM
W1011 06:47:14.155000 139636161556864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484121 closing signal SIGTERM
E1011 06:47:19.146000 139636161556864 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: -9) local_rank: 2 (pid: 1484119) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-11_06:47:14
  host      : sphinx6.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : -9 (pid: 1484119)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 1484119
========================================================
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=8750286.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
SLURM_JOBID=8750288
SLURM_JOB_NODELIST=sphinx6
SLURM_NNODES=1
SLURMTMPDIR=
working directory = /sailhome/qinanyu/sft/stanford_alpaca
W1011 06:48:00.118000 139967155655040 torch/distributed/run.py:779] 
W1011 06:48:00.118000 139967155655040 torch/distributed/run.py:779] *****************************************
W1011 06:48:00.118000 139967155655040 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1011 06:48:00.118000 139967155655040 torch/distributed/run.py:779] *****************************************
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
Token is valid (permission: fineGrained).
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
Your token has been saved to /afs/cs.stanford.edu/u/qinanyu/.cache/huggingface/token
Login successful
import work
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
wandb: Currently logged in as: qinan (limber). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/cs.stanford.edu/u/qinanyu/.netrc
in train
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.39s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/training_args.py:1886: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.57s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:13,  6.52s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.60s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.08s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.15s/it]
loaded
loaded
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.96s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.99s/it]
loaded
loaded
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
smart embedding
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
load trainer
load trainer
load trainer
load trainer
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241011_064954-yxgs4vbv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /nlp/scr/qinanyu/model_cache/result_alpaca_abortion
wandb: ⭐️ View project at https://wandb.ai/limber/huggingface
wandb: 🚀 View run at https://wandb.ai/limber/huggingface/runs/yxgs4vbv
  0%|          | 0/468 [00:00<?, ?it/s]  0%|          | 1/468 [00:09<1:14:34,  9.58s/it]                                                 {'loss': 1.7387, 'grad_norm': 17.299928665161133, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.01}
  0%|          | 1/468 [00:09<1:14:34,  9.58s/it]  0%|          | 2/468 [00:15<56:19,  7.25s/it]                                                 {'loss': 1.6459, 'grad_norm': 11.64317798614502, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.01}
  0%|          | 2/468 [00:15<56:19,  7.25s/it]  1%|          | 3/468 [00:20<50:07,  6.47s/it]                                               {'loss': 1.2931, 'grad_norm': 9.568846702575684, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}
  1%|          | 3/468 [00:20<50:07,  6.47s/it]  1%|          | 4/468 [00:26<46:56,  6.07s/it]                                               {'loss': 1.7158, 'grad_norm': 7.568187236785889, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.03}
  1%|          | 4/468 [00:26<46:56,  6.07s/it]  1%|          | 5/468 [00:31<45:27,  5.89s/it]                                               {'loss': 1.4083, 'grad_norm': 7.436946868896484, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.03}
  1%|          | 5/468 [00:31<45:27,  5.89s/it]  1%|▏         | 6/468 [00:37<45:00,  5.84s/it]                                               {'loss': 1.3986, 'grad_norm': 5.960572242736816, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.04}
  1%|▏         | 6/468 [00:37<45:00,  5.84s/it]  1%|▏         | 7/468 [00:43<44:31,  5.80s/it]                                               {'loss': 1.6042, 'grad_norm': 10.178298950195312, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.04}
  1%|▏         | 7/468 [00:43<44:31,  5.80s/it]  2%|▏         | 8/468 [00:48<43:50,  5.72s/it]                                               {'loss': 1.5729, 'grad_norm': 4.868322372436523, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.05}
  2%|▏         | 8/468 [00:48<43:50,  5.72s/it]  2%|▏         | 9/468 [00:54<43:13,  5.65s/it]                                               {'loss': 1.4512, 'grad_norm': 6.745266914367676, 'learning_rate': 1.2e-05, 'epoch': 0.06}
  2%|▏         | 9/468 [00:54<43:13,  5.65s/it]  2%|▏         | 10/468 [00:59<42:56,  5.63s/it]                                                {'loss': 1.2864, 'grad_norm': 5.004669189453125, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.06}
  2%|▏         | 10/468 [00:59<42:56,  5.63s/it]  2%|▏         | 11/468 [01:05<42:35,  5.59s/it]                                                {'loss': 1.3604, 'grad_norm': 5.078942775726318, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.07}
  2%|▏         | 11/468 [01:05<42:35,  5.59s/it]  3%|▎         | 12/468 [01:10<42:13,  5.55s/it]                                                {'loss': 1.4445, 'grad_norm': 5.628371238708496, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.08}
  3%|▎         | 12/468 [01:10<42:13,  5.55s/it]  3%|▎         | 13/468 [01:16<42:13,  5.57s/it]                                                {'loss': 1.5392, 'grad_norm': 5.064407825469971, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.08}
  3%|▎         | 13/468 [01:16<42:13,  5.57s/it]  3%|▎         | 14/468 [01:22<42:12,  5.58s/it]                                                {'loss': 1.4124, 'grad_norm': 4.898659706115723, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.09}
  3%|▎         | 14/468 [01:22<42:12,  5.58s/it]  3%|▎         | 15/468 [01:27<42:09,  5.58s/it]                                                {'loss': 1.4739, 'grad_norm': 5.302660942077637, 'learning_rate': 2e-05, 'epoch': 0.1}
  3%|▎         | 15/468 [01:27<42:09,  5.58s/it]  3%|▎         | 16/468 [01:33<42:17,  5.61s/it]                                                {'loss': 1.463, 'grad_norm': 7.119004726409912, 'learning_rate': 1.9999759524074374e-05, 'epoch': 0.1}
  3%|▎         | 16/468 [01:33<42:17,  5.61s/it]  4%|▎         | 17/468 [01:38<41:58,  5.58s/it]                                                {'loss': 1.4679, 'grad_norm': 4.394078254699707, 'learning_rate': 1.999903810786324e-05, 'epoch': 0.11}
  4%|▎         | 17/468 [01:38<41:58,  5.58s/it]  4%|▍         | 18/468 [01:44<41:39,  5.55s/it]                                                {'loss': 1.3922, 'grad_norm': 4.733721733093262, 'learning_rate': 1.999783578606323e-05, 'epoch': 0.12}
  4%|▍         | 18/468 [01:44<41:39,  5.55s/it]  4%|▍         | 19/468 [01:49<41:33,  5.55s/it]                                                {'loss': 1.5743, 'grad_norm': 4.579777240753174, 'learning_rate': 1.9996152616500244e-05, 'epoch': 0.12}
  4%|▍         | 19/468 [01:49<41:33,  5.55s/it]  4%|▍         | 20/468 [01:55<42:07,  5.64s/it]                                                {'loss': 1.3719, 'grad_norm': 4.445603370666504, 'learning_rate': 1.999398868012663e-05, 'epoch': 0.13}
  4%|▍         | 20/468 [01:55<42:07,  5.64s/it]  4%|▍         | 21/468 [02:01<41:28,  5.57s/it]                                                {'loss': 1.6, 'grad_norm': 6.940543174743652, 'learning_rate': 1.9991344081017312e-05, 'epoch': 0.13}
  4%|▍         | 21/468 [02:01<41:28,  5.57s/it]  5%|▍         | 22/468 [02:06<41:52,  5.63s/it]                                                {'loss': 1.3673, 'grad_norm': 6.035294532775879, 'learning_rate': 1.998821894636477e-05, 'epoch': 0.14}
  5%|▍         | 22/468 [02:06<41:52,  5.63s/it]  5%|▍         | 23/468 [02:12<42:16,  5.70s/it]                                                {'loss': 1.365, 'grad_norm': 4.312431812286377, 'learning_rate': 1.9984613426472934e-05, 'epoch': 0.15}
  5%|▍         | 23/468 [02:12<42:16,  5.70s/it]  5%|▌         | 24/468 [02:18<41:52,  5.66s/it]                                                {'loss': 1.4045, 'grad_norm': 4.25860071182251, 'learning_rate': 1.9980527694749952e-05, 'epoch': 0.15}
  5%|▌         | 24/468 [02:18<41:52,  5.66s/it]  5%|▌         | 25/468 [02:23<41:17,  5.59s/it]                                                {'loss': 1.5046, 'grad_norm': 5.709314346313477, 'learning_rate': 1.9975961947699848e-05, 'epoch': 0.16}
  5%|▌         | 25/468 [02:23<41:17,  5.59s/it]  6%|▌         | 26/468 [02:29<41:13,  5.60s/it]                                                {'loss': 1.4611, 'grad_norm': 4.155947208404541, 'learning_rate': 1.9970916404913068e-05, 'epoch': 0.17}
  6%|▌         | 26/468 [02:29<41:13,  5.60s/it]  6%|▌         | 27/468 [02:35<41:19,  5.62s/it]                                                {'loss': 1.5161, 'grad_norm': 4.015580654144287, 'learning_rate': 1.996539130905593e-05, 'epoch': 0.17}
  6%|▌         | 27/468 [02:35<41:19,  5.62s/it]  6%|▌         | 28/468 [02:40<40:50,  5.57s/it]                                                {'loss': 1.3815, 'grad_norm': 5.5053391456604, 'learning_rate': 1.9959386925858944e-05, 'epoch': 0.18}
  6%|▌         | 28/468 [02:40<40:50,  5.57s/it]  6%|▌         | 29/468 [02:46<40:41,  5.56s/it]                                                {'loss': 1.4103, 'grad_norm': 4.281830787658691, 'learning_rate': 1.9952903544104026e-05, 'epoch': 0.19}
  6%|▌         | 29/468 [02:46<40:41,  5.56s/it]  6%|▋         | 30/468 [02:51<41:01,  5.62s/it]                                                {'loss': 1.5075, 'grad_norm': 5.795373916625977, 'learning_rate': 1.9945941475610623e-05, 'epoch': 0.19}
  6%|▋         | 30/468 [02:51<41:01,  5.62s/it]  7%|▋         | 31/468 [02:57<41:00,  5.63s/it]                                                {'loss': 1.3121, 'grad_norm': 5.083995342254639, 'learning_rate': 1.9938501055220712e-05, 'epoch': 0.2}
  7%|▋         | 31/468 [02:57<41:00,  5.63s/it]  7%|▋         | 32/468 [03:03<40:59,  5.64s/it]                                                {'loss': 1.4148, 'grad_norm': 6.439355850219727, 'learning_rate': 1.9930582640782684e-05, 'epoch': 0.2}
  7%|▋         | 32/468 [03:03<40:59,  5.64s/it]  7%|▋         | 33/468 [03:08<40:59,  5.65s/it]                                                {'loss': 1.5105, 'grad_norm': 4.553835391998291, 'learning_rate': 1.9922186613134152e-05, 'epoch': 0.21}
  7%|▋         | 33/468 [03:08<40:59,  5.65s/it]  7%|▋         | 34/468 [03:14<41:15,  5.70s/it]                                                {'loss': 1.4614, 'grad_norm': 4.460733413696289, 'learning_rate': 1.9913313376083615e-05, 'epoch': 0.22}
  7%|▋         | 34/468 [03:14<41:15,  5.70s/it]  7%|▋         | 35/468 [03:20<41:08,  5.70s/it]                                                {'loss': 1.5634, 'grad_norm': 4.427824020385742, 'learning_rate': 1.9903963356391057e-05, 'epoch': 0.22}
  7%|▋         | 35/468 [03:20<41:08,  5.70s/it]  8%|▊         | 36/468 [03:25<40:30,  5.63s/it]                                                {'loss': 1.3749, 'grad_norm': 5.760916709899902, 'learning_rate': 1.9894137003747404e-05, 'epoch': 0.23}
  8%|▊         | 36/468 [03:25<40:30,  5.63s/it]  8%|▊         | 37/468 [03:31<40:13,  5.60s/it]                                                {'loss': 1.4196, 'grad_norm': 5.300926208496094, 'learning_rate': 1.9883834790752904e-05, 'epoch': 0.24}
  8%|▊         | 37/468 [03:31<40:13,  5.60s/it]  8%|▊         | 38/468 [03:36<40:12,  5.61s/it]                                                {'loss': 1.378, 'grad_norm': 4.657336235046387, 'learning_rate': 1.98730572128944e-05, 'epoch': 0.24}
  8%|▊         | 38/468 [03:37<40:12,  5.61s/it]  8%|▊         | 39/468 [03:42<39:45,  5.56s/it]                                                {'loss': 1.3814, 'grad_norm': 4.531696319580078, 'learning_rate': 1.986180478852149e-05, 'epoch': 0.25}
  8%|▊         | 39/468 [03:42<39:45,  5.56s/it]  9%|▊         | 40/468 [03:47<39:48,  5.58s/it]                                                {'loss': 1.4342, 'grad_norm': 4.279449462890625, 'learning_rate': 1.9850078058821615e-05, 'epoch': 0.26}
  9%|▊         | 40/468 [03:48<39:48,  5.58s/it]  9%|▉         | 41/468 [03:53<40:01,  5.62s/it]                                                {'loss': 1.4325, 'grad_norm': 4.323545455932617, 'learning_rate': 1.9837877587794003e-05, 'epoch': 0.26}
  9%|▉         | 41/468 [03:53<40:01,  5.62s/it]  9%|▉         | 42/468 [03:59<40:13,  5.67s/it]                                                {'loss': 1.2427, 'grad_norm': 4.561918258666992, 'learning_rate': 1.9825203962222573e-05, 'epoch': 0.27}
  9%|▉         | 42/468 [03:59<40:13,  5.67s/it]  9%|▉         | 43/468 [04:05<40:11,  5.67s/it]                                                {'loss': 1.3871, 'grad_norm': 4.856410026550293, 'learning_rate': 1.9812057791647687e-05, 'epoch': 0.27}
  9%|▉         | 43/468 [04:05<40:11,  5.67s/it]  9%|▉         | 44/468 [04:10<40:07,  5.68s/it]                                                {'loss': 1.4683, 'grad_norm': 4.519903182983398, 'learning_rate': 1.979843970833686e-05, 'epoch': 0.28}
  9%|▉         | 44/468 [04:10<40:07,  5.68s/it] 10%|▉         | 45/468 [04:16<40:06,  5.69s/it]                                                {'loss': 1.4397, 'grad_norm': 4.024024963378906, 'learning_rate': 1.9784350367254322e-05, 'epoch': 0.29}
 10%|▉         | 45/468 [04:16<40:06,  5.69s/it] 10%|▉         | 46/468 [04:22<40:02,  5.69s/it]                                                {'loss': 1.3587, 'grad_norm': 4.3425679206848145, 'learning_rate': 1.9769790446029543e-05, 'epoch': 0.29}
 10%|▉         | 46/468 [04:22<40:02,  5.69s/it] 10%|█         | 47/468 [04:27<39:30,  5.63s/it]                                                {'loss': 1.468, 'grad_norm': 4.430211544036865, 'learning_rate': 1.9754760644924635e-05, 'epoch': 0.3}
 10%|█         | 47/468 [04:27<39:30,  5.63s/it] 10%|█         | 48/468 [04:33<39:14,  5.61s/it]                                                {'loss': 1.2081, 'grad_norm': 4.010322093963623, 'learning_rate': 1.9739261686800662e-05, 'epoch': 0.31}
 10%|█         | 48/468 [04:33<39:14,  5.61s/it] 10%|█         | 49/468 [04:38<39:04,  5.59s/it]                                                {'loss': 1.2201, 'grad_norm': 3.9512972831726074, 'learning_rate': 1.9723294317082876e-05, 'epoch': 0.31}
 10%|█         | 49/468 [04:38<39:04,  5.59s/it] 11%|█         | 50/468 [04:44<39:07,  5.62s/it]                                                {'loss': 1.3088, 'grad_norm': 4.132850170135498, 'learning_rate': 1.970685930372489e-05, 'epoch': 0.32}
 11%|█         | 50/468 [04:44<39:07,  5.62s/it] 11%|█         | 51/468 [04:50<38:49,  5.59s/it]                                                {'loss': 1.4873, 'grad_norm': 4.258707523345947, 'learning_rate': 1.968995743717171e-05, 'epoch': 0.33}
 11%|█         | 51/468 [04:50<38:49,  5.59s/it] 11%|█         | 52/468 [04:55<38:52,  5.61s/it]                                                {'loss': 1.3159, 'grad_norm': 4.654259204864502, 'learning_rate': 1.967258953032174e-05, 'epoch': 0.33}
 11%|█         | 52/468 [04:55<38:52,  5.61s/it] 11%|█▏        | 53/468 [05:01<39:23,  5.70s/it]                                                {'loss': 1.6135, 'grad_norm': 3.889216184616089, 'learning_rate': 1.965475641848767e-05, 'epoch': 0.34}
 11%|█▏        | 53/468 [05:01<39:23,  5.70s/it] 12%|█▏        | 54/468 [05:07<39:02,  5.66s/it]                                                {'loss': 1.3325, 'grad_norm': 4.140605449676514, 'learning_rate': 1.963645895935632e-05, 'epoch': 0.35}
 12%|█▏        | 54/468 [05:07<39:02,  5.66s/it] 12%|█▏        | 55/468 [05:12<38:21,  5.57s/it]                                                {'loss': 1.3561, 'grad_norm': 4.008365154266357, 'learning_rate': 1.9617698032947364e-05, 'epoch': 0.35}
 12%|█▏        | 55/468 [05:12<38:21,  5.57s/it] 12%|█▏        | 56/468 [05:17<37:48,  5.51s/it]                                                {'loss': 1.4909, 'grad_norm': 3.9656999111175537, 'learning_rate': 1.9598474541571045e-05, 'epoch': 0.36}
 12%|█▏        | 56/468 [05:18<37:48,  5.51s/it] 12%|█▏        | 57/468 [05:23<38:08,  5.57s/it]                                                {'loss': 1.3004, 'grad_norm': 5.139795303344727, 'learning_rate': 1.9578789409784727e-05, 'epoch': 0.36}
 12%|█▏        | 57/468 [05:23<38:08,  5.57s/it] 12%|█▏        | 58/468 [05:29<37:54,  5.55s/it]                                                {'loss': 1.461, 'grad_norm': 4.764914035797119, 'learning_rate': 1.9558643584348478e-05, 'epoch': 0.37}
 12%|█▏        | 58/468 [05:29<37:54,  5.55s/it] 13%|█▎        | 59/468 [05:34<37:46,  5.54s/it]                                                {'loss': 1.5718, 'grad_norm': 5.670238494873047, 'learning_rate': 1.9538038034179496e-05, 'epoch': 0.38}
 13%|█▎        | 59/468 [05:34<37:46,  5.54s/it] 13%|█▎        | 60/468 [05:40<37:43,  5.55s/it]                                                {'loss': 1.3935, 'grad_norm': 4.659449100494385, 'learning_rate': 1.951697375030553e-05, 'epoch': 0.38}
 13%|█▎        | 60/468 [05:40<37:43,  5.55s/it] 13%|█▎        | 61/468 [05:45<38:00,  5.60s/it]                                                {'loss': 1.1572, 'grad_norm': 4.518929481506348, 'learning_rate': 1.949545174581722e-05, 'epoch': 0.39}
 13%|█▎        | 61/468 [05:46<38:00,  5.60s/it] 13%|█▎        | 62/468 [05:51<38:03,  5.63s/it]                                                {'loss': 1.2651, 'grad_norm': 4.184192657470703, 'learning_rate': 1.9473473055819348e-05, 'epoch': 0.4}
 13%|█▎        | 62/468 [05:51<38:03,  5.63s/it] 13%|█▎        | 63/468 [05:56<37:20,  5.53s/it]                                                {'loss': 1.4623, 'grad_norm': 4.47421932220459, 'learning_rate': 1.9451038737381078e-05, 'epoch': 0.4}
 13%|█▎        | 63/468 [05:57<37:20,  5.53s/it] 14%|█▎        | 64/468 [06:02<36:55,  5.48s/it]                                                {'loss': 1.3382, 'grad_norm': 4.6155219078063965, 'learning_rate': 1.9428149869485113e-05, 'epoch': 0.41}
 14%|█▎        | 64/468 [06:02<36:55,  5.48s/it] 14%|█▍        | 65/468 [06:08<37:25,  5.57s/it]                                                {'loss': 1.3594, 'grad_norm': 4.847157001495361, 'learning_rate': 1.940480755297579e-05, 'epoch': 0.42}
 14%|█▍        | 65/468 [06:08<37:25,  5.57s/it] 14%|█▍        | 66/468 [06:13<37:49,  5.65s/it]                                                {'loss': 1.113, 'grad_norm': 4.479552745819092, 'learning_rate': 1.9381012910506146e-05, 'epoch': 0.42}
 14%|█▍        | 66/468 [06:14<37:49,  5.65s/it] 14%|█▍        | 67/468 [06:19<37:44,  5.65s/it]                                                {'loss': 1.3667, 'grad_norm': 4.876309871673584, 'learning_rate': 1.9356767086483908e-05, 'epoch': 0.43}
 14%|█▍        | 67/468 [06:19<37:44,  5.65s/it] 15%|█▍        | 68/468 [06:25<37:56,  5.69s/it]                                                {'loss': 1.3437, 'grad_norm': 4.6417059898376465, 'learning_rate': 1.9332071247016476e-05, 'epoch': 0.43}
 15%|█▍        | 68/468 [06:25<37:56,  5.69s/it] 15%|█▍        | 69/468 [06:31<38:04,  5.72s/it]                                                {'loss': 1.4744, 'grad_norm': 4.777535438537598, 'learning_rate': 1.930692657985482e-05, 'epoch': 0.44}
 15%|█▍        | 69/468 [06:31<38:04,  5.72s/it] 15%|█▍        | 70/468 [06:36<37:50,  5.70s/it]                                                {'loss': 1.3808, 'grad_norm': 5.523942947387695, 'learning_rate': 1.9281334294336364e-05, 'epoch': 0.45}
 15%|█▍        | 70/468 [06:36<37:50,  5.70s/it] 15%|█▌        | 71/468 [06:42<37:29,  5.67s/it]                                                {'loss': 1.2512, 'grad_norm': 4.375326633453369, 'learning_rate': 1.9255295621326813e-05, 'epoch': 0.45}
 15%|█▌        | 71/468 [06:42<37:29,  5.67s/it] 15%|█▌        | 72/468 [06:47<37:12,  5.64s/it]                                                {'loss': 1.4803, 'grad_norm': 7.861710071563721, 'learning_rate': 1.9228811813160972e-05, 'epoch': 0.46}
 15%|█▌        | 72/468 [06:48<37:12,  5.64s/it] 16%|█▌        | 73/468 [06:53<36:58,  5.62s/it]                                                {'loss': 1.4647, 'grad_norm': 4.148715496063232, 'learning_rate': 1.9201884143582496e-05, 'epoch': 0.47}
 16%|█▌        | 73/468 [06:53<36:58,  5.62s/it] 16%|█▌        | 74/468 [06:59<37:12,  5.67s/it]                                                {'loss': 1.2976, 'grad_norm': 4.430498123168945, 'learning_rate': 1.9174513907682634e-05, 'epoch': 0.47}
 16%|█▌        | 74/468 [06:59<37:12,  5.67s/it] 16%|█▌        | 75/468 [07:04<36:56,  5.64s/it]                                                {'loss': 1.3863, 'grad_norm': 4.273655414581299, 'learning_rate': 1.9146702421837952e-05, 'epoch': 0.48}
 16%|█▌        | 75/468 [07:04<36:56,  5.64s/it] 16%|█▌        | 76/468 [07:10<36:48,  5.63s/it]                                                {'loss': 1.3532, 'grad_norm': 4.607746601104736, 'learning_rate': 1.9118451023647008e-05, 'epoch': 0.49}
 16%|█▌        | 76/468 [07:10<36:48,  5.63s/it] 16%|█▋        | 77/468 [07:15<36:22,  5.58s/it]                                                {'loss': 1.1881, 'grad_norm': 4.522761344909668, 'learning_rate': 1.908976107186603e-05, 'epoch': 0.49}
 16%|█▋        | 77/468 [07:16<36:22,  5.58s/it] 17%|█▋        | 78/468 [07:21<36:43,  5.65s/it]                                                {'loss': 1.2823, 'grad_norm': 4.383029937744141, 'learning_rate': 1.906063394634356e-05, 'epoch': 0.5}
 17%|█▋        | 78/468 [07:21<36:43,  5.65s/it] 17%|█▋        | 79/468 [07:27<36:27,  5.62s/it]                                                {'loss': 1.3132, 'grad_norm': 8.431097030639648, 'learning_rate': 1.9031071047954095e-05, 'epoch': 0.5}
 17%|█▋        | 79/468 [07:27<36:27,  5.62s/it] 17%|█▋        | 80/468 [07:33<36:43,  5.68s/it]                                                {'loss': 1.4328, 'grad_norm': 5.066092014312744, 'learning_rate': 1.90010737985307e-05, 'epoch': 0.51}
 17%|█▋        | 80/468 [07:33<36:43,  5.68s/it] 17%|█▋        | 81/468 [07:39<36:59,  5.74s/it]                                                {'loss': 1.2817, 'grad_norm': 4.614785194396973, 'learning_rate': 1.8970643640796642e-05, 'epoch': 0.52}
 17%|█▋        | 81/468 [07:39<36:59,  5.74s/it] 18%|█▊        | 82/468 [07:44<36:22,  5.66s/it]                                                {'loss': 1.4322, 'grad_norm': 4.387228965759277, 'learning_rate': 1.893978203829599e-05, 'epoch': 0.52}
 18%|█▊        | 82/468 [07:44<36:22,  5.66s/it] 18%|█▊        | 83/468 [07:50<36:02,  5.62s/it]                                                {'loss': 1.3719, 'grad_norm': 4.143526077270508, 'learning_rate': 1.8908490475323234e-05, 'epoch': 0.53}
 18%|█▊        | 83/468 [07:50<36:02,  5.62s/it] 18%|█▊        | 84/468 [07:55<36:05,  5.64s/it]                                                {'loss': 1.1419, 'grad_norm': 4.549938201904297, 'learning_rate': 1.887677045685188e-05, 'epoch': 0.54}
 18%|█▊        | 84/468 [07:55<36:05,  5.64s/it] 18%|█▊        | 85/468 [08:01<35:33,  5.57s/it]                                                {'loss': 1.3427, 'grad_norm': 4.0003437995910645, 'learning_rate': 1.8844623508462093e-05, 'epoch': 0.54}
 18%|█▊        | 85/468 [08:01<35:33,  5.57s/it] 18%|█▊        | 86/468 [08:06<35:34,  5.59s/it]                                                {'loss': 1.4102, 'grad_norm': 5.140966892242432, 'learning_rate': 1.8812051176267307e-05, 'epoch': 0.55}
 18%|█▊        | 86/468 [08:06<35:34,  5.59s/it] 19%|█▊        | 87/468 [08:12<35:48,  5.64s/it]                                                {'loss': 1.2467, 'grad_norm': 4.1421942710876465, 'learning_rate': 1.877905502683987e-05, 'epoch': 0.56}
 19%|█▊        | 87/468 [08:12<35:48,  5.64s/it] 19%|█▉        | 88/468 [08:18<35:44,  5.64s/it]                                                {'loss': 1.2866, 'grad_norm': 4.419912338256836, 'learning_rate': 1.8745636647135693e-05, 'epoch': 0.56}
 19%|█▉        | 88/468 [08:18<35:44,  5.64s/it] 19%|█▉        | 89/468 [08:23<35:29,  5.62s/it]                                                {'loss': 1.4555, 'grad_norm': 4.077475547790527, 'learning_rate': 1.871179764441794e-05, 'epoch': 0.57}
 19%|█▉        | 89/468 [08:23<35:29,  5.62s/it] 19%|█▉        | 90/468 [08:29<35:15,  5.60s/it]                                                {'loss': 1.164, 'grad_norm': 3.954158067703247, 'learning_rate': 1.8677539646179706e-05, 'epoch': 0.58}
 19%|█▉        | 90/468 [08:29<35:15,  5.60s/it] 19%|█▉        | 91/468 [08:35<35:49,  5.70s/it]                                                {'loss': 1.3688, 'grad_norm': 4.1672892570495605, 'learning_rate': 1.8642864300065767e-05, 'epoch': 0.58}
 19%|█▉        | 91/468 [08:35<35:49,  5.70s/it] 20%|█▉        | 92/468 [08:40<35:32,  5.67s/it]                                                {'loss': 1.285, 'grad_norm': 5.22395133972168, 'learning_rate': 1.8607773273793298e-05, 'epoch': 0.59}
 20%|█▉        | 92/468 [08:40<35:32,  5.67s/it] 20%|█▉        | 93/468 [08:46<34:59,  5.60s/it]                                                {'loss': 1.2258, 'grad_norm': 4.001850605010986, 'learning_rate': 1.8572268255071718e-05, 'epoch': 0.59}
 20%|█▉        | 93/468 [08:46<34:59,  5.60s/it] 20%|██        | 94/468 [08:51<35:01,  5.62s/it]                                                {'loss': 1.2263, 'grad_norm': 4.446313381195068, 'learning_rate': 1.853635095152147e-05, 'epoch': 0.6}
 20%|██        | 94/468 [08:52<35:01,  5.62s/it] 20%|██        | 95/468 [08:57<35:00,  5.63s/it]                                                {'loss': 1.2525, 'grad_norm': 3.961857795715332, 'learning_rate': 1.8500023090591917e-05, 'epoch': 0.61}
 20%|██        | 95/468 [08:57<35:00,  5.63s/it] 21%|██        | 96/468 [09:03<35:24,  5.71s/it]                                                {'loss': 1.3843, 'grad_norm': 4.531850814819336, 'learning_rate': 1.8463286419478256e-05, 'epoch': 0.61}
 21%|██        | 96/468 [09:03<35:24,  5.71s/it] 21%|██        | 97/468 [09:09<35:19,  5.71s/it]                                                {'loss': 1.128, 'grad_norm': 3.945622444152832, 'learning_rate': 1.8426142705037487e-05, 'epoch': 0.62}
 21%|██        | 97/468 [09:09<35:19,  5.71s/it] 21%|██        | 98/468 [09:15<35:28,  5.75s/it]                                                {'loss': 1.209, 'grad_norm': 5.722453594207764, 'learning_rate': 1.8388593733703428e-05, 'epoch': 0.63}
 21%|██        | 98/468 [09:15<35:28,  5.75s/it] 21%|██        | 99/468 [09:20<35:26,  5.76s/it]                                                {'loss': 1.2918, 'grad_norm': 4.695511817932129, 'learning_rate': 1.8350641311400813e-05, 'epoch': 0.63}
 21%|██        | 99/468 [09:20<35:26,  5.76s/it] 21%|██▏       | 100/468 [09:26<34:43,  5.66s/it]                                                 {'loss': 1.4419, 'grad_norm': 4.524670600891113, 'learning_rate': 1.831228726345841e-05, 'epoch': 0.64}
 21%|██▏       | 100/468 [09:26<34:43,  5.66s/it] 22%|██▏       | 101/468 [09:32<35:00,  5.72s/it]                                                 {'loss': 1.3305, 'grad_norm': 5.107243537902832, 'learning_rate': 1.8273533434521262e-05, 'epoch': 0.65}
 22%|██▏       | 101/468 [09:32<35:00,  5.72s/it] 22%|██▏       | 102/468 [09:37<35:04,  5.75s/it]                                                 {'loss': 1.3324, 'grad_norm': 4.294907093048096, 'learning_rate': 1.8234381688461943e-05, 'epoch': 0.65}
 22%|██▏       | 102/468 [09:38<35:04,  5.75s/it] 22%|██▏       | 103/468 [09:43<34:52,  5.73s/it]                                                 {'loss': 1.18, 'grad_norm': 3.8260040283203125, 'learning_rate': 1.8194833908290933e-05, 'epoch': 0.66}
 22%|██▏       | 103/468 [09:43<34:52,  5.73s/it] 22%|██▏       | 104/468 [09:49<34:21,  5.66s/it]                                                 {'loss': 1.1571, 'grad_norm': 5.318776607513428, 'learning_rate': 1.815489199606603e-05, 'epoch': 0.66}
 22%|██▏       | 104/468 [09:49<34:21,  5.66s/it] 22%|██▏       | 105/468 [09:54<34:17,  5.67s/it]                                                 {'loss': 1.3076, 'grad_norm': 4.325337886810303, 'learning_rate': 1.8114557872800906e-05, 'epoch': 0.67}
 22%|██▏       | 105/468 [09:54<34:17,  5.67s/it] 23%|██▎       | 106/468 [10:00<34:00,  5.64s/it]                                                 {'loss': 1.3791, 'grad_norm': 4.8514885902404785, 'learning_rate': 1.8073833478372682e-05, 'epoch': 0.68}
 23%|██▎       | 106/468 [10:00<34:00,  5.64s/it] 23%|██▎       | 107/468 [10:05<33:40,  5.60s/it]                                                 {'loss': 1.1796, 'grad_norm': 4.003446102142334, 'learning_rate': 1.803272077142865e-05, 'epoch': 0.68}
 23%|██▎       | 107/468 [10:05<33:40,  5.60s/it] 23%|██▎       | 108/468 [10:11<34:32,  5.76s/it]                                                 {'loss': 0.8883, 'grad_norm': 3.7741477489471436, 'learning_rate': 1.799122172929206e-05, 'epoch': 0.69}
 23%|██▎       | 108/468 [10:12<34:32,  5.76s/it] 23%|██▎       | 109/468 [10:17<34:14,  5.72s/it]                                                 {'loss': 1.1595, 'grad_norm': 3.7771739959716797, 'learning_rate': 1.794933834786702e-05, 'epoch': 0.7}
 23%|██▎       | 109/468 [10:17<34:14,  5.72s/it] 24%|██▎       | 110/468 [10:23<33:58,  5.69s/it]                                                 {'loss': 1.0953, 'grad_norm': 3.707918405532837, 'learning_rate': 1.7907072641542527e-05, 'epoch': 0.7}
 24%|██▎       | 110/468 [10:23<33:58,  5.69s/it] 24%|██▎       | 111/468 [10:28<33:50,  5.69s/it]                                                 {'loss': 1.2678, 'grad_norm': 4.119504928588867, 'learning_rate': 1.7864426643095537e-05, 'epoch': 0.71}
 24%|██▎       | 111/468 [10:29<33:50,  5.69s/it] 24%|██▍       | 112/468 [10:34<33:44,  5.69s/it]                                                 {'loss': 1.5291, 'grad_norm': 4.096253871917725, 'learning_rate': 1.782140240359325e-05, 'epoch': 0.72}
 24%|██▍       | 112/468 [10:34<33:44,  5.69s/it] 24%|██▍       | 113/468 [10:40<33:21,  5.64s/it]                                                 {'loss': 1.4884, 'grad_norm': 5.246116638183594, 'learning_rate': 1.7778001992294426e-05, 'epoch': 0.72}
 24%|██▍       | 113/468 [10:40<33:21,  5.64s/it] 24%|██▍       | 114/468 [10:45<32:36,  5.53s/it]                                                 {'loss': 1.1981, 'grad_norm': 4.808418273925781, 'learning_rate': 1.773422749654988e-05, 'epoch': 0.73}
 24%|██▍       | 114/468 [10:45<32:36,  5.53s/it] 25%|██▍       | 115/468 [10:51<32:41,  5.56s/it]                                                 {'loss': 1.163, 'grad_norm': 4.364627361297607, 'learning_rate': 1.769008102170209e-05, 'epoch': 0.73}
 25%|██▍       | 115/468 [10:51<32:41,  5.56s/it] 25%|██▍       | 116/468 [10:56<32:09,  5.48s/it]                                                 {'loss': 1.3133, 'grad_norm': 4.617801666259766, 'learning_rate': 1.7645564690983936e-05, 'epoch': 0.74}
 25%|██▍       | 116/468 [10:56<32:09,  5.48s/it] 25%|██▌       | 117/468 [11:02<32:31,  5.56s/it]                                                 {'loss': 1.1349, 'grad_norm': 4.656129837036133, 'learning_rate': 1.7600680645416583e-05, 'epoch': 0.75}
 25%|██▌       | 117/468 [11:02<32:31,  5.56s/it] 25%|██▌       | 118/468 [11:07<32:34,  5.58s/it]                                                 {'loss': 1.1329, 'grad_norm': 4.580706596374512, 'learning_rate': 1.7555431043706517e-05, 'epoch': 0.75}
 25%|██▌       | 118/468 [11:07<32:34,  5.58s/it] 25%|██▌       | 119/468 [11:13<32:42,  5.62s/it]                                                 {'loss': 1.1056, 'grad_norm': 4.1063432693481445, 'learning_rate': 1.7509818062141704e-05, 'epoch': 0.76}
 25%|██▌       | 119/468 [11:13<32:42,  5.62s/it] 26%|██▌       | 120/468 [11:19<32:37,  5.62s/it]                                                 {'loss': 1.2863, 'grad_norm': 4.108471870422363, 'learning_rate': 1.746384389448694e-05, 'epoch': 0.77}
 26%|██▌       | 120/468 [11:19<32:37,  5.62s/it] 26%|██▌       | 121/468 [11:24<32:43,  5.66s/it]                                                 {'loss': 1.0335, 'grad_norm': 3.9432477951049805, 'learning_rate': 1.7417510751878324e-05, 'epoch': 0.77}
 26%|██▌       | 121/468 [11:24<32:43,  5.66s/it] 26%|██▌       | 122/468 [11:30<32:15,  5.59s/it]                                                 {'loss': 1.0992, 'grad_norm': 3.788173198699951, 'learning_rate': 1.737082086271693e-05, 'epoch': 0.78}
 26%|██▌       | 122/468 [11:30<32:15,  5.59s/it] 26%|██▋       | 123/468 [11:35<31:43,  5.52s/it]                                                 {'loss': 1.094, 'grad_norm': 4.0544233322143555, 'learning_rate': 1.7323776472561625e-05, 'epoch': 0.79}
 26%|██▋       | 123/468 [11:35<31:43,  5.52s/it] 26%|██▋       | 124/468 [11:41<31:55,  5.57s/it]                                                 {'loss': 1.3818, 'grad_norm': 4.203092098236084, 'learning_rate': 1.727637984402106e-05, 'epoch': 0.79}
 26%|██▋       | 124/468 [11:41<31:55,  5.57s/it] 27%|██▋       | 125/468 [11:46<31:49,  5.57s/it]                                                 {'loss': 1.0934, 'grad_norm': 4.453300952911377, 'learning_rate': 1.7228633256644854e-05, 'epoch': 0.8}
 27%|██▋       | 125/468 [11:46<31:49,  5.57s/it] 27%|██▋       | 126/468 [11:52<32:03,  5.63s/it]                                                 {'loss': 1.0012, 'grad_norm': 4.211502552032471, 'learning_rate': 1.7180539006813973e-05, 'epoch': 0.81}
 27%|██▋       | 126/468 [11:52<32:03,  5.63s/it] 27%|██▋       | 127/468 [11:58<32:13,  5.67s/it]                                                 {'loss': 1.0406, 'grad_norm': 4.076357841491699, 'learning_rate': 1.713209940763026e-05, 'epoch': 0.81}
 27%|██▋       | 127/468 [11:58<32:13,  5.67s/it] 27%|██▋       | 128/468 [12:04<32:05,  5.66s/it]                                                 {'loss': 1.2105, 'grad_norm': 4.783620357513428, 'learning_rate': 1.7083316788805212e-05, 'epoch': 0.82}
 27%|██▋       | 128/468 [12:04<32:05,  5.66s/it] 28%|██▊       | 129/468 [12:09<31:52,  5.64s/it]                                                 {'loss': 1.0845, 'grad_norm': 4.9115071296691895, 'learning_rate': 1.7034193496547903e-05, 'epoch': 0.82}
 28%|██▊       | 129/468 [12:09<31:52,  5.64s/it] 28%|██▊       | 130/468 [12:15<31:41,  5.63s/it]                                                 {'loss': 1.1488, 'grad_norm': 4.319429874420166, 'learning_rate': 1.6984731893452174e-05, 'epoch': 0.83}
 28%|██▊       | 130/468 [12:15<31:41,  5.63s/it] 28%|██▊       | 131/468 [12:20<31:52,  5.67s/it]                                                 {'loss': 0.8177, 'grad_norm': 4.679932117462158, 'learning_rate': 1.6934934358382987e-05, 'epoch': 0.84}
 28%|██▊       | 131/468 [12:21<31:52,  5.67s/it] 28%|██▊       | 132/468 [12:26<31:25,  5.61s/it]                                                 {'loss': 1.0843, 'grad_norm': 4.93923282623291, 'learning_rate': 1.6884803286362e-05, 'epoch': 0.84}
 28%|██▊       | 132/468 [12:26<31:25,  5.61s/it] 28%|██▊       | 133/468 [12:31<31:10,  5.58s/it]                                                 {'loss': 1.0546, 'grad_norm': 5.080621242523193, 'learning_rate': 1.683434108845241e-05, 'epoch': 0.85}
 28%|██▊       | 133/468 [12:32<31:10,  5.58s/it] 29%|██▊       | 134/468 [12:37<31:10,  5.60s/it]                                                 {'loss': 1.1687, 'grad_norm': 4.1427001953125, 'learning_rate': 1.6783550191642962e-05, 'epoch': 0.86}
 29%|██▊       | 134/468 [12:37<31:10,  5.60s/it] 29%|██▉       | 135/468 [12:43<31:00,  5.59s/it]                                                 {'loss': 1.3039, 'grad_norm': 3.731973886489868, 'learning_rate': 1.6732433038731245e-05, 'epoch': 0.86}
 29%|██▉       | 135/468 [12:43<31:00,  5.59s/it] 29%|██▉       | 136/468 [12:48<30:55,  5.59s/it]                                                 {'loss': 1.0532, 'grad_norm': 3.632979393005371, 'learning_rate': 1.668099208820619e-05, 'epoch': 0.87}
 29%|██▉       | 136/468 [12:48<30:55,  5.59s/it] 29%|██▉       | 137/468 [12:54<30:18,  5.49s/it]                                                 {'loss': 1.1007, 'grad_norm': 4.019385814666748, 'learning_rate': 1.662922981412983e-05, 'epoch': 0.88}
 29%|██▉       | 137/468 [12:54<30:18,  5.49s/it] 29%|██▉       | 138/468 [12:59<30:31,  5.55s/it]                                                 {'loss': 1.0337, 'grad_norm': 4.311468124389648, 'learning_rate': 1.657714870601833e-05, 'epoch': 0.88}
 29%|██▉       | 138/468 [12:59<30:31,  5.55s/it] 30%|██▉       | 139/468 [13:05<30:09,  5.50s/it]                                                 {'loss': 1.1515, 'grad_norm': 4.2865705490112305, 'learning_rate': 1.6524751268722216e-05, 'epoch': 0.89}
 30%|██▉       | 139/468 [13:05<30:09,  5.50s/it] 30%|██▉       | 140/468 [13:10<30:20,  5.55s/it]                                                 {'loss': 1.0655, 'grad_norm': 3.557605743408203, 'learning_rate': 1.647204002230594e-05, 'epoch': 0.89}
 30%|██▉       | 140/468 [13:10<30:20,  5.55s/it] 30%|███       | 141/468 [13:16<30:19,  5.56s/it]                                                 {'loss': 1.0886, 'grad_norm': 4.753520488739014, 'learning_rate': 1.641901750192666e-05, 'epoch': 0.9}
 30%|███       | 141/468 [13:16<30:19,  5.56s/it] 30%|███       | 142/468 [13:21<30:12,  5.56s/it]                                                 {'loss': 1.1375, 'grad_norm': 4.390632152557373, 'learning_rate': 1.63656862577123e-05, 'epoch': 0.91}
 30%|███       | 142/468 [13:22<30:12,  5.56s/it] 31%|███       | 143/468 [13:27<30:19,  5.60s/it]                                                 {'loss': 1.1728, 'grad_norm': 4.178752899169922, 'learning_rate': 1.6312048854638927e-05, 'epoch': 0.91}
 31%|███       | 143/468 [13:27<30:19,  5.60s/it] 31%|███       | 144/468 [13:33<30:21,  5.62s/it]                                                 {'loss': 1.1072, 'grad_norm': 4.615017414093018, 'learning_rate': 1.6258107872407376e-05, 'epoch': 0.92}
 31%|███       | 144/468 [13:33<30:21,  5.62s/it] 31%|███       | 145/468 [13:38<30:20,  5.64s/it]                                                 {'loss': 1.1626, 'grad_norm': 3.864184617996216, 'learning_rate': 1.620386590531917e-05, 'epoch': 0.93}
 31%|███       | 145/468 [13:39<30:20,  5.64s/it] 31%|███       | 146/468 [13:44<30:04,  5.61s/it]                                                 {'loss': 1.2534, 'grad_norm': 4.227813243865967, 'learning_rate': 1.614932556215176e-05, 'epoch': 0.93}
 31%|███       | 146/468 [13:44<30:04,  5.61s/it] 31%|███▏      | 147/468 [13:50<30:17,  5.66s/it]                                                 {'loss': 1.0717, 'grad_norm': 3.9771714210510254, 'learning_rate': 1.609448946603304e-05, 'epoch': 0.94}
 31%|███▏      | 147/468 [13:50<30:17,  5.66s/it] 32%|███▏      | 148/468 [13:55<30:03,  5.64s/it]                                                 {'loss': 1.1489, 'grad_norm': 3.8744149208068848, 'learning_rate': 1.6039360254315213e-05, 'epoch': 0.95}
 32%|███▏      | 148/468 [13:55<30:03,  5.64s/it] 32%|███▏      | 149/468 [14:01<29:46,  5.60s/it]                                                 {'loss': 1.1707, 'grad_norm': 4.562150955200195, 'learning_rate': 1.598394057844792e-05, 'epoch': 0.95}
 32%|███▏      | 149/468 [14:01<29:46,  5.60s/it] 32%|███▏      | 150/468 [14:06<29:38,  5.59s/it]                                                 {'loss': 1.0021, 'grad_norm': 4.170925140380859, 'learning_rate': 1.592823310385073e-05, 'epoch': 0.96}
 32%|███▏      | 150/468 [14:07<29:38,  5.59s/it] 32%|███▏      | 151/468 [14:12<29:50,  5.65s/it]                                                 {'loss': 1.0829, 'grad_norm': 4.282622337341309, 'learning_rate': 1.5872240509784943e-05, 'epoch': 0.96}
 32%|███▏      | 151/468 [14:12<29:50,  5.65s/it] 32%|███▏      | 152/468 [14:18<29:38,  5.63s/it]                                                 {'loss': 1.091, 'grad_norm': 5.292291641235352, 'learning_rate': 1.5815965489224746e-05, 'epoch': 0.97}
 32%|███▏      | 152/468 [14:18<29:38,  5.63s/it] 33%|███▎      | 153/468 [14:23<29:27,  5.61s/it]                                                 {'loss': 1.0611, 'grad_norm': 4.528207778930664, 'learning_rate': 1.5759410748727663e-05, 'epoch': 0.98}
 33%|███▎      | 153/468 [14:23<29:27,  5.61s/it] 33%|███▎      | 154/468 [14:29<29:10,  5.58s/it]                                                 {'loss': 1.2024, 'grad_norm': 3.8254337310791016, 'learning_rate': 1.5702579008304403e-05, 'epoch': 0.98}
 33%|███▎      | 154/468 [14:29<29:10,  5.58s/it] 33%|███▎      | 155/468 [14:34<29:06,  5.58s/it]                                                 {'loss': 1.2224, 'grad_norm': 3.9295413494110107, 'learning_rate': 1.5645473001288057e-05, 'epoch': 0.99}
 33%|███▎      | 155/468 [14:35<29:06,  5.58s/it] 33%|███▎      | 156/468 [14:40<29:04,  5.59s/it]                                                 {'loss': 0.9841, 'grad_norm': 3.774704694747925, 'learning_rate': 1.5588095474202597e-05, 'epoch': 1.0}
 33%|███▎      | 156/468 [14:40<29:04,  5.59s/it] 34%|███▎      | 157/468 [14:46<28:51,  5.57s/it]                                                 {'loss': 1.166, 'grad_norm': 3.9849772453308105, 'learning_rate': 1.5530449186630805e-05, 'epoch': 1.0}
 34%|███▎      | 157/468 [14:46<28:51,  5.57s/it] 34%|███▍      | 158/468 [14:51<28:23,  5.50s/it]                                                 {'loss': 1.1979, 'grad_norm': 4.598723888397217, 'learning_rate': 1.547253691108156e-05, 'epoch': 1.01}
 34%|███▍      | 158/468 [14:51<28:23,  5.50s/it] 34%|███▍      | 159/468 [14:57<28:27,  5.53s/it]                                                 {'loss': 0.8416, 'grad_norm': 3.6481857299804688, 'learning_rate': 1.5414361432856475e-05, 'epoch': 1.02}
 34%|███▍      | 159/468 [14:57<28:27,  5.53s/it] 34%|███▍      | 160/468 [15:02<27:51,  5.43s/it]                                                 {'loss': 1.0306, 'grad_norm': 4.21428918838501, 'learning_rate': 1.5355925549915943e-05, 'epoch': 1.02}
 34%|███▍      | 160/468 [15:02<27:51,  5.43s/it] 34%|███▍      | 161/468 [15:07<28:10,  5.51s/it]                                                 {'loss': 0.8926, 'grad_norm': 4.036348819732666, 'learning_rate': 1.5297232072744576e-05, 'epoch': 1.03}
 34%|███▍      | 161/468 [15:07<28:10,  5.51s/it] 35%|███▍      | 162/468 [15:13<28:13,  5.53s/it]                                                 {'loss': 1.2002, 'grad_norm': 4.1331000328063965, 'learning_rate': 1.5238283824216015e-05, 'epoch': 1.04}
 35%|███▍      | 162/468 [15:13<28:13,  5.53s/it] 35%|███▍      | 163/468 [15:19<28:50,  5.67s/it]                                                 {'loss': 0.9227, 'grad_norm': 3.2574143409729004, 'learning_rate': 1.5179083639457193e-05, 'epoch': 1.04}
 35%|███▍      | 163/468 [15:19<28:50,  5.67s/it] 35%|███▌      | 164/468 [15:25<29:05,  5.74s/it]                                                 {'loss': 0.9779, 'grad_norm': 4.655989170074463, 'learning_rate': 1.5119634365711955e-05, 'epoch': 1.05}
 35%|███▌      | 164/468 [15:25<29:05,  5.74s/it] 35%|███▌      | 165/468 [15:31<28:51,  5.72s/it]                                                 {'loss': 0.9751, 'grad_norm': 4.007674217224121, 'learning_rate': 1.5059938862204126e-05, 'epoch': 1.05}
 35%|███▌      | 165/468 [15:31<28:51,  5.72s/it] 35%|███▌      | 166/468 [15:36<28:26,  5.65s/it]                                                 {'loss': 0.923, 'grad_norm': 4.217422962188721, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.06}
 35%|███▌      | 166/468 [15:36<28:26,  5.65s/it] 36%|███▌      | 167/468 [15:42<28:20,  5.65s/it]                                                 {'loss': 0.9794, 'grad_norm': 4.098721981048584, 'learning_rate': 1.4939820661870253e-05, 'epoch': 1.07}
 36%|███▌      | 167/468 [15:42<28:20,  5.65s/it] 36%|███▌      | 168/468 [15:47<28:22,  5.68s/it]                                                 {'loss': 0.9001, 'grad_norm': 4.004072666168213, 'learning_rate': 1.4879403742151283e-05, 'epoch': 1.07}
 36%|███▌      | 168/468 [15:48<28:22,  5.68s/it] 36%|███▌      | 169/468 [15:53<28:10,  5.65s/it]                                                 {'loss': 0.7291, 'grad_norm': 3.8152565956115723, 'learning_rate': 1.481875214660604e-05, 'epoch': 1.08}
 36%|███▌      | 169/468 [15:53<28:10,  5.65s/it] 36%|███▋      | 170/468 [15:58<27:40,  5.57s/it]                                                 {'loss': 0.9759, 'grad_norm': 4.159551620483398, 'learning_rate': 1.4757868792284231e-05, 'epoch': 1.09}
 36%|███▋      | 170/468 [15:59<27:40,  5.57s/it] 37%|███▋      | 171/468 [16:04<27:47,  5.61s/it]                                                 {'loss': 0.9822, 'grad_norm': 5.080440998077393, 'learning_rate': 1.469675660738206e-05, 'epoch': 1.09}
 37%|███▋      | 171/468 [16:04<27:47,  5.61s/it] 37%|███▋      | 172/468 [16:10<27:40,  5.61s/it]                                                 {'loss': 0.8273, 'grad_norm': 3.881629228591919, 'learning_rate': 1.463541853110137e-05, 'epoch': 1.1}
 37%|███▋      | 172/468 [16:10<27:40,  5.61s/it] 37%|███▋      | 173/468 [16:15<27:33,  5.60s/it]                                                 {'loss': 0.8726, 'grad_norm': 4.237768650054932, 'learning_rate': 1.4573857513508297e-05, 'epoch': 1.11}
 37%|███▋      | 173/468 [16:15<27:33,  5.60s/it] 37%|███▋      | 174/468 [16:21<27:13,  5.56s/it]                                                 {'loss': 0.9777, 'grad_norm': 4.299768924713135, 'learning_rate': 1.4512076515391375e-05, 'epoch': 1.11}
 37%|███▋      | 174/468 [16:21<27:13,  5.56s/it] 37%|███▋      | 175/468 [16:26<26:59,  5.53s/it]                                                 {'loss': 0.7989, 'grad_norm': 3.7921972274780273, 'learning_rate': 1.4450078508119148e-05, 'epoch': 1.12}
 37%|███▋      | 175/468 [16:26<26:59,  5.53s/it] 38%|███▊      | 176/468 [16:32<26:57,  5.54s/it]                                                 {'loss': 1.0051, 'grad_norm': 4.225880146026611, 'learning_rate': 1.4387866473497254e-05, 'epoch': 1.12}
 38%|███▊      | 176/468 [16:32<26:57,  5.54s/it] 38%|███▊      | 177/468 [16:37<27:05,  5.59s/it]                                                 {'loss': 0.8702, 'grad_norm': 4.270367622375488, 'learning_rate': 1.4325443403625012e-05, 'epoch': 1.13}
 38%|███▊      | 177/468 [16:38<27:05,  5.59s/it] 38%|███▊      | 178/468 [16:43<26:43,  5.53s/it]                                                 {'loss': 0.7467, 'grad_norm': 4.594254970550537, 'learning_rate': 1.4262812300751528e-05, 'epoch': 1.14}
 38%|███▊      | 178/468 [16:43<26:43,  5.53s/it] 38%|███▊      | 179/468 [16:48<26:43,  5.55s/it]                                                 {'loss': 0.758, 'grad_norm': 4.398962497711182, 'learning_rate': 1.4199976177131283e-05, 'epoch': 1.14}
 38%|███▊      | 179/468 [16:49<26:43,  5.55s/it] 38%|███▊      | 180/468 [16:54<26:41,  5.56s/it]                                                 {'loss': 0.9825, 'grad_norm': 4.487221717834473, 'learning_rate': 1.4136938054879284e-05, 'epoch': 1.15}
 38%|███▊      | 180/468 [16:54<26:41,  5.56s/it] 39%|███▊      | 181/468 [17:00<26:44,  5.59s/it]                                                 {'loss': 0.8036, 'grad_norm': 3.86767840385437, 'learning_rate': 1.4073700965825681e-05, 'epoch': 1.16}
 39%|███▊      | 181/468 [17:00<26:44,  5.59s/it] 39%|███▉      | 182/468 [17:05<26:44,  5.61s/it]                                                 {'loss': 0.718, 'grad_norm': 4.139060020446777, 'learning_rate': 1.4010267951369985e-05, 'epoch': 1.16}
 39%|███▉      | 182/468 [17:05<26:44,  5.61s/it] 39%|███▉      | 183/468 [17:11<26:13,  5.52s/it]                                                 {'loss': 0.7158, 'grad_norm': 4.505482196807861, 'learning_rate': 1.3946642062334765e-05, 'epoch': 1.17}
 39%|███▉      | 183/468 [17:11<26:13,  5.52s/it] 39%|███▉      | 184/468 [17:16<26:18,  5.56s/it]                                                 {'loss': 0.8414, 'grad_norm': 4.597110748291016, 'learning_rate': 1.3882826358818936e-05, 'epoch': 1.18}
 39%|███▉      | 184/468 [17:16<26:18,  5.56s/it] 40%|███▉      | 185/468 [17:22<26:03,  5.53s/it]                                                 {'loss': 0.7612, 'grad_norm': 4.14007568359375, 'learning_rate': 1.381882391005058e-05, 'epoch': 1.18}
 40%|███▉      | 185/468 [17:22<26:03,  5.53s/it] 40%|███▉      | 186/468 [17:28<26:17,  5.59s/it]                                                 {'loss': 0.8211, 'grad_norm': 4.435880184173584, 'learning_rate': 1.3754637794239303e-05, 'epoch': 1.19}
 40%|███▉      | 186/468 [17:28<26:17,  5.59s/it] 40%|███▉      | 187/468 [17:33<26:20,  5.62s/it]                                                 {'loss': 0.8567, 'grad_norm': 3.891683340072632, 'learning_rate': 1.3690271098428234e-05, 'epoch': 1.19}
 40%|███▉      | 187/468 [17:33<26:20,  5.62s/it] 40%|████      | 188/468 [17:39<26:02,  5.58s/it]                                                 {'loss': 0.8174, 'grad_norm': 3.9963388442993164, 'learning_rate': 1.362572691834553e-05, 'epoch': 1.2}
 40%|████      | 188/468 [17:39<26:02,  5.58s/it] 40%|████      | 189/468 [17:44<25:51,  5.56s/it]                                                 {'loss': 0.9207, 'grad_norm': 4.144701957702637, 'learning_rate': 1.356100835825547e-05, 'epoch': 1.21}
 40%|████      | 189/468 [17:44<25:51,  5.56s/it] 41%|████      | 190/468 [17:50<25:48,  5.57s/it]                                                 {'loss': 0.6656, 'grad_norm': 3.6138250827789307, 'learning_rate': 1.3496118530809195e-05, 'epoch': 1.21}
 41%|████      | 190/468 [17:50<25:48,  5.57s/it] 41%|████      | 191/468 [17:55<25:49,  5.59s/it]                                                 {'loss': 0.9399, 'grad_norm': 4.463230609893799, 'learning_rate': 1.3431060556894959e-05, 'epoch': 1.22}
 41%|████      | 191/468 [17:56<25:49,  5.59s/it] 41%|████      | 192/468 [18:01<25:41,  5.59s/it]                                                 {'loss': 0.9371, 'grad_norm': 4.405485153198242, 'learning_rate': 1.3365837565488065e-05, 'epoch': 1.23}
 41%|████      | 192/468 [18:01<25:41,  5.59s/it] 41%|████      | 193/468 [18:07<25:47,  5.63s/it]                                                 {'loss': 0.8394, 'grad_norm': 4.3385796546936035, 'learning_rate': 1.3300452693500358e-05, 'epoch': 1.23}
 41%|████      | 193/468 [18:07<25:47,  5.63s/it] 41%|████▏     | 194/468 [18:12<25:38,  5.61s/it]                                                 {'loss': 0.8253, 'grad_norm': 4.294088840484619, 'learning_rate': 1.3234909085629362e-05, 'epoch': 1.24}
 41%|████▏     | 194/468 [18:12<25:38,  5.61s/it] 42%|████▏     | 195/468 [18:18<25:36,  5.63s/it]                                                 {'loss': 0.9855, 'grad_norm': 4.491063117980957, 'learning_rate': 1.316920989420703e-05, 'epoch': 1.25}
 42%|████▏     | 195/468 [18:18<25:36,  5.63s/it] 42%|████▏     | 196/468 [18:24<25:29,  5.62s/it]                                                 {'loss': 0.8394, 'grad_norm': 3.8770437240600586, 'learning_rate': 1.3103358279048136e-05, 'epoch': 1.25}
 42%|████▏     | 196/468 [18:24<25:29,  5.62s/it] 42%|████▏     | 197/468 [18:29<25:21,  5.61s/it]                                                 {'loss': 0.8844, 'grad_norm': 4.140678882598877, 'learning_rate': 1.30373574072983e-05, 'epoch': 1.26}
 42%|████▏     | 197/468 [18:29<25:21,  5.61s/it] 42%|████▏     | 198/468 [18:35<25:11,  5.60s/it]                                                 {'loss': 0.8354, 'grad_norm': 3.562434434890747, 'learning_rate': 1.2971210453281675e-05, 'epoch': 1.27}
 42%|████▏     | 198/468 [18:35<25:11,  5.60s/it] 43%|████▎     | 199/468 [18:41<25:16,  5.64s/it]                                                 {'loss': 0.8962, 'grad_norm': 4.29905366897583, 'learning_rate': 1.2904920598348252e-05, 'epoch': 1.27}
 43%|████▎     | 199/468 [18:41<25:16,  5.64s/it] 43%|████▎     | 200/468 [18:46<24:56,  5.59s/it]                                                 {'loss': 0.766, 'grad_norm': 4.025594711303711, 'learning_rate': 1.2838491030720882e-05, 'epoch': 1.28}
 43%|████▎     | 200/468 [18:46<24:56,  5.59s/it] 43%|████▎     | 201/468 [18:51<24:28,  5.50s/it]                                                 {'loss': 0.7452, 'grad_norm': 3.9499640464782715, 'learning_rate': 1.2771924945341906e-05, 'epoch': 1.28}
 43%|████▎     | 201/468 [18:51<24:28,  5.50s/it] 43%|████▎     | 202/468 [18:57<24:28,  5.52s/it]                                                 {'loss': 0.8364, 'grad_norm': 3.762453556060791, 'learning_rate': 1.2705225543719537e-05, 'epoch': 1.29}
 43%|████▎     | 202/468 [18:57<24:28,  5.52s/it] 43%|████▎     | 203/468 [19:02<24:25,  5.53s/it]                                                 {'loss': 0.9275, 'grad_norm': 3.922999143600464, 'learning_rate': 1.2638396033773836e-05, 'epoch': 1.3}
 43%|████▎     | 203/468 [19:03<24:25,  5.53s/it] 44%|████▎     | 204/468 [19:08<24:39,  5.60s/it]                                                 {'loss': 0.8207, 'grad_norm': 3.9464168548583984, 'learning_rate': 1.257143962968246e-05, 'epoch': 1.3}
 44%|████▎     | 204/468 [19:08<24:39,  5.60s/it] 44%|████▍     | 205/468 [19:14<24:30,  5.59s/it]                                                 {'loss': 1.0075, 'grad_norm': 4.15501594543457, 'learning_rate': 1.250435955172606e-05, 'epoch': 1.31}
 44%|████▍     | 205/468 [19:14<24:30,  5.59s/it] 44%|████▍     | 206/468 [19:19<24:12,  5.54s/it]                                                 {'loss': 0.7354, 'grad_norm': 3.656006336212158, 'learning_rate': 1.2437159026133397e-05, 'epoch': 1.32}
 44%|████▍     | 206/468 [19:19<24:12,  5.54s/it] 44%|████▍     | 207/468 [19:25<23:55,  5.50s/it]                                                 {'loss': 0.618, 'grad_norm': 4.070546627044678, 'learning_rate': 1.236984128492619e-05, 'epoch': 1.32}
 44%|████▍     | 207/468 [19:25<23:55,  5.50s/it] 44%|████▍     | 208/468 [19:30<23:38,  5.45s/it]                                                 {'loss': 0.5923, 'grad_norm': 3.568089246749878, 'learning_rate': 1.230240956576367e-05, 'epoch': 1.33}
 44%|████▍     | 208/468 [19:30<23:38,  5.45s/it] 45%|████▍     | 209/468 [19:36<23:44,  5.50s/it]                                                 {'loss': 0.6996, 'grad_norm': 4.319395542144775, 'learning_rate': 1.2234867111786851e-05, 'epoch': 1.34}
 45%|████▍     | 209/468 [19:36<23:44,  5.50s/it] 45%|████▍     | 210/468 [19:41<23:51,  5.55s/it]                                                 {'loss': 0.955, 'grad_norm': 5.146353244781494, 'learning_rate': 1.2167217171462566e-05, 'epoch': 1.34}
 45%|████▍     | 210/468 [19:41<23:51,  5.55s/it] 45%|████▌     | 211/468 [19:46<23:27,  5.48s/it]                                                 {'loss': 0.7641, 'grad_norm': 4.221734046936035, 'learning_rate': 1.2099462998427211e-05, 'epoch': 1.35}
 45%|████▌     | 211/468 [19:47<23:27,  5.48s/it] 45%|████▌     | 212/468 [19:52<23:34,  5.52s/it]                                                 {'loss': 0.7269, 'grad_norm': 4.155114650726318, 'learning_rate': 1.2031607851330282e-05, 'epoch': 1.35}
 45%|████▌     | 212/468 [19:52<23:34,  5.52s/it] 46%|████▌     | 213/468 [19:58<23:26,  5.52s/it]                                                 {'loss': 0.723, 'grad_norm': 4.856581687927246, 'learning_rate': 1.1963654993677645e-05, 'epoch': 1.36}
 46%|████▌     | 213/468 [19:58<23:26,  5.52s/it] 46%|████▌     | 214/468 [20:03<23:33,  5.57s/it]                                                 {'loss': 0.7243, 'grad_norm': 4.059493541717529, 'learning_rate': 1.189560769367456e-05, 'epoch': 1.37}
 46%|████▌     | 214/468 [20:03<23:33,  5.57s/it] 46%|████▌     | 215/468 [20:09<23:31,  5.58s/it]                                                 {'loss': 0.9725, 'grad_norm': 4.338677406311035, 'learning_rate': 1.1827469224068531e-05, 'epoch': 1.37}
 46%|████▌     | 215/468 [20:09<23:31,  5.58s/it] 46%|████▌     | 216/468 [20:15<23:41,  5.64s/it]                                                 {'loss': 0.8345, 'grad_norm': 9.827402114868164, 'learning_rate': 1.1759242861991855e-05, 'epoch': 1.38}
 46%|████▌     | 216/468 [20:15<23:41,  5.64s/it] 46%|████▋     | 217/468 [20:21<23:50,  5.70s/it]                                                 {'loss': 0.8963, 'grad_norm': 3.8361613750457764, 'learning_rate': 1.1690931888804055e-05, 'epoch': 1.39}
 46%|████▋     | 217/468 [20:21<23:50,  5.70s/it] 47%|████▋     | 218/468 [20:27<24:20,  5.84s/it]                                                 {'loss': 0.7034, 'grad_norm': 3.859276056289673, 'learning_rate': 1.1622539589934027e-05, 'epoch': 1.39}
 47%|████▋     | 218/468 [20:27<24:20,  5.84s/it] 47%|████▋     | 219/468 [20:33<24:24,  5.88s/it]                                                 {'loss': 0.6659, 'grad_norm': 4.208369731903076, 'learning_rate': 1.155406925472205e-05, 'epoch': 1.4}
 47%|████▋     | 219/468 [20:33<24:24,  5.88s/it] 47%|████▋     | 220/468 [20:38<24:03,  5.82s/it]                                                 {'loss': 0.6997, 'grad_norm': 3.559941291809082, 'learning_rate': 1.148552417626157e-05, 'epoch': 1.41}
 47%|████▋     | 220/468 [20:38<24:03,  5.82s/it] 47%|████▋     | 221/468 [20:44<23:56,  5.82s/it]                                                 {'loss': 0.7017, 'grad_norm': 4.014286041259766, 'learning_rate': 1.1416907651240826e-05, 'epoch': 1.41}
 47%|████▋     | 221/468 [20:44<23:56,  5.82s/it] 47%|████▋     | 222/468 [20:50<23:23,  5.70s/it]                                                 {'loss': 0.8097, 'grad_norm': 4.777035236358643, 'learning_rate': 1.1348222979784289e-05, 'epoch': 1.42}
 47%|████▋     | 222/468 [20:50<23:23,  5.70s/it] 48%|████▊     | 223/468 [20:55<23:14,  5.69s/it]                                                 {'loss': 0.6059, 'grad_norm': 4.08951473236084, 'learning_rate': 1.1279473465293953e-05, 'epoch': 1.42}
 48%|████▊     | 223/468 [20:55<23:14,  5.69s/it] 48%|████▊     | 224/468 [21:01<22:57,  5.65s/it]                                                 {'loss': 0.9065, 'grad_norm': 3.972805976867676, 'learning_rate': 1.1210662414290439e-05, 'epoch': 1.43}
 48%|████▊     | 224/468 [21:01<22:57,  5.65s/it] 48%|████▊     | 225/468 [21:06<22:51,  5.65s/it]                                                 {'loss': 0.74, 'grad_norm': 3.8269009590148926, 'learning_rate': 1.1141793136253987e-05, 'epoch': 1.44}
 48%|████▊     | 225/468 [21:07<22:51,  5.65s/it] 48%|████▊     | 226/468 [21:12<22:47,  5.65s/it]                                                 {'loss': 0.7662, 'grad_norm': 4.467615127563477, 'learning_rate': 1.107286894346527e-05, 'epoch': 1.44}
 48%|████▊     | 226/468 [21:12<22:47,  5.65s/it] 49%|████▊     | 227/468 [21:18<22:39,  5.64s/it]                                                 {'loss': 0.7158, 'grad_norm': 3.7157208919525146, 'learning_rate': 1.1003893150846103e-05, 'epoch': 1.45}
 49%|████▊     | 227/468 [21:18<22:39,  5.64s/it] 49%|████▊     | 228/468 [21:24<22:46,  5.70s/it]                                                 {'loss': 0.7589, 'grad_norm': 4.624570846557617, 'learning_rate': 1.09348690758e-05, 'epoch': 1.46}
 49%|████▊     | 228/468 [21:24<22:46,  5.70s/it] 49%|████▉     | 229/468 [21:29<22:53,  5.75s/it]                                                 {'loss': 0.6805, 'grad_norm': 4.455095291137695, 'learning_rate': 1.0865800038052632e-05, 'epoch': 1.46}
 49%|████▉     | 229/468 [21:30<22:53,  5.75s/it] 49%|████▉     | 230/468 [21:35<22:47,  5.75s/it]                                                 {'loss': 0.5969, 'grad_norm': 3.8483664989471436, 'learning_rate': 1.0796689359492154e-05, 'epoch': 1.47}
 49%|████▉     | 230/468 [21:35<22:47,  5.75s/it] 49%|████▉     | 231/468 [21:41<22:14,  5.63s/it]                                                 {'loss': 0.8514, 'grad_norm': 5.203668594360352, 'learning_rate': 1.072754036400944e-05, 'epoch': 1.48}
 49%|████▉     | 231/468 [21:41<22:14,  5.63s/it] 50%|████▉     | 232/468 [21:46<22:01,  5.60s/it]                                                 {'loss': 0.6639, 'grad_norm': 4.060433387756348, 'learning_rate': 1.0658356377338235e-05, 'epoch': 1.48}
 50%|████▉     | 232/468 [21:46<22:01,  5.60s/it] 50%|████▉     | 233/468 [21:52<22:19,  5.70s/it]                                                 {'loss': 0.6862, 'grad_norm': 3.633265972137451, 'learning_rate': 1.0589140726895179e-05, 'epoch': 1.49}
 50%|████▉     | 233/468 [21:52<22:19,  5.70s/it] 50%|█████     | 234/468 [21:58<22:08,  5.68s/it]                                                 {'loss': 0.694, 'grad_norm': 4.776611804962158, 'learning_rate': 1.0519896741619803e-05, 'epoch': 1.5}
 50%|█████     | 234/468 [21:58<22:08,  5.68s/it] 50%|█████     | 235/468 [22:03<22:07,  5.70s/it]                                                 {'loss': 0.8334, 'grad_norm': 4.234662055969238, 'learning_rate': 1.0450627751814396e-05, 'epoch': 1.5}
 50%|█████     | 235/468 [22:03<22:07,  5.70s/it] 50%|█████     | 236/468 [22:09<21:57,  5.68s/it]                                                 {'loss': 0.7209, 'grad_norm': 5.305736541748047, 'learning_rate': 1.0381337088983838e-05, 'epoch': 1.51}
 50%|█████     | 236/468 [22:09<21:57,  5.68s/it] 51%|█████     | 237/468 [22:15<21:43,  5.64s/it]                                                 {'loss': 0.8703, 'grad_norm': 3.499378204345703, 'learning_rate': 1.0312028085675393e-05, 'epoch': 1.51}
 51%|█████     | 237/468 [22:15<21:43,  5.64s/it] 51%|█████     | 238/468 [22:20<21:17,  5.55s/it]                                                 {'loss': 0.7695, 'grad_norm': 3.771949291229248, 'learning_rate': 1.0242704075318402e-05, 'epoch': 1.52}
 51%|█████     | 238/468 [22:20<21:17,  5.55s/it] 51%|█████     | 239/468 [22:25<21:12,  5.56s/it]                                                 {'loss': 0.8974, 'grad_norm': 3.753061532974243, 'learning_rate': 1.0173368392063978e-05, 'epoch': 1.53}
 51%|█████     | 239/468 [22:26<21:12,  5.56s/it] 51%|█████▏    | 240/468 [22:31<21:04,  5.54s/it]                                                 {'loss': 0.736, 'grad_norm': 4.366219520568848, 'learning_rate': 1.0104024370624644e-05, 'epoch': 1.53}
 51%|█████▏    | 240/468 [22:31<21:04,  5.54s/it] 51%|█████▏    | 241/468 [22:36<20:53,  5.52s/it]                                                 {'loss': 0.6507, 'grad_norm': 3.817883014678955, 'learning_rate': 1.0034675346113945e-05, 'epoch': 1.54}
 51%|█████▏    | 241/468 [22:37<20:53,  5.52s/it] 52%|█████▏    | 242/468 [22:42<20:54,  5.55s/it]                                                 {'loss': 0.5661, 'grad_norm': 4.063380241394043, 'learning_rate': 9.965324653886057e-06, 'epoch': 1.55}
 52%|█████▏    | 242/468 [22:42<20:54,  5.55s/it] 52%|█████▏    | 243/468 [22:48<20:56,  5.58s/it]                                                 {'loss': 0.6389, 'grad_norm': 4.864945411682129, 'learning_rate': 9.89597562937536e-06, 'epoch': 1.55}
 52%|█████▏    | 243/468 [22:48<20:56,  5.58s/it] 52%|█████▏    | 244/468 [22:53<20:44,  5.56s/it]                                                 {'loss': 0.5015, 'grad_norm': 4.155246257781982, 'learning_rate': 9.826631607936024e-06, 'epoch': 1.56}
 52%|█████▏    | 244/468 [22:53<20:44,  5.56s/it] 52%|█████▏    | 245/468 [22:59<20:53,  5.62s/it]                                                 {'loss': 0.5551, 'grad_norm': 4.7889180183410645, 'learning_rate': 9.757295924681601e-06, 'epoch': 1.57}
 52%|█████▏    | 245/468 [22:59<20:53,  5.62s/it] 53%|█████▎    | 246/468 [23:05<20:54,  5.65s/it]                                                 {'loss': 0.7934, 'grad_norm': 4.592621326446533, 'learning_rate': 9.687971914324607e-06, 'epoch': 1.57}
 53%|█████▎    | 246/468 [23:05<20:54,  5.65s/it] 53%|█████▎    | 247/468 [23:10<20:36,  5.59s/it]                                                 {'loss': 0.7395, 'grad_norm': 4.318225383758545, 'learning_rate': 9.618662911016165e-06, 'epoch': 1.58}
 53%|█████▎    | 247/468 [23:10<20:36,  5.59s/it] 53%|█████▎    | 248/468 [23:16<20:34,  5.61s/it]                                                 {'loss': 0.7349, 'grad_norm': 4.825052261352539, 'learning_rate': 9.54937224818561e-06, 'epoch': 1.58}
 53%|█████▎    | 248/468 [23:16<20:34,  5.61s/it] 53%|█████▎    | 249/468 [23:22<21:08,  5.79s/it]                                                 {'loss': 0.7325, 'grad_norm': 4.042157173156738, 'learning_rate': 9.480103258380198e-06, 'epoch': 1.59}
 53%|█████▎    | 249/468 [23:22<21:08,  5.79s/it] 53%|█████▎    | 250/468 [23:28<20:56,  5.76s/it]                                                 {'loss': 0.6711, 'grad_norm': 3.695882558822632, 'learning_rate': 9.410859273104823e-06, 'epoch': 1.6}
 53%|█████▎    | 250/468 [23:28<20:56,  5.76s/it] 54%|█████▎    | 251/468 [23:33<20:33,  5.69s/it]                                                 {'loss': 0.7448, 'grad_norm': 4.653846740722656, 'learning_rate': 9.341643622661768e-06, 'epoch': 1.6}
 54%|█████▎    | 251/468 [23:33<20:33,  5.69s/it] 54%|█████▍    | 252/468 [23:39<20:38,  5.73s/it]                                                 {'loss': 0.7269, 'grad_norm': 3.795363664627075, 'learning_rate': 9.272459635990563e-06, 'epoch': 1.61}
 54%|█████▍    | 252/468 [23:39<20:38,  5.73s/it] 54%|█████▍    | 253/468 [23:45<20:31,  5.73s/it]                                                 {'loss': 0.7175, 'grad_norm': 4.337954998016357, 'learning_rate': 9.20331064050785e-06, 'epoch': 1.62}
 54%|█████▍    | 253/468 [23:45<20:31,  5.73s/it] 54%|█████▍    | 254/468 [23:50<20:12,  5.67s/it]                                                 {'loss': 0.7564, 'grad_norm': 4.157830715179443, 'learning_rate': 9.134199961947368e-06, 'epoch': 1.62}
 54%|█████▍    | 254/468 [23:50<20:12,  5.67s/it] 54%|█████▍    | 255/468 [23:56<19:56,  5.62s/it]                                                 {'loss': 0.8244, 'grad_norm': 3.8593921661376953, 'learning_rate': 9.065130924199998e-06, 'epoch': 1.63}
 54%|█████▍    | 255/468 [23:56<19:56,  5.62s/it] 55%|█████▍    | 256/468 [24:01<19:50,  5.61s/it]                                                 {'loss': 0.6409, 'grad_norm': 3.629176616668701, 'learning_rate': 8.996106849153897e-06, 'epoch': 1.64}
 55%|█████▍    | 256/468 [24:02<19:50,  5.61s/it] 55%|█████▍    | 257/468 [24:07<19:52,  5.65s/it]                                                 {'loss': 0.3993, 'grad_norm': 3.4044382572174072, 'learning_rate': 8.927131056534734e-06, 'epoch': 1.64}
 55%|█████▍    | 257/468 [24:07<19:52,  5.65s/it] 55%|█████▌    | 258/468 [24:13<19:32,  5.58s/it]                                                 {'loss': 0.7389, 'grad_norm': 4.150040626525879, 'learning_rate': 8.858206863746018e-06, 'epoch': 1.65}
 55%|█████▌    | 258/468 [24:13<19:32,  5.58s/it] 55%|█████▌    | 259/468 [24:18<19:28,  5.59s/it]                                                 {'loss': 0.4837, 'grad_norm': 3.3155581951141357, 'learning_rate': 8.789337585709565e-06, 'epoch': 1.65}
 55%|█████▌    | 259/468 [24:18<19:28,  5.59s/it] 56%|█████▌    | 260/468 [24:24<19:41,  5.68s/it]                                                 {'loss': 0.4904, 'grad_norm': 3.8947951793670654, 'learning_rate': 8.720526534706052e-06, 'epoch': 1.66}
 56%|█████▌    | 260/468 [24:24<19:41,  5.68s/it] 56%|█████▌    | 261/468 [24:30<19:32,  5.67s/it]                                                 {'loss': 0.5656, 'grad_norm': 4.098310470581055, 'learning_rate': 8.651777020215713e-06, 'epoch': 1.67}
 56%|█████▌    | 261/468 [24:30<19:32,  5.67s/it] 56%|█████▌    | 262/468 [24:35<19:08,  5.58s/it]                                                 {'loss': 0.6036, 'grad_norm': 4.88411808013916, 'learning_rate': 8.583092348759176e-06, 'epoch': 1.67}
 56%|█████▌    | 262/468 [24:35<19:08,  5.58s/it] 56%|█████▌    | 263/468 [24:41<18:53,  5.53s/it]                                                 {'loss': 0.6869, 'grad_norm': 4.804973125457764, 'learning_rate': 8.514475823738431e-06, 'epoch': 1.68}
 56%|█████▌    | 263/468 [24:41<18:53,  5.53s/it] 56%|█████▋    | 264/468 [24:46<18:36,  5.48s/it]                                                 {'loss': 0.5342, 'grad_norm': 4.344832897186279, 'learning_rate': 8.445930745277953e-06, 'epoch': 1.69}
 56%|█████▋    | 264/468 [24:46<18:36,  5.48s/it] 57%|█████▋    | 265/468 [24:51<18:32,  5.48s/it]                                                 {'loss': 0.4928, 'grad_norm': 3.6538772583007812, 'learning_rate': 8.377460410065973e-06, 'epoch': 1.69}
 57%|█████▋    | 265/468 [24:51<18:32,  5.48s/it] 57%|█████▋    | 266/468 [24:57<18:23,  5.46s/it]                                                 {'loss': 0.5289, 'grad_norm': 3.9852774143218994, 'learning_rate': 8.309068111195947e-06, 'epoch': 1.7}
 57%|█████▋    | 266/468 [24:57<18:23,  5.46s/it] 57%|█████▋    | 267/468 [25:03<18:35,  5.55s/it]                                                 {'loss': 0.7585, 'grad_norm': 4.557837963104248, 'learning_rate': 8.240757138008149e-06, 'epoch': 1.71}
 57%|█████▋    | 267/468 [25:03<18:35,  5.55s/it] 57%|█████▋    | 268/468 [25:08<18:39,  5.60s/it]                                                 {'loss': 0.6699, 'grad_norm': 4.809844017028809, 'learning_rate': 8.172530775931476e-06, 'epoch': 1.71}
 57%|█████▋    | 268/468 [25:08<18:39,  5.60s/it] 57%|█████▋    | 269/468 [25:14<18:32,  5.59s/it]                                                 {'loss': 0.6089, 'grad_norm': 4.429973125457764, 'learning_rate': 8.104392306325442e-06, 'epoch': 1.72}
 57%|█████▋    | 269/468 [25:14<18:32,  5.59s/it] 58%|█████▊    | 270/468 [25:19<18:24,  5.58s/it]                                                 {'loss': 0.7631, 'grad_norm': 4.0810933113098145, 'learning_rate': 8.036345006322358e-06, 'epoch': 1.73}
 58%|█████▊    | 270/468 [25:19<18:24,  5.58s/it] 58%|█████▊    | 271/468 [25:25<18:17,  5.57s/it]                                                 {'loss': 0.6408, 'grad_norm': 4.029016017913818, 'learning_rate': 7.96839214866972e-06, 'epoch': 1.73}
 58%|█████▊    | 271/468 [25:25<18:17,  5.57s/it] 58%|█████▊    | 272/468 [25:31<18:33,  5.68s/it]                                                 {'loss': 0.6113, 'grad_norm': 3.9742660522460938, 'learning_rate': 7.900537001572792e-06, 'epoch': 1.74}
 58%|█████▊    | 272/468 [25:31<18:33,  5.68s/it] 58%|█████▊    | 273/468 [25:37<18:58,  5.84s/it]                                                 {'loss': 0.6215, 'grad_norm': 4.251564979553223, 'learning_rate': 7.832782828537437e-06, 'epoch': 1.74}
 58%|█████▊    | 273/468 [25:37<18:58,  5.84s/it] 59%|█████▊    | 274/468 [25:43<18:41,  5.78s/it]                                                 {'loss': 0.4022, 'grad_norm': 3.651646614074707, 'learning_rate': 7.765132888213147e-06, 'epoch': 1.75}
 59%|█████▊    | 274/468 [25:43<18:41,  5.78s/it] 59%|█████▉    | 275/468 [25:48<18:34,  5.77s/it]                                                 {'loss': 0.5807, 'grad_norm': 5.446770668029785, 'learning_rate': 7.697590434236331e-06, 'epoch': 1.76}
 59%|█████▉    | 275/468 [25:49<18:34,  5.77s/it] 59%|█████▉    | 276/468 [25:54<18:34,  5.81s/it]                                                 {'loss': 0.5977, 'grad_norm': 3.6418986320495605, 'learning_rate': 7.630158715073813e-06, 'epoch': 1.76}
 59%|█████▉    | 276/468 [25:54<18:34,  5.81s/it] 59%|█████▉    | 277/468 [26:00<18:31,  5.82s/it]                                                 {'loss': 0.556, 'grad_norm': 3.596979856491089, 'learning_rate': 7.56284097386661e-06, 'epoch': 1.77}
 59%|█████▉    | 277/468 [26:00<18:31,  5.82s/it] 59%|█████▉    | 278/468 [26:06<18:09,  5.74s/it]                                                 {'loss': 0.7393, 'grad_norm': 4.52813720703125, 'learning_rate': 7.495640448273947e-06, 'epoch': 1.78}
 59%|█████▉    | 278/468 [26:06<18:09,  5.74s/it] 60%|█████▉    | 279/468 [26:12<18:08,  5.76s/it]                                                 {'loss': 0.3619, 'grad_norm': 3.7025249004364014, 'learning_rate': 7.428560370317542e-06, 'epoch': 1.78}
 60%|█████▉    | 279/468 [26:12<18:08,  5.76s/it] 60%|█████▉    | 280/468 [26:17<17:56,  5.72s/it]                                                 {'loss': 0.4888, 'grad_norm': 3.7378249168395996, 'learning_rate': 7.361603966226165e-06, 'epoch': 1.79}
 60%|█████▉    | 280/468 [26:17<17:56,  5.72s/it] 60%|██████    | 281/468 [26:23<17:44,  5.69s/it]                                                 {'loss': 0.601, 'grad_norm': 5.092636585235596, 'learning_rate': 7.294774456280466e-06, 'epoch': 1.8}
 60%|██████    | 281/468 [26:23<17:44,  5.69s/it] 60%|██████    | 282/468 [26:28<17:33,  5.66s/it]                                                 {'loss': 0.7198, 'grad_norm': 4.581603527069092, 'learning_rate': 7.228075054658096e-06, 'epoch': 1.8}
 60%|██████    | 282/468 [26:29<17:33,  5.66s/it] 60%|██████    | 283/468 [26:34<17:18,  5.61s/it]                                                 {'loss': 0.5588, 'grad_norm': 4.213079452514648, 'learning_rate': 7.1615089692791225e-06, 'epoch': 1.81}
 60%|██████    | 283/468 [26:34<17:18,  5.61s/it] 61%|██████    | 284/468 [26:40<17:21,  5.66s/it]                                                 {'loss': 0.5169, 'grad_norm': 3.7297449111938477, 'learning_rate': 7.095079401651749e-06, 'epoch': 1.81}
 61%|██████    | 284/468 [26:40<17:21,  5.66s/it] 61%|██████    | 285/468 [26:45<17:12,  5.64s/it]                                                 {'loss': 0.6611, 'grad_norm': 3.7085318565368652, 'learning_rate': 7.028789546718327e-06, 'epoch': 1.82}
 61%|██████    | 285/468 [26:45<17:12,  5.64s/it] 61%|██████    | 286/468 [26:51<17:06,  5.64s/it]                                                 {'loss': 0.5654, 'grad_norm': 3.914260149002075, 'learning_rate': 6.962642592701703e-06, 'epoch': 1.83}
 61%|██████    | 286/468 [26:51<17:06,  5.64s/it] 61%|██████▏   | 287/468 [26:56<16:52,  5.60s/it]                                                 {'loss': 0.5237, 'grad_norm': 4.407342910766602, 'learning_rate': 6.896641720951868e-06, 'epoch': 1.83}
 61%|██████▏   | 287/468 [26:56<16:52,  5.60s/it] 62%|██████▏   | 288/468 [27:02<16:41,  5.56s/it]                                                 {'loss': 0.6776, 'grad_norm': 4.254629135131836, 'learning_rate': 6.8307901057929735e-06, 'epoch': 1.84}
 62%|██████▏   | 288/468 [27:02<16:41,  5.56s/it] 62%|██████▏   | 289/468 [27:07<16:23,  5.49s/it]                                                 {'loss': 0.4746, 'grad_norm': 3.445517063140869, 'learning_rate': 6.765090914370643e-06, 'epoch': 1.85}
 62%|██████▏   | 289/468 [27:07<16:23,  5.49s/it] 62%|██████▏   | 290/468 [27:13<16:22,  5.52s/it]                                                 {'loss': 0.6387, 'grad_norm': 4.359499454498291, 'learning_rate': 6.6995473064996455e-06, 'epoch': 1.85}
 62%|██████▏   | 290/468 [27:13<16:22,  5.52s/it] 62%|██████▏   | 291/468 [27:18<16:15,  5.51s/it]                                                 {'loss': 0.4696, 'grad_norm': 4.195409297943115, 'learning_rate': 6.634162434511939e-06, 'epoch': 1.86}
 62%|██████▏   | 291/468 [27:18<16:15,  5.51s/it] 62%|██████▏   | 292/468 [27:24<16:18,  5.56s/it]                                                 {'loss': 0.6329, 'grad_norm': 4.226425647735596, 'learning_rate': 6.568939443105045e-06, 'epoch': 1.87}
 62%|██████▏   | 292/468 [27:24<16:18,  5.56s/it] 63%|██████▎   | 293/468 [27:29<16:09,  5.54s/it]                                                 {'loss': 0.5103, 'grad_norm': 4.989119529724121, 'learning_rate': 6.5038814691908095e-06, 'epoch': 1.87}
 63%|██████▎   | 293/468 [27:30<16:09,  5.54s/it] 63%|██████▎   | 294/468 [27:35<16:00,  5.52s/it]                                                 {'loss': 0.6976, 'grad_norm': 3.804143190383911, 'learning_rate': 6.438991641744531e-06, 'epoch': 1.88}
 63%|██████▎   | 294/468 [27:35<16:00,  5.52s/it] 63%|██████▎   | 295/468 [27:40<15:45,  5.47s/it]                                                 {'loss': 0.4778, 'grad_norm': 3.646873950958252, 'learning_rate': 6.374273081654474e-06, 'epoch': 1.88}
 63%|██████▎   | 295/468 [27:40<15:45,  5.47s/it] 63%|██████▎   | 296/468 [27:46<15:41,  5.47s/it]                                                 {'loss': 0.4729, 'grad_norm': 3.488591194152832, 'learning_rate': 6.30972890157177e-06, 'epoch': 1.89}
 63%|██████▎   | 296/468 [27:46<15:41,  5.47s/it] 63%|██████▎   | 297/468 [27:51<15:39,  5.50s/it]                                                 {'loss': 0.3848, 'grad_norm': 3.3771116733551025, 'learning_rate': 6.245362205760703e-06, 'epoch': 1.9}
 63%|██████▎   | 297/468 [27:51<15:39,  5.50s/it] 64%|██████▎   | 298/468 [27:57<15:42,  5.54s/it]                                                 {'loss': 0.7984, 'grad_norm': 4.795200347900391, 'learning_rate': 6.1811760899494276e-06, 'epoch': 1.9}
 64%|██████▎   | 298/468 [27:57<15:42,  5.54s/it] 64%|██████▍   | 299/468 [28:03<15:43,  5.59s/it]                                                 {'loss': 0.7028, 'grad_norm': 4.134452819824219, 'learning_rate': 6.117173641181064e-06, 'epoch': 1.91}
 64%|██████▍   | 299/468 [28:03<15:43,  5.59s/it] 64%|██████▍   | 300/468 [28:08<15:28,  5.53s/it]                                                 {'loss': 0.4805, 'grad_norm': 4.085136413574219, 'learning_rate': 6.053357937665237e-06, 'epoch': 1.92}
 64%|██████▍   | 300/468 [28:08<15:28,  5.53s/it] 64%|██████▍   | 301/468 [28:14<15:25,  5.54s/it]                                                 {'loss': 0.7002, 'grad_norm': 4.566370487213135, 'learning_rate': 5.9897320486300195e-06, 'epoch': 1.92}
 64%|██████▍   | 301/468 [28:14<15:25,  5.54s/it] 65%|██████▍   | 302/468 [28:19<15:21,  5.55s/it]                                                 {'loss': 0.4062, 'grad_norm': 6.165078163146973, 'learning_rate': 5.926299034174321e-06, 'epoch': 1.93}
 65%|██████▍   | 302/468 [28:19<15:21,  5.55s/it] 65%|██████▍   | 303/468 [28:25<15:20,  5.58s/it]                                                 {'loss': 0.5391, 'grad_norm': 3.8856232166290283, 'learning_rate': 5.863061945120719e-06, 'epoch': 1.94}
 65%|██████▍   | 303/468 [28:25<15:20,  5.58s/it] 65%|██████▍   | 304/468 [28:30<15:14,  5.57s/it]                                                 {'loss': 0.6354, 'grad_norm': 3.821948528289795, 'learning_rate': 5.800023822868717e-06, 'epoch': 1.94}
 65%|██████▍   | 304/468 [28:31<15:14,  5.57s/it] 65%|██████▌   | 305/468 [28:36<15:09,  5.58s/it]                                                 {'loss': 0.5953, 'grad_norm': 4.5172200202941895, 'learning_rate': 5.737187699248474e-06, 'epoch': 1.95}
 65%|██████▌   | 305/468 [28:36<15:09,  5.58s/it] 65%|██████▌   | 306/468 [28:42<15:16,  5.66s/it]                                                 {'loss': 0.4961, 'grad_norm': 3.2865023612976074, 'learning_rate': 5.674556596374993e-06, 'epoch': 1.96}
 65%|██████▌   | 306/468 [28:42<15:16,  5.66s/it] 66%|██████▌   | 307/468 [28:47<14:56,  5.57s/it]                                                 {'loss': 0.499, 'grad_norm': 4.259216785430908, 'learning_rate': 5.612133526502752e-06, 'epoch': 1.96}
 66%|██████▌   | 307/468 [28:47<14:56,  5.57s/it] 66%|██████▌   | 308/468 [28:53<14:43,  5.52s/it]                                                 {'loss': 0.7743, 'grad_norm': 4.588491916656494, 'learning_rate': 5.549921491880856e-06, 'epoch': 1.97}
 66%|██████▌   | 308/468 [28:53<14:43,  5.52s/it] 66%|██████▌   | 309/468 [28:58<14:42,  5.55s/it]                                                 {'loss': 0.6843, 'grad_norm': 4.763411045074463, 'learning_rate': 5.487923484608629e-06, 'epoch': 1.97}
 66%|██████▌   | 309/468 [28:58<14:42,  5.55s/it] 66%|██████▌   | 310/468 [29:04<14:33,  5.53s/it]                                                 {'loss': 0.6084, 'grad_norm': 4.414261341094971, 'learning_rate': 5.4261424864917075e-06, 'epoch': 1.98}
 66%|██████▌   | 310/468 [29:04<14:33,  5.53s/it] 66%|██████▋   | 311/468 [29:10<14:44,  5.63s/it]                                                 {'loss': 0.5509, 'grad_norm': 4.957952976226807, 'learning_rate': 5.364581468898629e-06, 'epoch': 1.99}
 66%|██████▋   | 311/468 [29:10<14:44,  5.63s/it] 67%|██████▋   | 312/468 [29:15<14:36,  5.62s/it]                                                 {'loss': 0.6133, 'grad_norm': 5.061119556427002, 'learning_rate': 5.3032433926179395e-06, 'epoch': 1.99}
 67%|██████▋   | 312/468 [29:15<14:36,  5.62s/it] 67%|██████▋   | 313/468 [29:21<14:28,  5.60s/it]                                                 {'loss': 0.7102, 'grad_norm': 4.997701644897461, 'learning_rate': 5.242131207715768e-06, 'epoch': 2.0}
 67%|██████▋   | 313/468 [29:21<14:28,  5.60s/it] 67%|██████▋   | 314/468 [29:26<14:15,  5.55s/it]                                                 {'loss': 0.4685, 'grad_norm': 3.730801582336426, 'learning_rate': 5.181247853393961e-06, 'epoch': 2.01}
 67%|██████▋   | 314/468 [29:26<14:15,  5.55s/it] 67%|██████▋   | 315/468 [29:32<14:16,  5.60s/it]                                                 {'loss': 0.4322, 'grad_norm': 3.992004871368408, 'learning_rate': 5.120596257848716e-06, 'epoch': 2.01}
 67%|██████▋   | 315/468 [29:32<14:16,  5.60s/it] 68%|██████▊   | 316/468 [29:37<13:58,  5.52s/it]                                                 {'loss': 0.3795, 'grad_norm': 3.3882858753204346, 'learning_rate': 5.060179338129754e-06, 'epoch': 2.02}
 68%|██████▊   | 316/468 [29:37<13:58,  5.52s/it] 68%|██████▊   | 317/468 [29:43<13:53,  5.52s/it]                                                 {'loss': 0.3833, 'grad_norm': 3.9513473510742188, 'learning_rate': 5.000000000000003e-06, 'epoch': 2.03}
 68%|██████▊   | 317/468 [29:43<13:53,  5.52s/it] 68%|██████▊   | 318/468 [29:48<13:49,  5.53s/it]                                                 {'loss': 0.3846, 'grad_norm': 3.7574055194854736, 'learning_rate': 4.940061137795876e-06, 'epoch': 2.03}
 68%|██████▊   | 318/468 [29:48<13:49,  5.53s/it] 68%|██████▊   | 319/468 [29:54<13:42,  5.52s/it]                                                 {'loss': 0.3038, 'grad_norm': 3.2647111415863037, 'learning_rate': 4.8803656342880475e-06, 'epoch': 2.04}
 68%|██████▊   | 319/468 [29:54<13:42,  5.52s/it] 68%|██████▊   | 320/468 [29:59<13:34,  5.50s/it]                                                 {'loss': 0.4983, 'grad_norm': 3.6258206367492676, 'learning_rate': 4.82091636054281e-06, 'epoch': 2.04}
 68%|██████▊   | 320/468 [29:59<13:34,  5.50s/it] 69%|██████▊   | 321/468 [30:05<13:43,  5.60s/it]                                                 {'loss': 0.5737, 'grad_norm': 4.663994312286377, 'learning_rate': 4.7617161757839895e-06, 'epoch': 2.05}
 69%|██████▊   | 321/468 [30:05<13:43,  5.60s/it] 69%|██████▉   | 322/468 [30:11<13:44,  5.65s/it]                                                 {'loss': 0.3022, 'grad_norm': 3.240018129348755, 'learning_rate': 4.702767927255432e-06, 'epoch': 2.06}
 69%|██████▉   | 322/468 [30:11<13:44,  5.65s/it] 69%|██████▉   | 323/468 [30:16<13:39,  5.65s/it]                                                 {'loss': 0.4669, 'grad_norm': 4.099645137786865, 'learning_rate': 4.644074450084061e-06, 'epoch': 2.06}
 69%|██████▉   | 323/468 [30:17<13:39,  5.65s/it] 69%|██████▉   | 324/468 [30:22<13:33,  5.65s/it]                                                 {'loss': 0.5043, 'grad_norm': 4.7606916427612305, 'learning_rate': 4.5856385671435285e-06, 'epoch': 2.07}
 69%|██████▉   | 324/468 [30:22<13:33,  5.65s/it] 69%|██████▉   | 325/468 [30:28<13:22,  5.62s/it]                                                 {'loss': 0.5029, 'grad_norm': 4.789445400238037, 'learning_rate': 4.527463088918439e-06, 'epoch': 2.08}
 69%|██████▉   | 325/468 [30:28<13:22,  5.62s/it] 70%|██████▉   | 326/468 [30:33<13:24,  5.67s/it]                                                 {'loss': 0.438, 'grad_norm': 4.6414995193481445, 'learning_rate': 4.469550813369198e-06, 'epoch': 2.08}
 70%|██████▉   | 326/468 [30:34<13:24,  5.67s/it] 70%|██████▉   | 327/468 [30:39<13:31,  5.75s/it]                                                 {'loss': 0.6077, 'grad_norm': 4.80430793762207, 'learning_rate': 4.411904525797408e-06, 'epoch': 2.09}
 70%|██████▉   | 327/468 [30:40<13:31,  5.75s/it] 70%|███████   | 328/468 [30:45<13:31,  5.80s/it]                                                 {'loss': 0.4579, 'grad_norm': 3.774984121322632, 'learning_rate': 4.354526998711945e-06, 'epoch': 2.1}
 70%|███████   | 328/468 [30:45<13:31,  5.80s/it] 70%|███████   | 329/468 [30:51<13:18,  5.75s/it]                                                 {'loss': 0.3781, 'grad_norm': 3.615771532058716, 'learning_rate': 4.297420991695598e-06, 'epoch': 2.1}
 70%|███████   | 329/468 [30:51<13:18,  5.75s/it] 71%|███████   | 330/468 [30:56<13:04,  5.68s/it]                                                 {'loss': 0.4378, 'grad_norm': 3.701629400253296, 'learning_rate': 4.240589251272342e-06, 'epoch': 2.11}
 71%|███████   | 330/468 [30:57<13:04,  5.68s/it] 71%|███████   | 331/468 [31:02<12:58,  5.68s/it]                                                 {'loss': 0.3846, 'grad_norm': 3.3300154209136963, 'learning_rate': 4.184034510775258e-06, 'epoch': 2.12}
 71%|███████   | 331/468 [31:02<12:58,  5.68s/it] 71%|███████   | 332/468 [31:07<12:37,  5.57s/it]                                                 {'loss': 0.3356, 'grad_norm': 3.538325309753418, 'learning_rate': 4.127759490215057e-06, 'epoch': 2.12}
 71%|███████   | 332/468 [31:08<12:37,  5.57s/it] 71%|███████   | 333/468 [31:13<12:31,  5.57s/it]                                                 {'loss': 0.3583, 'grad_norm': 3.3568265438079834, 'learning_rate': 4.0717668961492725e-06, 'epoch': 2.13}
 71%|███████   | 333/468 [31:13<12:31,  5.57s/it] 71%|███████▏  | 334/468 [31:19<12:25,  5.56s/it]                                                 {'loss': 0.455, 'grad_norm': 3.565450668334961, 'learning_rate': 4.016059421552082e-06, 'epoch': 2.13}
 71%|███████▏  | 334/468 [31:19<12:25,  5.56s/it] 72%|███████▏  | 335/468 [31:25<12:43,  5.74s/it]                                                 {'loss': 0.3939, 'grad_norm': 3.567866086959839, 'learning_rate': 3.9606397456847875e-06, 'epoch': 2.14}
 72%|███████▏  | 335/468 [31:25<12:43,  5.74s/it] 72%|███████▏  | 336/468 [31:30<12:26,  5.66s/it]                                                 {'loss': 0.4823, 'grad_norm': 4.265475273132324, 'learning_rate': 3.905510533966959e-06, 'epoch': 2.15}
 72%|███████▏  | 336/468 [31:30<12:26,  5.66s/it] 72%|███████▏  | 337/468 [31:36<12:24,  5.68s/it]                                                 {'loss': 0.4364, 'grad_norm': 4.060686111450195, 'learning_rate': 3.850674437848243e-06, 'epoch': 2.15}
 72%|███████▏  | 337/468 [31:36<12:24,  5.68s/it] 72%|███████▏  | 338/468 [31:41<12:13,  5.65s/it]                                                 {'loss': 0.4156, 'grad_norm': 3.833601236343384, 'learning_rate': 3.79613409468083e-06, 'epoch': 2.16}
 72%|███████▏  | 338/468 [31:42<12:13,  5.65s/it] 72%|███████▏  | 339/468 [31:47<11:59,  5.58s/it]                                                 {'loss': 0.537, 'grad_norm': 4.475203037261963, 'learning_rate': 3.7418921275926245e-06, 'epoch': 2.17}
 72%|███████▏  | 339/468 [31:47<11:59,  5.58s/it] 73%|███████▎  | 340/468 [31:53<12:06,  5.67s/it]                                                 {'loss': 0.4951, 'grad_norm': 4.767886638641357, 'learning_rate': 3.687951145361073e-06, 'epoch': 2.17}
 73%|███████▎  | 340/468 [31:53<12:06,  5.67s/it] 73%|███████▎  | 341/468 [31:59<12:02,  5.69s/it]                                                 {'loss': 0.4859, 'grad_norm': 3.7320706844329834, 'learning_rate': 3.634313742287703e-06, 'epoch': 2.18}
 73%|███████▎  | 341/468 [31:59<12:02,  5.69s/it] 73%|███████▎  | 342/468 [32:04<11:54,  5.67s/it]                                                 {'loss': 0.4257, 'grad_norm': 5.987720966339111, 'learning_rate': 3.5809824980733445e-06, 'epoch': 2.19}
 73%|███████▎  | 342/468 [32:04<11:54,  5.67s/it] 73%|███████▎  | 343/468 [32:10<11:53,  5.70s/it]                                                 {'loss': 0.3921, 'grad_norm': 3.5206973552703857, 'learning_rate': 3.527959977694061e-06, 'epoch': 2.19}
 73%|███████▎  | 343/468 [32:10<11:53,  5.70s/it] 74%|███████▎  | 344/468 [32:16<11:48,  5.72s/it]                                                 {'loss': 0.3156, 'grad_norm': 3.712242603302002, 'learning_rate': 3.475248731277785e-06, 'epoch': 2.2}
 74%|███████▎  | 344/468 [32:16<11:48,  5.72s/it] 74%|███████▎  | 345/468 [32:21<11:41,  5.70s/it]                                                 {'loss': 0.4432, 'grad_norm': 3.7156128883361816, 'learning_rate': 3.422851293981676e-06, 'epoch': 2.2}
 74%|███████▎  | 345/468 [32:21<11:41,  5.70s/it] 74%|███████▍  | 346/468 [32:27<11:27,  5.63s/it]                                                 {'loss': 0.4449, 'grad_norm': 3.8393630981445312, 'learning_rate': 3.3707701858701736e-06, 'epoch': 2.21}
 74%|███████▍  | 346/468 [32:27<11:27,  5.63s/it] 74%|███████▍  | 347/468 [32:33<11:22,  5.64s/it]                                                 {'loss': 0.495, 'grad_norm': 3.9257702827453613, 'learning_rate': 3.3190079117938167e-06, 'epoch': 2.22}
 74%|███████▍  | 347/468 [32:33<11:22,  5.64s/it] 74%|███████▍  | 348/468 [32:38<11:14,  5.62s/it]                                                 {'loss': 0.3424, 'grad_norm': 4.22869348526001, 'learning_rate': 3.2675669612687565e-06, 'epoch': 2.22}
 74%|███████▍  | 348/468 [32:38<11:14,  5.62s/it] 75%|███████▍  | 349/468 [32:44<11:12,  5.65s/it]                                                 {'loss': 0.3712, 'grad_norm': 3.736762762069702, 'learning_rate': 3.2164498083570393e-06, 'epoch': 2.23}
 75%|███████▍  | 349/468 [32:44<11:12,  5.65s/it] 75%|███████▍  | 350/468 [32:49<11:03,  5.62s/it]                                                 {'loss': 0.3415, 'grad_norm': 3.299891471862793, 'learning_rate': 3.165658911547592e-06, 'epoch': 2.24}
 75%|███████▍  | 350/468 [32:49<11:03,  5.62s/it] 75%|███████▌  | 351/468 [32:55<10:50,  5.56s/it]                                                 {'loss': 0.5353, 'grad_norm': 4.125754356384277, 'learning_rate': 3.115196713638e-06, 'epoch': 2.24}
 75%|███████▌  | 351/468 [32:55<10:50,  5.56s/it] 75%|███████▌  | 352/468 [33:00<10:49,  5.60s/it]                                                 {'loss': 0.5414, 'grad_norm': 4.443691253662109, 'learning_rate': 3.0650656416170155e-06, 'epoch': 2.25}
 75%|███████▌  | 352/468 [33:01<10:49,  5.60s/it] 75%|███████▌  | 353/468 [33:06<10:42,  5.59s/it]                                                 {'loss': 0.2667, 'grad_norm': 3.3237006664276123, 'learning_rate': 3.0152681065478252e-06, 'epoch': 2.26}
 75%|███████▌  | 353/468 [33:06<10:42,  5.59s/it] 76%|███████▌  | 354/468 [33:12<10:37,  5.59s/it]                                                 {'loss': 0.5316, 'grad_norm': 3.769054651260376, 'learning_rate': 2.965806503452098e-06, 'epoch': 2.26}
 76%|███████▌  | 354/468 [33:12<10:37,  5.59s/it] 76%|███████▌  | 355/468 [33:17<10:27,  5.55s/it]                                                 {'loss': 0.3756, 'grad_norm': 3.874959707260132, 'learning_rate': 2.9166832111947953e-06, 'epoch': 2.27}
 76%|███████▌  | 355/468 [33:17<10:27,  5.55s/it] 76%|███████▌  | 356/468 [33:23<10:29,  5.62s/it]                                                 {'loss': 0.396, 'grad_norm': 3.9993443489074707, 'learning_rate': 2.8679005923697444e-06, 'epoch': 2.27}
 76%|███████▌  | 356/468 [33:23<10:29,  5.62s/it] 76%|███████▋  | 357/468 [33:28<10:23,  5.62s/it]                                                 {'loss': 0.4859, 'grad_norm': 4.511472225189209, 'learning_rate': 2.819460993186032e-06, 'epoch': 2.28}
 76%|███████▋  | 357/468 [33:29<10:23,  5.62s/it] 76%|███████▋  | 358/468 [33:34<10:21,  5.65s/it]                                                 {'loss': 0.4425, 'grad_norm': 3.993051290512085, 'learning_rate': 2.7713667433551495e-06, 'epoch': 2.29}
 76%|███████▋  | 358/468 [33:34<10:21,  5.65s/it] 77%|███████▋  | 359/468 [33:40<10:10,  5.60s/it]                                                 {'loss': 0.3104, 'grad_norm': 3.693389892578125, 'learning_rate': 2.7236201559789456e-06, 'epoch': 2.29}
 77%|███████▋  | 359/468 [33:40<10:10,  5.60s/it] 77%|███████▋  | 360/468 [33:45<10:08,  5.64s/it]                                                 {'loss': 0.4003, 'grad_norm': 3.33740234375, 'learning_rate': 2.6762235274383775e-06, 'epoch': 2.3}
 77%|███████▋  | 360/468 [33:45<10:08,  5.64s/it] 77%|███████▋  | 361/468 [33:51<10:00,  5.61s/it]                                                 {'loss': 0.4289, 'grad_norm': 3.4303629398345947, 'learning_rate': 2.629179137283071e-06, 'epoch': 2.31}
 77%|███████▋  | 361/468 [33:51<10:00,  5.61s/it] 77%|███████▋  | 362/468 [33:56<09:49,  5.56s/it]                                                 {'loss': 0.386, 'grad_norm': 4.185197830200195, 'learning_rate': 2.582489248121677e-06, 'epoch': 2.31}
 77%|███████▋  | 362/468 [33:57<09:49,  5.56s/it] 78%|███████▊  | 363/468 [34:02<09:49,  5.61s/it]                                                 {'loss': 0.3113, 'grad_norm': 3.0716426372528076, 'learning_rate': 2.5361561055130625e-06, 'epoch': 2.32}
 78%|███████▊  | 363/468 [34:02<09:49,  5.61s/it] 78%|███████▊  | 364/468 [34:07<09:35,  5.53s/it]                                                 {'loss': 0.3776, 'grad_norm': 4.187790870666504, 'learning_rate': 2.490181937858296e-06, 'epoch': 2.33}
 78%|███████▊  | 364/468 [34:08<09:35,  5.53s/it] 78%|███████▊  | 365/468 [34:13<09:38,  5.62s/it]                                                 {'loss': 0.303, 'grad_norm': 3.428647041320801, 'learning_rate': 2.444568956293486e-06, 'epoch': 2.33}
 78%|███████▊  | 365/468 [34:13<09:38,  5.62s/it] 78%|███████▊  | 366/468 [34:19<09:35,  5.64s/it]                                                 {'loss': 0.4838, 'grad_norm': 4.595102310180664, 'learning_rate': 2.3993193545834182e-06, 'epoch': 2.34}
 78%|███████▊  | 366/468 [34:19<09:35,  5.64s/it] 78%|███████▊  | 367/468 [34:25<09:28,  5.63s/it]                                                 {'loss': 0.3463, 'grad_norm': 3.8068437576293945, 'learning_rate': 2.3544353090160664e-06, 'epoch': 2.35}
 78%|███████▊  | 367/468 [34:25<09:28,  5.63s/it] 79%|███████▊  | 368/468 [34:30<09:16,  5.56s/it]                                                 {'loss': 0.4522, 'grad_norm': 5.312670707702637, 'learning_rate': 2.3099189782979126e-06, 'epoch': 2.35}
 79%|███████▊  | 368/468 [34:30<09:16,  5.56s/it] 79%|███████▉  | 369/468 [34:36<09:17,  5.64s/it]                                                 {'loss': 0.4107, 'grad_norm': 4.367471218109131, 'learning_rate': 2.265772503450122e-06, 'epoch': 2.36}
 79%|███████▉  | 369/468 [34:36<09:17,  5.64s/it] 79%|███████▉  | 370/468 [34:41<09:10,  5.62s/it]                                                 {'loss': 0.3498, 'grad_norm': 3.777752161026001, 'learning_rate': 2.2219980077055756e-06, 'epoch': 2.36}
 79%|███████▉  | 370/468 [34:42<09:10,  5.62s/it] 79%|███████▉  | 371/468 [34:47<08:55,  5.52s/it]                                                 {'loss': 0.4115, 'grad_norm': 4.256607532501221, 'learning_rate': 2.178597596406752e-06, 'epoch': 2.37}
 79%|███████▉  | 371/468 [34:47<08:55,  5.52s/it] 79%|███████▉  | 372/468 [34:52<08:55,  5.58s/it]                                                 {'loss': 0.2826, 'grad_norm': 3.4837989807128906, 'learning_rate': 2.1355733569044633e-06, 'epoch': 2.38}
 79%|███████▉  | 372/468 [34:52<08:55,  5.58s/it] 80%|███████▉  | 373/468 [34:58<08:55,  5.64s/it]                                                 {'loss': 0.4409, 'grad_norm': 4.262624740600586, 'learning_rate': 2.092927358457476e-06, 'epoch': 2.38}
 80%|███████▉  | 373/468 [34:58<08:55,  5.64s/it] 80%|███████▉  | 374/468 [35:04<08:53,  5.67s/it]                                                 {'loss': 0.2775, 'grad_norm': 4.39590311050415, 'learning_rate': 2.0506616521329803e-06, 'epoch': 2.39}
 80%|███████▉  | 374/468 [35:04<08:53,  5.67s/it] 80%|████████  | 375/468 [35:10<08:48,  5.68s/it]                                                 {'loss': 0.2975, 'grad_norm': 3.256934642791748, 'learning_rate': 2.008778270707944e-06, 'epoch': 2.4}
 80%|████████  | 375/468 [35:10<08:48,  5.68s/it] 80%|████████  | 376/468 [35:15<08:36,  5.62s/it]                                                 {'loss': 0.41, 'grad_norm': 4.272186756134033, 'learning_rate': 1.9672792285713528e-06, 'epoch': 2.4}
 80%|████████  | 376/468 [35:15<08:36,  5.62s/it] 81%|████████  | 377/468 [35:20<08:22,  5.52s/it]                                                 {'loss': 0.3627, 'grad_norm': 3.8573265075683594, 'learning_rate': 1.9261665216273197e-06, 'epoch': 2.41}
 81%|████████  | 377/468 [35:20<08:22,  5.52s/it] 81%|████████  | 378/468 [35:26<08:12,  5.47s/it]                                                 {'loss': 0.3954, 'grad_norm': 4.512790679931641, 'learning_rate': 1.8854421271990964e-06, 'epoch': 2.42}
 81%|████████  | 378/468 [35:26<08:12,  5.47s/it] 81%|████████  | 379/468 [35:31<08:05,  5.45s/it]                                                 {'loss': 0.4198, 'grad_norm': 4.736869812011719, 'learning_rate': 1.845108003933972e-06, 'epoch': 2.42}
 81%|████████  | 379/468 [35:31<08:05,  5.45s/it] 81%|████████  | 380/468 [35:37<08:00,  5.46s/it]                                                 {'loss': 0.4204, 'grad_norm': 4.261781692504883, 'learning_rate': 1.8051660917090718e-06, 'epoch': 2.43}
 81%|████████  | 380/468 [35:37<08:00,  5.46s/it] 81%|████████▏ | 381/468 [35:42<07:53,  5.45s/it]                                                 {'loss': 0.3441, 'grad_norm': 4.119100093841553, 'learning_rate': 1.7656183115380577e-06, 'epoch': 2.43}
 81%|████████▏ | 381/468 [35:42<07:53,  5.45s/it] 82%|████████▏ | 382/468 [35:48<07:59,  5.58s/it]                                                 {'loss': 0.4957, 'grad_norm': 5.191164970397949, 'learning_rate': 1.7264665654787405e-06, 'epoch': 2.44}
 82%|████████▏ | 382/468 [35:48<07:59,  5.58s/it] 82%|████████▏ | 383/468 [35:54<08:01,  5.67s/it]                                                 {'loss': 0.4227, 'grad_norm': 4.1443190574646, 'learning_rate': 1.6877127365415924e-06, 'epoch': 2.45}
 82%|████████▏ | 383/468 [35:54<08:01,  5.67s/it] 82%|████████▏ | 384/468 [35:59<07:53,  5.64s/it]                                                 {'loss': 0.4521, 'grad_norm': 3.378005027770996, 'learning_rate': 1.6493586885991908e-06, 'epoch': 2.45}
 82%|████████▏ | 384/468 [35:59<07:53,  5.64s/it] 82%|████████▏ | 385/468 [36:05<07:49,  5.65s/it]                                                 {'loss': 0.4687, 'grad_norm': 4.334033966064453, 'learning_rate': 1.6114062662965757e-06, 'epoch': 2.46}
 82%|████████▏ | 385/468 [36:05<07:49,  5.65s/it] 82%|████████▏ | 386/468 [36:11<07:43,  5.65s/it]                                                 {'loss': 0.3824, 'grad_norm': 3.718963623046875, 'learning_rate': 1.5738572949625163e-06, 'epoch': 2.47}
 82%|████████▏ | 386/468 [36:11<07:43,  5.65s/it] 83%|████████▎ | 387/468 [36:16<07:37,  5.65s/it]                                                 {'loss': 0.302, 'grad_norm': 3.881078004837036, 'learning_rate': 1.536713580521746e-06, 'epoch': 2.47}
 83%|████████▎ | 387/468 [36:16<07:37,  5.65s/it] 83%|████████▎ | 388/468 [36:22<07:30,  5.63s/it]                                                 {'loss': 0.3667, 'grad_norm': 3.8546559810638428, 'learning_rate': 1.4999769094080853e-06, 'epoch': 2.48}
 83%|████████▎ | 388/468 [36:22<07:30,  5.63s/it] 83%|████████▎ | 389/468 [36:28<07:25,  5.64s/it]                                                 {'loss': 0.4326, 'grad_norm': 3.51643967628479, 'learning_rate': 1.4636490484785316e-06, 'epoch': 2.49}
 83%|████████▎ | 389/468 [36:28<07:25,  5.64s/it] 83%|████████▎ | 390/468 [36:33<07:16,  5.59s/it]                                                 {'loss': 0.3884, 'grad_norm': 3.644951820373535, 'learning_rate': 1.4277317449282834e-06, 'epoch': 2.49}
 83%|████████▎ | 390/468 [36:33<07:16,  5.59s/it] 84%|████████▎ | 391/468 [36:39<07:07,  5.55s/it]                                                 {'loss': 0.508, 'grad_norm': 4.182404518127441, 'learning_rate': 1.3922267262067025e-06, 'epoch': 2.5}
 84%|████████▎ | 391/468 [36:39<07:07,  5.55s/it] 84%|████████▍ | 392/468 [36:44<07:04,  5.58s/it]                                                 {'loss': 0.4549, 'grad_norm': 3.913961887359619, 'learning_rate': 1.3571356999342366e-06, 'epoch': 2.5}
 84%|████████▍ | 392/468 [36:44<07:04,  5.58s/it] 84%|████████▍ | 393/468 [36:50<07:02,  5.63s/it]                                                 {'loss': 0.328, 'grad_norm': 3.5752506256103516, 'learning_rate': 1.3224603538202929e-06, 'epoch': 2.51}
 84%|████████▍ | 393/468 [36:50<07:02,  5.63s/it] 84%|████████▍ | 394/468 [36:55<06:50,  5.55s/it]                                                 {'loss': 0.4509, 'grad_norm': 4.5995683670043945, 'learning_rate': 1.28820235558206e-06, 'epoch': 2.52}
 84%|████████▍ | 394/468 [36:55<06:50,  5.55s/it] 84%|████████▍ | 395/468 [37:01<06:48,  5.60s/it]                                                 {'loss': 0.3169, 'grad_norm': 3.500018835067749, 'learning_rate': 1.2543633528643084e-06, 'epoch': 2.52}
 84%|████████▍ | 395/468 [37:01<06:48,  5.60s/it] 85%|████████▍ | 396/468 [37:07<06:45,  5.63s/it]                                                 {'loss': 0.5199, 'grad_norm': 4.085587024688721, 'learning_rate': 1.220944973160133e-06, 'epoch': 2.53}
 85%|████████▍ | 396/468 [37:07<06:45,  5.63s/it] 85%|████████▍ | 397/468 [37:12<06:40,  5.63s/it]                                                 {'loss': 0.4534, 'grad_norm': 3.323127031326294, 'learning_rate': 1.1879488237326952e-06, 'epoch': 2.54}
 85%|████████▍ | 397/468 [37:12<06:40,  5.63s/it] 85%|████████▌ | 398/468 [37:18<06:37,  5.68s/it]                                                 {'loss': 0.2335, 'grad_norm': 3.3776769638061523, 'learning_rate': 1.1553764915379095e-06, 'epoch': 2.54}
 85%|████████▌ | 398/468 [37:18<06:37,  5.68s/it] 85%|████████▌ | 399/468 [37:24<06:30,  5.65s/it]                                                 {'loss': 0.2406, 'grad_norm': 3.5179364681243896, 'learning_rate': 1.1232295431481222e-06, 'epoch': 2.55}
 85%|████████▌ | 399/468 [37:24<06:30,  5.65s/it] 85%|████████▌ | 400/468 [37:29<06:22,  5.62s/it]                                                 {'loss': 0.3724, 'grad_norm': 5.120204925537109, 'learning_rate': 1.0915095246767692e-06, 'epoch': 2.56}
 85%|████████▌ | 400/468 [37:29<06:22,  5.62s/it] 86%|████████▌ | 401/468 [37:35<06:20,  5.69s/it]                                                 {'loss': 0.3156, 'grad_norm': 3.219918966293335, 'learning_rate': 1.0602179617040098e-06, 'epoch': 2.56}
 86%|████████▌ | 401/468 [37:35<06:20,  5.69s/it] 86%|████████▌ | 402/468 [37:41<06:11,  5.62s/it]                                                 {'loss': 0.2665, 'grad_norm': 4.011894226074219, 'learning_rate': 1.0293563592033595e-06, 'epoch': 2.57}
 86%|████████▌ | 402/468 [37:41<06:11,  5.62s/it] 86%|████████▌ | 403/468 [37:46<06:02,  5.57s/it]                                                 {'loss': 0.3962, 'grad_norm': 4.453055381774902, 'learning_rate': 9.989262014693013e-07, 'epoch': 2.58}
 86%|████████▌ | 403/468 [37:46<06:02,  5.57s/it] 86%|████████▋ | 404/468 [37:51<05:53,  5.52s/it]                                                 {'loss': 0.2369, 'grad_norm': 3.415513277053833, 'learning_rate': 9.68928952045909e-07, 'epoch': 2.58}
 86%|████████▋ | 404/468 [37:52<05:53,  5.52s/it] 87%|████████▋ | 405/468 [37:57<05:40,  5.41s/it]                                                 {'loss': 0.2877, 'grad_norm': 3.9618499279022217, 'learning_rate': 9.393660536564408e-07, 'epoch': 2.59}
 87%|████████▋ | 405/468 [37:57<05:40,  5.41s/it] 87%|████████▋ | 406/468 [38:02<05:39,  5.47s/it]                                                 {'loss': 0.3798, 'grad_norm': 4.032931327819824, 'learning_rate': 9.102389281339719e-07, 'epoch': 2.59}
 87%|████████▋ | 406/468 [38:02<05:39,  5.47s/it] 87%|████████▋ | 407/468 [38:08<05:33,  5.47s/it]                                                 {'loss': 0.3654, 'grad_norm': 3.982518196105957, 'learning_rate': 8.815489763529938e-07, 'epoch': 2.6}
 87%|████████▋ | 407/468 [38:08<05:33,  5.47s/it] 87%|████████▋ | 408/468 [38:13<05:31,  5.53s/it]                                                 {'loss': 0.4294, 'grad_norm': 4.667022228240967, 'learning_rate': 8.532975781620511e-07, 'epoch': 2.61}
 87%|████████▋ | 408/468 [38:13<05:31,  5.53s/it] 87%|████████▋ | 409/468 [38:19<05:28,  5.57s/it]                                                 {'loss': 0.285, 'grad_norm': 4.562662601470947, 'learning_rate': 8.254860923173691e-07, 'epoch': 2.61}
 87%|████████▋ | 409/468 [38:19<05:28,  5.57s/it] 88%|████████▊ | 410/468 [38:25<05:26,  5.62s/it]                                                 {'loss': 0.4001, 'grad_norm': 4.629003047943115, 'learning_rate': 7.981158564175074e-07, 'epoch': 2.62}
 88%|████████▊ | 410/468 [38:25<05:26,  5.62s/it] 88%|████████▊ | 411/468 [38:30<05:17,  5.57s/it]                                                 {'loss': 0.4931, 'grad_norm': 4.716485023498535, 'learning_rate': 7.711881868390292e-07, 'epoch': 2.63}
 88%|████████▊ | 411/468 [38:30<05:17,  5.57s/it] 88%|████████▊ | 412/468 [38:36<05:16,  5.65s/it]                                                 {'loss': 0.3539, 'grad_norm': 4.123644828796387, 'learning_rate': 7.447043786731867e-07, 'epoch': 2.63}
 88%|████████▊ | 412/468 [38:36<05:16,  5.65s/it] 88%|████████▊ | 413/468 [38:42<05:14,  5.73s/it]                                                 {'loss': 0.42, 'grad_norm': 4.063852310180664, 'learning_rate': 7.18665705663637e-07, 'epoch': 2.64}
 88%|████████▊ | 413/468 [38:42<05:14,  5.73s/it] 88%|████████▊ | 414/468 [38:47<05:03,  5.62s/it]                                                 {'loss': 0.2415, 'grad_norm': 4.725801467895508, 'learning_rate': 6.930734201451817e-07, 'epoch': 2.65}
 88%|████████▊ | 414/468 [38:47<05:03,  5.62s/it] 89%|████████▊ | 415/468 [38:53<04:57,  5.61s/it]                                                 {'loss': 0.3428, 'grad_norm': 3.7986111640930176, 'learning_rate': 6.679287529835266e-07, 'epoch': 2.65}
 89%|████████▊ | 415/468 [38:53<04:57,  5.61s/it] 89%|████████▉ | 416/468 [38:58<04:49,  5.57s/it]                                                 {'loss': 0.3656, 'grad_norm': 4.557804107666016, 'learning_rate': 6.432329135160953e-07, 'epoch': 2.66}
 89%|████████▉ | 416/468 [38:58<04:49,  5.57s/it] 89%|████████▉ | 417/468 [39:04<04:50,  5.70s/it]                                                 {'loss': 0.4355, 'grad_norm': 3.4211151599884033, 'learning_rate': 6.189870894938587e-07, 'epoch': 2.66}
 89%|████████▉ | 417/468 [39:05<04:50,  5.70s/it] 89%|████████▉ | 418/468 [39:10<04:45,  5.71s/it]                                                 {'loss': 0.5656, 'grad_norm': 4.008572101593018, 'learning_rate': 5.951924470242121e-07, 'epoch': 2.67}
 89%|████████▉ | 418/468 [39:10<04:45,  5.71s/it] 90%|████████▉ | 419/468 [39:16<04:36,  5.65s/it]                                                 {'loss': 0.1965, 'grad_norm': 3.473942279815674, 'learning_rate': 5.718501305148893e-07, 'epoch': 2.68}
 90%|████████▉ | 419/468 [39:16<04:36,  5.65s/it] 90%|████████▉ | 420/468 [39:22<04:34,  5.72s/it]                                                 {'loss': 0.4094, 'grad_norm': 3.714733123779297, 'learning_rate': 5.489612626189245e-07, 'epoch': 2.68}
 90%|████████▉ | 420/468 [39:22<04:34,  5.72s/it] 90%|████████▉ | 421/468 [39:27<04:24,  5.62s/it]                                                 {'loss': 0.3628, 'grad_norm': 3.9498581886291504, 'learning_rate': 5.265269441806564e-07, 'epoch': 2.69}
 90%|████████▉ | 421/468 [39:27<04:24,  5.62s/it] 90%|█████████ | 422/468 [39:33<04:21,  5.68s/it]                                                 {'loss': 0.3156, 'grad_norm': 4.096220970153809, 'learning_rate': 5.045482541827828e-07, 'epoch': 2.7}
 90%|█████████ | 422/468 [39:33<04:21,  5.68s/it] 90%|█████████ | 423/468 [39:38<04:11,  5.58s/it]                                                 {'loss': 0.2647, 'grad_norm': 3.799422264099121, 'learning_rate': 4.830262496944693e-07, 'epoch': 2.7}
 90%|█████████ | 423/468 [39:38<04:11,  5.58s/it] 91%|█████████ | 424/468 [39:44<04:08,  5.64s/it]                                                 {'loss': 0.389, 'grad_norm': 4.2050347328186035, 'learning_rate': 4.6196196582050543e-07, 'epoch': 2.71}
 91%|█████████ | 424/468 [39:44<04:08,  5.64s/it] 91%|█████████ | 425/468 [39:49<03:58,  5.55s/it]                                                 {'loss': 0.3286, 'grad_norm': 3.675903797149658, 'learning_rate': 4.4135641565152265e-07, 'epoch': 2.72}
 91%|█████████ | 425/468 [39:49<03:58,  5.55s/it] 91%|█████████ | 426/468 [39:55<03:56,  5.62s/it]                                                 {'loss': 0.3653, 'grad_norm': 6.68895149230957, 'learning_rate': 4.21210590215273e-07, 'epoch': 2.72}
 91%|█████████ | 426/468 [39:55<03:56,  5.62s/it] 91%|█████████ | 427/468 [40:00<03:48,  5.58s/it]                                                 {'loss': 0.3118, 'grad_norm': 4.5670318603515625, 'learning_rate': 4.015254584289585e-07, 'epoch': 2.73}
 91%|█████████ | 427/468 [40:01<03:48,  5.58s/it] 91%|█████████▏| 428/468 [40:06<03:45,  5.64s/it]                                                 {'loss': 0.4062, 'grad_norm': 4.614014625549316, 'learning_rate': 3.8230196705263734e-07, 'epoch': 2.73}
 91%|█████████▏| 428/468 [40:06<03:45,  5.64s/it] 92%|█████████▏| 429/468 [40:12<03:40,  5.65s/it]                                                 {'loss': 0.3566, 'grad_norm': 4.263007164001465, 'learning_rate': 3.635410406436857e-07, 'epoch': 2.74}
 92%|█████████▏| 429/468 [40:12<03:40,  5.65s/it] 92%|█████████▏| 430/468 [40:17<03:33,  5.61s/it]                                                 {'loss': 0.3086, 'grad_norm': 4.693458557128906, 'learning_rate': 3.452435815123323e-07, 'epoch': 2.75}
 92%|█████████▏| 430/468 [40:18<03:33,  5.61s/it] 92%|█████████▏| 431/468 [40:23<03:27,  5.62s/it]                                                 {'loss': 0.5771, 'grad_norm': 4.5385332107543945, 'learning_rate': 3.2741046967826205e-07, 'epoch': 2.75}
 92%|█████████▏| 431/468 [40:23<03:27,  5.62s/it] 92%|█████████▏| 432/468 [40:29<03:23,  5.65s/it]                                                 {'loss': 0.383, 'grad_norm': 3.625337600708008, 'learning_rate': 3.100425628282899e-07, 'epoch': 2.76}
 92%|█████████▏| 432/468 [40:29<03:23,  5.65s/it] 93%|█████████▎| 433/468 [40:34<03:15,  5.59s/it]                                                 {'loss': 0.4057, 'grad_norm': 3.7639706134796143, 'learning_rate': 2.9314069627511045e-07, 'epoch': 2.77}
 93%|█████████▎| 433/468 [40:34<03:15,  5.59s/it] 93%|█████████▎| 434/468 [40:40<03:10,  5.60s/it]                                                 {'loss': 0.3315, 'grad_norm': 3.7317233085632324, 'learning_rate': 2.767056829171255e-07, 'epoch': 2.77}
 93%|█████████▎| 434/468 [40:40<03:10,  5.60s/it] 93%|█████████▎| 435/468 [40:45<03:04,  5.60s/it]                                                 {'loss': 0.3038, 'grad_norm': 3.6646149158477783, 'learning_rate': 2.607383131993424e-07, 'epoch': 2.78}
 93%|█████████▎| 435/468 [40:46<03:04,  5.60s/it] 93%|█████████▎| 436/468 [40:51<03:00,  5.63s/it]                                                 {'loss': 0.4115, 'grad_norm': 3.5803465843200684, 'learning_rate': 2.452393550753662e-07, 'epoch': 2.79}
 93%|█████████▎| 436/468 [40:51<03:00,  5.63s/it] 93%|█████████▎| 437/468 [40:57<02:58,  5.75s/it]                                                 {'loss': 0.3742, 'grad_norm': 3.8674097061157227, 'learning_rate': 2.302095539704563e-07, 'epoch': 2.79}
 93%|█████████▎| 437/468 [40:57<02:58,  5.75s/it] 94%|█████████▎| 438/468 [41:03<02:56,  5.87s/it]                                                 {'loss': 0.2057, 'grad_norm': 3.4922797679901123, 'learning_rate': 2.1564963274568028e-07, 'epoch': 2.8}
 94%|█████████▎| 438/468 [41:03<02:56,  5.87s/it] 94%|█████████▍| 439/468 [41:09<02:47,  5.78s/it]                                                 {'loss': 0.3978, 'grad_norm': 3.9958655834198, 'learning_rate': 2.0156029166314316e-07, 'epoch': 2.81}
 94%|█████████▍| 439/468 [41:09<02:47,  5.78s/it] 94%|█████████▍| 440/468 [41:15<02:40,  5.74s/it]                                                 {'loss': 0.3827, 'grad_norm': 3.6912450790405273, 'learning_rate': 1.8794220835231413e-07, 'epoch': 2.81}
 94%|█████████▍| 440/468 [41:15<02:40,  5.74s/it] 94%|█████████▍| 441/468 [41:20<02:33,  5.68s/it]                                                 {'loss': 0.3953, 'grad_norm': 3.4973185062408447, 'learning_rate': 1.7479603777742937e-07, 'epoch': 2.82}
 94%|█████████▍| 441/468 [41:20<02:33,  5.68s/it] 94%|█████████▍| 442/468 [41:26<02:27,  5.68s/it]                                                 {'loss': 0.2895, 'grad_norm': 3.7524406909942627, 'learning_rate': 1.6212241220599835e-07, 'epoch': 2.82}
 94%|█████████▍| 442/468 [41:26<02:27,  5.68s/it] 95%|█████████▍| 443/468 [41:32<02:22,  5.70s/it]                                                 {'loss': 0.3537, 'grad_norm': 3.2852160930633545, 'learning_rate': 1.49921941178387e-07, 'epoch': 2.83}
 95%|█████████▍| 443/468 [41:32<02:22,  5.70s/it] 95%|█████████▍| 444/468 [41:37<02:16,  5.67s/it]                                                 {'loss': 0.3262, 'grad_norm': 3.9021105766296387, 'learning_rate': 1.3819521147851122e-07, 'epoch': 2.84}
 95%|█████████▍| 444/468 [41:37<02:16,  5.67s/it] 95%|█████████▌| 445/468 [41:43<02:10,  5.69s/it]                                                 {'loss': 0.454, 'grad_norm': 4.469316482543945, 'learning_rate': 1.2694278710560282e-07, 'epoch': 2.84}
 95%|█████████▌| 445/468 [41:43<02:10,  5.69s/it] 95%|█████████▌| 446/468 [41:49<02:05,  5.69s/it]                                                 {'loss': 0.1743, 'grad_norm': 3.5435192584991455, 'learning_rate': 1.1616520924709773e-07, 'epoch': 2.85}
 95%|█████████▌| 446/468 [41:49<02:05,  5.69s/it] 96%|█████████▌| 447/468 [41:54<01:59,  5.70s/it]                                                 {'loss': 0.3226, 'grad_norm': 4.002218246459961, 'learning_rate': 1.0586299625259699e-07, 'epoch': 2.86}
 96%|█████████▌| 447/468 [41:54<01:59,  5.70s/it] 96%|█████████▌| 448/468 [41:59<01:50,  5.53s/it]                                                 {'loss': 0.393, 'grad_norm': 4.074556350708008, 'learning_rate': 9.603664360894327e-08, 'epoch': 2.86}
 96%|█████████▌| 448/468 [42:00<01:50,  5.53s/it] 96%|█████████▌| 449/468 [42:05<01:46,  5.61s/it]                                                 {'loss': 0.351, 'grad_norm': 4.562231540679932, 'learning_rate': 8.668662391638439e-08, 'epoch': 2.87}
 96%|█████████▌| 449/468 [42:05<01:46,  5.61s/it] 96%|█████████▌| 450/468 [42:11<01:41,  5.63s/it]                                                 {'loss': 0.3405, 'grad_norm': 4.830517292022705, 'learning_rate': 7.781338686584928e-08, 'epoch': 2.88}
 96%|█████████▌| 450/468 [42:11<01:41,  5.63s/it] 96%|█████████▋| 451/468 [42:16<01:34,  5.59s/it]                                                 {'loss': 0.3775, 'grad_norm': 3.96207332611084, 'learning_rate': 6.94173592173164e-08, 'epoch': 2.88}
 96%|█████████▋| 451/468 [42:16<01:34,  5.59s/it] 97%|█████████▋| 452/468 [42:22<01:28,  5.54s/it]                                                 {'loss': 0.3466, 'grad_norm': 4.2905402183532715, 'learning_rate': 6.149894477928909e-08, 'epoch': 2.89}
 97%|█████████▋| 452/468 [42:22<01:28,  5.54s/it] 97%|█████████▋| 453/468 [42:27<01:22,  5.51s/it]                                                 {'loss': 0.3067, 'grad_norm': 3.1920387744903564, 'learning_rate': 5.405852438937764e-08, 'epoch': 2.89}
 97%|█████████▋| 453/468 [42:27<01:22,  5.51s/it] 97%|█████████▋| 454/468 [42:32<01:15,  5.42s/it]                                                 {'loss': 0.2952, 'grad_norm': 3.3130340576171875, 'learning_rate': 4.7096455895976334e-08, 'epoch': 2.9}
 97%|█████████▋| 454/468 [42:33<01:15,  5.42s/it] 97%|█████████▋| 455/468 [42:38<01:10,  5.42s/it]                                                 {'loss': 0.434, 'grad_norm': 4.420216083526611, 'learning_rate': 4.0613074141059307e-08, 'epoch': 2.91}
 97%|█████████▋| 455/468 [42:38<01:10,  5.42s/it] 97%|█████████▋| 456/468 [42:43<01:05,  5.45s/it]                                                 {'loss': 0.3824, 'grad_norm': 3.5174520015716553, 'learning_rate': 3.460869094407127e-08, 'epoch': 2.91}
 97%|█████████▋| 456/468 [42:44<01:05,  5.45s/it] 98%|█████████▊| 457/468 [42:49<01:00,  5.48s/it]                                                 {'loss': 0.2966, 'grad_norm': 2.922600746154785, 'learning_rate': 2.9083595086933924e-08, 'epoch': 2.92}
 98%|█████████▊| 457/468 [42:49<01:00,  5.48s/it] 98%|█████████▊| 458/468 [42:54<00:54,  5.49s/it]                                                 {'loss': 0.4173, 'grad_norm': 4.304383754730225, 'learning_rate': 2.403805230015488e-08, 'epoch': 2.93}
 98%|█████████▊| 458/468 [42:55<00:54,  5.49s/it] 98%|█████████▊| 459/468 [43:00<00:49,  5.52s/it]                                                 {'loss': 0.4117, 'grad_norm': 4.494554042816162, 'learning_rate': 1.947230525005006e-08, 'epoch': 2.93}
 98%|█████████▊| 459/468 [43:00<00:49,  5.52s/it] 98%|█████████▊| 460/468 [43:06<00:44,  5.59s/it]                                                 {'loss': 0.4821, 'grad_norm': 4.523683547973633, 'learning_rate': 1.5386573527067516e-08, 'epoch': 2.94}
 98%|█████████▊| 460/468 [43:06<00:44,  5.59s/it] 99%|█████████▊| 461/468 [43:11<00:39,  5.59s/it]                                                 {'loss': 0.2774, 'grad_norm': 3.973456382751465, 'learning_rate': 1.178105363523252e-08, 'epoch': 2.95}
 99%|█████████▊| 461/468 [43:12<00:39,  5.59s/it] 99%|█████████▊| 462/468 [43:17<00:33,  5.63s/it]                                                 {'loss': 0.3764, 'grad_norm': 3.6687145233154297, 'learning_rate': 8.655918982689582e-09, 'epoch': 2.95}
 99%|█████████▊| 462/468 [43:17<00:33,  5.63s/it] 99%|█████████▉| 463/468 [43:23<00:28,  5.65s/it]                                                 {'loss': 0.4337, 'grad_norm': 4.607154369354248, 'learning_rate': 6.011319873370225e-09, 'epoch': 2.96}
 99%|█████████▉| 463/468 [43:23<00:28,  5.65s/it] 99%|█████████▉| 464/468 [43:28<00:22,  5.64s/it]                                                 {'loss': 0.3963, 'grad_norm': 3.9203906059265137, 'learning_rate': 3.847383499756552e-09, 'epoch': 2.96}
 99%|█████████▉| 464/468 [43:29<00:22,  5.64s/it] 99%|█████████▉| 465/468 [43:34<00:16,  5.64s/it]                                                 {'loss': 0.2171, 'grad_norm': 3.7617294788360596, 'learning_rate': 2.164213936770576e-09, 'epoch': 2.97}
 99%|█████████▉| 465/468 [43:34<00:16,  5.64s/it]100%|█████████▉| 466/468 [43:40<00:11,  5.58s/it]                                                 {'loss': 0.2529, 'grad_norm': 3.542584180831909, 'learning_rate': 9.618921367637869e-10, 'epoch': 2.98}
100%|█████████▉| 466/468 [43:40<00:11,  5.58s/it]100%|█████████▉| 467/468 [43:45<00:05,  5.56s/it]                                                 {'loss': 0.356, 'grad_norm': 3.6710686683654785, 'learning_rate': 2.404759256247058e-10, 'epoch': 2.98}
100%|█████████▉| 467/468 [43:45<00:05,  5.56s/it]100%|██████████| 468/468 [43:51<00:00,  5.55s/it]                                                 {'loss': 0.1951, 'grad_norm': 2.887481689453125, 'learning_rate': 0.0, 'epoch': 2.99}
100%|██████████| 468/468 [43:51<00:00,  5.55s/it]/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 266, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank2]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank2]:     self._save_rng_state(output_dir)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank2]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank2]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank2]:     return container(name_or_buffer)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank2]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank2]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_alpaca_abortion/checkpoint-468/rng_state_2.pth cannot be opened.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 266, in <module>
[rank2]:     train()
[rank2]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in train
[rank2]:     trainer.train()
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank2]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank2]:     self._save_rng_state(output_dir)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank2]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank2]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank2]:     return container(name_or_buffer)
[rank2]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank2]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank2]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_alpaca_abortion/checkpoint-468/rng_state_2.pth cannot be opened.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 266, in <module>
[rank3]:     train()
[rank3]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in train
[rank3]:     trainer.train()
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank3]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank3]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank3]:     self._save_rng_state(output_dir)
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank3]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank3]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank3]:     return container(name_or_buffer)
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank3]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank3]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_alpaca_abortion/checkpoint-468/rng_state_3.pth cannot be opened.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 266, in <module>
[rank3]:     train()
[rank3]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in train
[rank3]:     trainer.train()
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank3]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank3]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank3]:     self._save_rng_state(output_dir)
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank3]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank3]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank3]:     return container(name_or_buffer)
[rank3]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank3]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank3]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_alpaca_abortion/checkpoint-468/rng_state_3.pth cannot be opened.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 266, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank1]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank1]:     self._save_rng_state(output_dir)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank1]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank1]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank1]:     return container(name_or_buffer)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank1]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank1]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_alpaca_abortion/checkpoint-468/rng_state_1.pth cannot be opened.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 266, in <module>
[rank1]:     train()
[rank1]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in train
[rank1]:     trainer.train()
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank1]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3014, in _save_checkpoint
[rank1]:     self._save_rng_state(output_dir)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3103, in _save_rng_state
[rank1]:     torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank1]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank1]:     return container(name_or_buffer)
[rank1]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank1]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank1]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_alpaca_abortion/checkpoint-468/rng_state_1.pth cannot be opened.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 266, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3012, in _save_checkpoint
[rank0]:     self._save_optimizer_and_scheduler(output_dir)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3150, in _save_optimizer_and_scheduler
[rank0]:     save_fsdp_optimizer(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/utils/fsdp_utils.py", line 193, in save_fsdp_optimizer
[rank0]:     torch.save(optim_state, output_optimizer_file)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank0]:     return container(name_or_buffer)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank0]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank0]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_alpaca_abortion/checkpoint-468/optimizer.bin cannot be opened.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 266, in <module>
[rank0]:     train()
[rank0]:   File "/sailhome/qinanyu/sft/stanford_alpaca/train.py", line 260, in train
[rank0]:     trainer.train()
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2467, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 2918, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3012, in _save_checkpoint
[rank0]:     self._save_optimizer_and_scheduler(output_dir)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/transformers/trainer.py", line 3150, in _save_optimizer_and_scheduler
[rank0]:     save_fsdp_optimizer(
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/accelerate/utils/fsdp_utils.py", line 193, in save_fsdp_optimizer
[rank0]:     torch.save(optim_state, output_optimizer_file)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 651, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 525, in _open_zipfile_writer
[rank0]:     return container(name_or_buffer)
[rank0]:   File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/serialization.py", line 496, in __init__
[rank0]:     super().__init__(torch._C.PyTorchFileWriter(self.name))
[rank0]: RuntimeError: File /nlp/scr/qinanyu/model_cache/result_alpaca_abortion/checkpoint-468/optimizer.bin cannot be opened.
[1;34mwandb[0m: 🚀 View run [33m/nlp/scr/qinanyu/model_cache/result_alpaca_abortion[0m at: [34mhttps://wandb.ai/limber/huggingface/runs/yxgs4vbv[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../../sailhome/qinanyu/sft/stanford_alpaca/wandb/run-20241011_064954-yxgs4vbv/logs[0m
W1011 07:45:28.795000 139967155655040 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484684 closing signal SIGTERM
W1011 07:45:28.795000 139967155655040 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484686 closing signal SIGTERM
W1011 07:45:28.795000 139967155655040 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484687 closing signal SIGTERM
E1011 07:45:31.283000 139967155655040 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 1484685) of binary: /nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/python
Traceback (most recent call last):
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/nlp/scr/qinanyu/miniconda3/envs/alpaca/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-11_07:45:28
  host      : sphinx6.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1484685)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
