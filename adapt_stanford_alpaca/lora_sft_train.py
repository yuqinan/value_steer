# -*- coding: utf-8 -*-
"""CS329x-HW2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DEk09bdvIFjhm4-fe4aXBvaTd5SUrQCx

# HW2: LLMs for Simulation and Interaction

In the previous homework, you were asked to fine-tune a model on a single-turn interaction dataset: Given an instruction, the model generates a response. Your annotated dataset was a collection of (instruction, response) pairs.

But, we typically use popular LLMs, like OpenAI's ChatGPT or Anthropic's Claude, in conversation/interaction settings.
We've also seen in class how we can evaluate LLMs in interactive settings, and how there have been many exciting applications of LLMs for education and healthcare.


In this homework assignment, you are now going to extend to interactions / multi-turn settings.
We're going to evaluate LLMs in an education interaction between a tutor and a student.

## Download dataset and install libraries

Please download the dataset at https://github.com/stanfordnlp/edu-convokit/blob/main/data/amber.zip

Unzip and upload the directory to this Colab notebook. Your Colab files should look like: `amber/*.json`.
"""

from huggingface_hub import login
login("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")

import pandas as pd
import os
from getpass import getpass

DIR = "amber"
USER_KEY = "speaker"
TEXT_KEY = "dialogue"
filenames = [_ for _ in os.listdir(DIR) if _.endswith(".json")]
assert len(filenames) == 45, "There should be 45 files in the directory"

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import transformers
import datasets
import torch

# Set up your HF tokens

huggingface_token = getpass('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')


# @title Helper functions


def clean_df(df, text_column, speaker_column,target_text_column):
    """
    Create new dataframe where the utterances from same speaker are grouped together.

    Arguments:
        df (pd.DataFrame): pandas dataframe
        text_column (str): name of column containing text to merge utterances
        speaker_column (str): name of column containing speaker names
        target_text_column (str): name of column to store merged text

    Returns:
        pd.DataFrame: dataframe with merged text
    """
    assert text_column in df.columns, f"Text column {text_column} not found in dataframe."
    assert speaker_column in df.columns, f"Speaker column {speaker_column} not found in dataframe."

    if target_text_column is None:
        target_text_column = text_column

    # Cast text_column to string
    df[text_column] = df[text_column].astype(str)

    new_df = []
    text = ""
    speaker = None

    for i, row in df.iterrows():
        if i == 0:
            text += row[text_column]
            speaker = row[speaker_column]
        else:
            if row[speaker_column] == speaker:
                text += " " + row[text_column]
            else:
                new_df.append({
                    target_text_column: text,
                    speaker_column: speaker
                })
                text = row[text_column]
                speaker = row[speaker_column]

    # Add last row
    new_df.append({
        target_text_column: text,
        speaker_column: speaker
    })
    return pd.DataFrame(new_df)

"""Let's look at some conversation examples."""

for i, filename in enumerate(filenames):
  df = pd.read_json(os.path.join(DIR, filename))

  # We're going to merge the utterances from the same speaker.
  df = clean_df(df, TEXT_KEY, USER_KEY, target_text_column=TEXT_KEY)

  # Save to replace the original dataframe
  df.to_json(os.path.join(DIR, filename), orient="records")


  if i < 3: # We're just going to look at the first 3 examples.
    print(df.head())
    print("\n\n\n")

"""Note how there are two speakers: The tutor and the student.

Some context on this dataset: Each transcript includes the same tutor ("Amber"), a single, rich mathematics problem (see below), and a different student.
Students are typically in 8th or 9th grade. The sessions were conducted on a zoom-like platform in which both the tutor and the student could annotate the image. They lasted around 1 hour each.

In this task, we're going to set the LLM to be the tutor.
"""

LLM_USER = "Tutor"

"""Let's split the files into training and test sets. Let's do an 80-20 split.

### Task: Dataset split
"""

#######
# YOUR CODE HERE #
split_idx = int(len(filenames) * 0.8)
train_filenames = filenames[:split_idx]
test_filenames = filenames[split_idx:]
#######

"""In this assignment, we're going to evaluate versions of the Llama 2 chat model in this interactive setting.

- Original model [no changes to the model]
- SFTed model
- RLHFed model

"""

import torch
import pandas as pd
from datasets import Dataset
from peft import LoraConfig
import transformers
from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, GenerationConfig
from tqdm.auto import tqdm
from trl import DPOTrainer, SFTTrainer

model_id = "meta-llama/Llama-2-7b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model_id,token = True, cache_dir= "/nlp/scr/qinanyu/model_cache", legacy = False)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(
    model_id, token = True, cache_dir= "/nlp/scr/qinanyu/model_cache", device_map = "auto",
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type='nf4')
)

base_pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

"""# Chat Formatting for LLMs

In order to actually use the model for chat, we need to format the data into the right chat template for each LLM.
Let's write a function to do that.
Try to pass in a few messages from the chat and let the LLM simulate a response in its role.
The structure should follow user/assistant/user/assistant/... and end on the user so that the LLM assistant can respond.

### Task: Format chat messages
"""

# For this homework, we're just going to use the first 3 messages of the conversation.
# Feel free to change this if you want to pass longer messages! But changing is not necessary for this HW.
NUM_MSGS = 3

##################
# YOUR CODE HERE #

def get_messages_and_last_idx(df, max_size=NUM_MSGS):
  """
  This function takes in a dataframe and max_size (odd), and returns a list of messages and the index of the last message.

  The list of messages should follow the structure of user/assistant/user/assistant/... and end on the user so that the LLM assistant can respond.
  You can format each message content as: {speaker}: {content}.

  Since we won't be using the entire conversation df, we also want to return the index of the last message we include in the conversation.

  Notes:
  - Read more about chat templates here: https://huggingface.co/docs/transformers/en/chat_templating

  """
  # Let's take a few messages from the chat. Role = "user" if speaker != LLM_USER. Otherwise, role = "assistant".
  messages = []
  for _, row in df.iterrows():
    if len(messages) >= max_size:
      break

    # The first message needs to be from the user
    role = "user" if row.speaker != LLM_USER else "assistant"
    if role == "assistant" and not messages:
        continue

    content = row.dialogue
    messages.append({"role": role, "content": f"{row.speaker}: {content}"})
  return messages, _

##################

def data_formulate(df, tokenizer, max_size=NUM_MSGS):
  """
  This function takes in a dataframe, tokenizer, and max size to return the formatted prompt and the index of the last message.
  """
  messages, end_idx = get_messages_and_last_idx(df, max_size)
  prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
  return prompt, end_idx

"""We can visualize how Llama 2 formats chat:"""

prompt, end_idx  = data_formulate(df, tokenizer, max_size=NUM_MSGS)

# Take a look at how Llama 2 Chat model formats the data.
print(prompt)

## Now let's generate a response from the model.
outputs = base_pipeline(
    prompt,
    max_new_tokens=256,
)
print(outputs[0]["generated_text"])

"""## Parsing responses

Now that we can format the data correctly, let's look compare the generated response to the actual response.
First based on the exmaples outputs above, create a function that just parses out the generated response.

### Task: Parse out the model's response
"""

##################
# YOUR CODE HERE #
def parse_generated_response(outputs):
    # Find the last instance of [/INST] and return the text after that.
    substring = "[/INST]"
    response = outputs[0]["generated_text"]
    idx = response.rfind(substring)
    return response[idx + len(substring):]
##################

"""## Comparing responses"""

# Just look at some examples
for filename in filenames[:3]:
    df = pd.read_json(os.path.join(DIR, filename))
    prompt, end_idx = data_formulate(df, tokenizer, max_size=NUM_MSGS)
    outputs = base_pipeline(
        prompt,
        max_new_tokens=256,
        temperature=0.01
    )

    print(f"Context:\n{prompt}")

    response = parse_generated_response(outputs)
    actual = df.iloc[end_idx].dialogue
    print(f"Generated Response: {response}")
    print(f"Actual Response: {actual}")
    print()
    print()

"""### Task: Qualitative observations

In what ways do the generated responses look similar to the actual responses? In what ways do they look different?
"""

##################
# YOUR RESPONSES HERE #
# ...
##################

"""# Evaluating LLMs in interactions

In the next steps we're going to think about evaluations.
From the previous readings, you learned about training reward models to evaluate the quality of generated responses.
You can also define your own evaluation metrics that capture what *you* think embodies good interactions.
This evaluation metric may differ by application and domain!


What we're going to do is

1) You're going to define your own heuristic/evaluation functions that measure how similar the generated responses are to the actual responses. You can be creative here!

2) You're going to load 2 trained reward models to evaluate the quality of the generated responses. You can use RewardBench as a resource: https://arxiv.org/pdf/2403.13787. Compare how your evaluation functions compare to the reward models.

Let's start with the first part: defining your own heuristic/evaluation functions.

### Task: Define your evaluation functions
"""

##################
# YOUR CODE HERE #

eval_name_1 = "length"
def eval1(response, actual):
  return (len(response) - len(actual))

eval_name_2 = "question"
def eval2(response, actual):
  return int("?" in response) - int("?" in actual)
##################

"""Let's also load two reward models to evaluate the quality of the generated responses.

Implement the next two functions that takes in a response and returns the reward.

### Task: Implement reward-model-based evaluations
"""

##################
# YOUR CODE HERE #
from typing import Dict, List

class ArmoRMPipeline:
    def __init__(self, model_id, device_map="auto", torch_dtype=torch.bfloat16, truncation=True, trust_remote_code=False, max_length=4096):
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_id,
            device_map=device_map,
            trust_remote_code=trust_remote_code,
            torch_dtype=torch_dtype,
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            use_fast=True,
        )
        self.truncation = truncation
        self.device = self.model.device
        self.max_length = max_length

    def __call__(self, messages: List[Dict[str, str]]) -> Dict[str, float]:
        """
        messages: OpenAI chat messages to be scored
        Note: no batching since due to length differences, the model will have to pad to the max length which is not efficient
        Returns: a dictionary with the score between 0 and 1
        """
        input_ids = self.tokenizer.apply_chat_template(
            messages,
            return_tensors="pt",
            padding=True,
            truncation=self.truncation,
            max_length=self.max_length,
        ).to(self.device)
        with torch.no_grad():
            output = self.model(input_ids)
            score = output.score.float().item()
        return {"score": score}

# Create Reward Model Pipeline
rm_1 = ArmoRMPipeline("RLHFlow/ArmoRM-Llama3-8B-v0.1", trust_remote_code=True)

def run_rm_1(messages):
  return rm_1(messages)

def run_rm_2(messages):
  return rm_1(messages)


##################

# Test your evaluation functions
prompt = 'What are some synonyms for the word "beautiful"?'
response1 = 'Nicely, Beautifully, Handsome, Stunning, Wonderful, Gorgeous, Pretty, Stunning, Elegant'
messages = [{"role": "user", "content": prompt}, {"role": "assistant", "content": response1}]

print(run_rm_1(messages))
print(run_rm_2(messages))

"""## Evaluating original model
Great, now with our evaluation functions, we can now evaluate the original Llama 2 chat model!
"""

def evaluate(model, tokenizer, df_save_fname):
    eval_df = [] # we'll track the evaluation results here

    pipeline = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        model_kwargs={"torch_dtype": torch.bfloat16},
        device_map="auto",
    )

    for filename in test_filenames:
        df = pd.read_json(os.path.join(DIR, filename))

        if len(df) <= NUM_MSGS:
            continue
        prompt, end_idx = data_formulate(df, tokenizer, max_size=NUM_MSGS)

        ##################
        # YOUR CODE HERE #
        outputs = pipeline(
            prompt,
            max_new_tokens=256,
            temperature=0.01
        )

        response = parse_generated_response(outputs)
        actual = df.iloc[end_idx].dialogue

        # Heuristic Functions
        metric1 = eval1(response, actual)
        metric2 = eval2(response, actual)

        # Reward Models
        messages, end_idx = get_messages_and_last_idx(df, max_size=NUM_MSGS)

        score1 = run_rm_1(messages + [ {"role": "assistant", "content": response}])
        score2 = run_rm_2(messages + [ {"role": "assistant", "content": response}])
        ##################

        eval_df.append({
            "filename": filename,
            eval_name_1: metric1,
            eval_name_2: metric2,
            "reward_model_1": score1["score"],
            "reward_model_2": score2["score"],
            "prompt": prompt,
            "actual": actual,
            "generated": response,
        })

    # Let's now convert the evaluation results into a DataFrame and plot the results
    eval_df = pd.DataFrame(eval_df)
    # Save the eval_df to a file
    eval_df.to_csv(df_save_fname)

    return eval_df


# Save our model. Keep this csv because we're going to be using it later to compare models!
eval_df = evaluate(model, tokenizer, "base_model_eval.csv")

"""# Training and evaluating SFT model

We're now going to move onto training a supervised fine-tuned model.
Let's first create the HF datasets for SFT and format them correctly.

### Task: Formatting SFT chat dataset

You can read more about creating HF datasets here: https://huggingface.co/docs/datasets/en/create_dataset
"""

##################
# YOUR CODE HERE #
from datasets import Dataset

conv_length = NUM_MSGS + 1 # We want to include the next response, hence +1
def get_dataset_from_filenames(filenames, tokenizer, conv_length):
  """
  This function takes in a list of filenames, tokenizer, and conv_length and returns a HF dataset.

  Think about how you can leverage the functions you implemented from before to format the data messages.
  """
  dataset = []
  for filename in filenames:
    df = pd.read_json(os.path.join(DIR, filename))
    if len(df) < conv_length:
        continue
    dataset.append(data_formulate(df, tokenizer, max_size=conv_length)[0])
  return Dataset.from_dict({"text": dataset})
##################

train_dataset = get_dataset_from_filenames(train_filenames, tokenizer, conv_length)
eval_dataset = get_dataset_from_filenames(test_filenames, tokenizer, conv_length)

"""## Train model"""

### SFT ###
LEARNING_RATE = 2e-4
BATCH_SIZE = 1
NUM_EPOCHS = 5

training_args = TrainingArguments(
    output_dir='./',
    per_device_train_batch_size=BATCH_SIZE,
    num_train_epochs=NUM_EPOCHS,
    gradient_accumulation_steps=8,
    gradient_checkpointing=False,
    learning_rate=LEARNING_RATE,
    optim="paged_adamw_8bit",
    logging_steps = 4,
    warmup_ratio = 0.1,
    report_to = 'none'
)

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=8,
    bias="none",
    task_type="CAUSAL_LM",
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    tokenizer=tokenizer,
    args=training_args,
)

trainer.train()
trainer.model.save_pretrained("sft_model")

"""And evaluate the model"""

# Run the evaluation function
eval_df = evaluate(trainer.model, tokenizer, "sft_eval.csv")

print("here")

